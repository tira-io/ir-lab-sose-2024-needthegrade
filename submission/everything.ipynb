{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import re\n",
    "import openai\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 56727.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 126958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs =  pt_dataset.get_corpus_iter()\n",
    "docs = list(docs)\n",
    "count = sum(1 for _ in docs)\n",
    "docs = docs[:126959]\n",
    "print(\"Number of documents:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that removes all special characters from a String, and returns either a String or a list of all words\n",
    "def clean_text(text, return_as_list = False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text) #remove non-alphanumeric characters, except spaces\n",
    "    if return_as_list:\n",
    "        word_list = cleaned_text.split()\n",
    "        return word_list\n",
    "    else:\n",
    "        return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Ensure NLTK data directory is set correctly\n",
    "nltk.data.path.append(\"/usr/local/nltk_data\")\n",
    "\n",
    "# Download 'stopwords' corpus to the specified directory\n",
    "nltk.download('stopwords', download_dir=\"/usr/nltk_data\")\n",
    "\n",
    "# Get the set of stopwords for the English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text, return_as_list = False):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Join the filtered words back into a single string\n",
    "\n",
    "    if return_as_list:\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_text(text, return_as_list = False):\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    if return_as_list:\n",
    "        return stemmed_words\n",
    "    # Join the stemmed words back into a single string\n",
    "    else:\n",
    "        return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our ngram tokenizer. It takes a string and returns a dict of all ngrams, where each ngram is seperated by $$ so it will be parsed as one token\n",
    "\n",
    "def tokenize_ngrams_to_dict(text, n1=1, n2=2):\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text.split(' ')\n",
    "    words = [word for word in words if len(''.join(format(ord(c), '08b') for c in word)) <= 60]\n",
    "\n",
    "    # Initialize an empty Counter to hold all n-grams\n",
    "    all_ngram_counts = Counter()\n",
    "    \n",
    "    # Loop through each n from n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Update the Counter with the current n-grams\n",
    "        all_ngram_counts.update(ngrams)\n",
    "    \n",
    "    return dict(all_ngram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'docno': 'O02-2002', 'toks': {'studi': 1, 'word': 7, 'similar': 8, 'use': 3, 'context': 5, 'vector': 3, 'model': 2, 'need': 1, 'measur': 2, 'process': 1, 'natur': 1, 'languag': 1, 'especi': 1, 'general': 1, 'classif': 1, 'exampl': 1, 'base': 3, 'usual': 1, 'two': 1, 'defin': 1, 'accord': 3, 'distanc': 1, 'semant': 6, 'class': 2, 'less': 1, 'consid': 1, 'syntact': 5, 'ie': 2, 'howev': 1, 'real': 1, 'applic': 1, 'requir': 1, 'weight': 1, 'differ': 1, 'mixtur': 1, 'paper': 1, 'propos': 1, 'relat': 1, 'co': 2, 'occurr': 2, 'adopt': 1, 'inform': 1, 'theoret': 1, 'solv': 1, 'problem': 1, 'data': 1, 'spars': 1, 'precis': 1, 'featur': 2, 'deriv': 1, 'pars': 1, 'environ': 1, 'adjust': 1, 'idf': 1, 'invers': 1, 'valu': 2, 'agglom': 1, 'cluster': 1, 'appli': 1, 'group': 2, 'turn': 1, 'togeth': 1, 'studi$$word': 1, 'word$$similar': 4, 'similar$$use': 1, 'use$$context': 1, 'context$$vector': 3, 'vector$$model': 1, 'model$$need': 1, 'need$$measur': 1, 'measur$$word': 1, 'similar$$process': 1, 'process$$natur': 1, 'natur$$languag': 1, 'languag$$especi': 1, 'especi$$use': 1, 'use$$general': 1, 'general$$classif': 1, 'classif$$exampl': 1, 'exampl$$base': 1, 'base$$usual': 1, 'usual$$measur': 1, 'measur$$similar': 1, 'similar$$two': 1, 'two$$word': 1, 'word$$defin': 1, 'defin$$accord': 1, 'accord$$distanc': 1, 'distanc$$semant': 1, 'semant$$class': 2, 'class$$semant': 1, 'semant$$less': 1, 'less$$semant': 1, 'semant$$base': 1, 'base$$consid': 1, 'consid$$syntact': 1, 'syntact$$ie': 1, 'ie$$howev': 1, 'howev$$real': 1, 'real$$applic': 1, 'applic$$semant': 1, 'semant$$syntact': 1, 'syntact$$similar': 1, 'similar$$requir': 1, 'requir$$weight': 1, 'weight$$differ': 1, 'differ$$word': 1, 'similar$$base': 1, 'base$$context': 1, 'vector$$mixtur': 1, 'mixtur$$syntact': 1, 'syntact$$semant': 2, 'semant$$ie': 1, 'ie$$paper': 1, 'paper$$propos': 1, 'propos$$use': 1, 'use$$syntact': 1, 'syntact$$relat': 1, 'relat$$co': 1, 'co$$occurr': 2, 'occurr$$context': 2, 'vector$$adopt': 1, 'adopt$$inform': 1, 'inform$$theoret': 1, 'theoret$$model': 1, 'model$$solv': 1, 'solv$$problem': 1, 'problem$$data': 1, 'data$$spars': 1, 'spars$$precis': 1, 'precis$$co': 1, 'context$$featur': 2, 'featur$$deriv': 1, 'deriv$$pars': 1, 'pars$$environ': 1, 'environ$$word': 1, 'word$$context': 1, 'featur$$adjust': 1, 'adjust$$accord': 1, 'accord$$idf': 1, 'idf$$invers': 1, 'invers$$valu': 1, 'valu$$agglom': 1, 'agglom$$cluster': 1, 'cluster$$appli': 1, 'appli$$group': 1, 'group$$similar': 1, 'similar$$word': 1, 'word$$accord': 1, 'accord$$similar': 1, 'similar$$valu': 1, 'valu$$turn': 1, 'turn$$word': 1, 'similar$$syntact': 1, 'class$$group': 1, 'group$$togeth': 1}}\n",
      "{'docno': 'L02-1310', 'toks': {'larg': 1, 'sens': 1, 'tag': 1, 'corpora': 1, 'larg$$sens': 1, 'sens$$tag': 1, 'tag$$corpora': 1}}\n",
      "{'docno': 'R13-1042', 'toks': {'use': 2, 'pairwis': 2, 'email': 6, 'classif': 2, 'thread': 6, 'task': 1, 'separ': 1, 'convers': 1, 'whose': 1, 'distort': 1, 'lost': 1, 'paper': 1, 'perform': 2, 'text': 4, 'similar': 5, 'measur': 1, 'non': 1, 'quot': 1, 'show': 1, 'content': 2, 'metric': 2, 'style': 1, 'class': 2, 'balanc': 1, 'set': 1, 'ii': 1, 'featur': 2, 'depend': 1, 'semant': 2, 'corpus': 4, 'still': 1, 'effect': 1, 'even': 1, 'control': 1, 'make': 1, 'avail': 1, 'enron': 2, 'newli': 1, 'extract': 1, '70': 1, '178': 1, 'use$$pairwis': 1, 'pairwis$$email': 1, 'email$$classif': 1, 'classif$$email': 1, 'email$$thread': 2, 'thread$$thread': 1, 'thread$$task': 1, 'task$$separ': 1, 'separ$$convers': 1, 'convers$$whose': 1, 'whose$$thread': 1, 'thread$$distort': 1, 'distort$$lost': 1, 'lost$$paper': 1, 'paper$$perform': 1, 'perform$$email': 1, 'thread$$pairwis': 1, 'pairwis$$classif': 1, 'classif$$use': 1, 'use$$text': 1, 'text$$similar': 3, 'similar$$measur': 1, 'measur$$non': 1, 'non$$quot': 1, 'quot$$text': 1, 'text$$email': 1, 'email$$show': 1, 'show$$content': 1, 'content$$text': 1, 'similar$$metric': 2, 'metric$$style': 1, 'style$$text': 1, 'metric$$class': 1, 'class$$balanc': 1, 'balanc$$class': 1, 'class$$set': 1, 'set$$ii': 1, 'ii$$featur': 1, 'featur$$perform': 1, 'perform$$depend': 1, 'depend$$semant': 1, 'semant$$similar': 2, 'similar$$corpus': 1, 'corpus$$content': 1, 'content$$featur': 1, 'featur$$still': 1, 'still$$effect': 1, 'effect$$even': 1, 'even$$control': 1, 'control$$semant': 1, 'similar$$make': 1, 'make$$avail': 1, 'avail$$enron': 1, 'enron$$thread': 1, 'thread$$corpus': 1, 'corpus$$newli': 1, 'newli$$extract': 1, 'extract$$corpus': 1, 'corpus$$70': 1, '70$$178': 1, '178$$thread': 1, 'thread$$email': 1, 'email$$enron': 1, 'enron$$email': 1, 'email$$corpus': 1}}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "        if 'text' in doc:\n",
    "            doc['text'] = clean_text(doc['text'])\n",
    "            doc['text'] = remove_stopwords(doc['text'])\n",
    "            doc['text'] = stem_text(doc['text'])\n",
    "            \n",
    "            doc_1gram = tokenize_ngrams_to_dict(doc['text'], n1=1, n2=2) # Apply n-gram tokenization to the dataset\n",
    "\n",
    "            doc['toks'] = doc_1gram # create new toks field for tokenfrequency\n",
    "            del doc['text']  #This will delete the 'text' field from the documents\n",
    "    \n",
    "for i, doc in enumerate(docs):\n",
    "     if i == 3:\n",
    "           break\n",
    "     print(doc)\n",
    "\n",
    "#remove all empty documents\n",
    "docs = [d for d in docs if any(k != '' for k in d['toks'].keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 126824\n",
      "Number of terms: 1735213\n",
      "Number of postings: 11740755\n",
      "Number of fields: 0\n",
      "Number of tokens: 14688164\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n",
      "0\n",
      "9406\n",
      "0$$0\n",
      "84\n",
      "0$$00\n",
      "11\n",
      "0$$000\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Initialize the IterDictIndexer with pretokenised set to True\n",
    "iter_indexer = pt.IterDictIndexer(\"./ngramindex\", overwrite=True, meta={'docno': 35}, pretokenised=True, verbose = True, type = pt.index.IndexingType.SINGLEPASS)\n",
    "\n",
    "# Index our pretokenized documents\n",
    "index_ref = iter_indexer.index(docs)\n",
    "\n",
    "index_ngram = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "#Print some stats about our index\n",
    "print(index_ngram.getCollectionStatistics())\n",
    "\n",
    "# Access the MetaIndex and Lexicon\n",
    "meta = index_ngram.getMetaIndex()\n",
    "lexicon = index_ngram.getLexicon()\n",
    "\n",
    "\n",
    "i = 0\n",
    "for term, le in index_ngram.getLexicon():\n",
    "    i = i+1\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(term) \n",
    "    print(le.getFrequency())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABBREVIATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI() #connect to OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt(prompt, model=\"gpt-4\", temperature=0):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "querys = pt_dataset.get_topics('query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_integers_from_qid(df):\n",
    "    # Convert the qid column to integers\n",
    "    return df['qid'].astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_by_qid(df, qid):\n",
    "    # Convert the qid to string and find the corresponding query\n",
    "    result = df.loc[df['qid'] == str(qid), 'query']\n",
    "    return result.iloc[0] if not result.empty else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_query_by_qid(df, qid, new_query):\n",
    "    # Convert the qid to string and update the query\n",
    "    df.loc[df['qid'] == str(qid), 'query'] = new_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_list = extract_integers_from_qid(querys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': False, '2': False, '3': False, '4': False, '5': False, '6': False, '7': False, '8': False, '9': False, '10': False, '11': True, '12': False, '13': False, '14': False, '15': False, '16': False, '17': False, '19': False, '20': False, '21': False, '22': False, '23': False, '24': False, '25': False, '26': False, '27': False, '28': False, '29': False, '30': False, '31': False, '32': False, '33': False, '34': False, '35': False, '36': False, '37': False, '38': False, '39': False, '40': False, '41': False, '42': False, '43': False, '44': False, '45': False, '46': False, '47': False, '48': False, '49': False, '50': False, '51': False, '52': False, '53': False, '54': False, '55': True, '56': False, '57': False, '58': False, '59': True, '60': False, '61': False, '62': False, '63': False, '64': False, '65': False, '66': False, '67': False, '68': False, '18': False}\n"
     ]
    }
   ],
   "source": [
    "answers = dict()\n",
    "for i in qid_list:\n",
    "    determine_abbrevation = f\"\"\" \n",
    "    You are an scientific expert especially in the domain of Information Retrieval. Your task is to detect whether\n",
    "    a given query, which is given as a text below delimited by triple quotes, contains an abbrevation then answer with yes or not then answer with no.\n",
    "    For example given a query 'What is crx' you should answer yes, since cxr is the abbrevation for the medical term\n",
    "    'chest X-Ray'. However if the given query is 'What is Information Retrieval' you should answer no, since there is no\n",
    "    abbrevation in the query.\n",
    "\n",
    "    query: '''{get_query_by_qid(querys, i)}'''\n",
    "    \"\"\"\n",
    "    answer = ask_gpt(prompt=determine_abbrevation) #check answer more carefully perhaps model will return not only {yes,no}\n",
    "    #print(answer)\n",
    "    qid = str(i)\n",
    "    if \"yes\" in answer.lower().strip():\n",
    "        answers[qid] = True\n",
    "    else:\n",
    "        answers[qid] = False\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old query:  algorithm acceleration with nvidia cuda\n",
      "New query:  algorithm acceleration with nvidia cuda compute unified device architecture\n",
      "Old query:  bm25\n",
      "New query:  bm25 best match 25\n",
      "Old query:  what is ahp\n",
      "New query:  what is ahp analytic hierarchy process\n"
     ]
    }
   ],
   "source": [
    "for key in answers.keys():\n",
    "    if bool(answers[key]):\n",
    "        #find query\n",
    "        for i in qid_list:\n",
    "            if str(i) == key:\n",
    "                query= get_query_by_qid(querys, key)\n",
    "                print(\"Old query: \",query)\n",
    "        #ask gpt to expand query\n",
    "        expand = f\"\"\" \n",
    "        You are an scientific expert especially in the domain of Information Retrieval. Your are given a query, which is below\n",
    "        delimited by triple quotes, which contains an abbrevation. Your task is to identify the abbrevation and write it, then\n",
    "        concat the original query with the written out abbrevation and return this new query as string only.\n",
    "        For example given a query 'What is crx' you should detect that the abbrevation is crx, since cxr is the abbrevation for the medical term\n",
    "        'chest X-Ray', then you should concat the originial query with the abbrevation 'chest X-Ray' resulting in a new query 'What is crx chest x-ray' which\n",
    "        you should return. Another example, given the query 'Algorithms of nlp' you should detect that the abbrevation is nlp, since nlp is the abbrevation\n",
    "        for the term 'natural language processing', then you should concat the original query 'Algorithms of nlp' with the abbrevation 'natural language processing'  \n",
    "        resulting in a new query 'Algorithms of nlp natural language processing' which you should return.\n",
    "        Please only answer with the new query. So your answer should only include the original query and the detected abbreviated words and no additional information.\n",
    "        Don't wrap your answer in quotation marks.\n",
    "\n",
    "        query: '''{query}'''\n",
    "        \"\"\"\n",
    "        new_query = ask_gpt(prompt=expand).lower().strip().replace(\"'\", \" \").replace('\"', ' ')\n",
    "        print(\"New query: \",new_query)\n",
    "        set_query_by_qid(querys, int(key), new_query) #overwrite old query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'retrieval system improving effectiveness retrieval$$system'], ['2', 'machine learning language identification machine$$learning language$$identification'], ['3', 'social media detect self harm social$$media self$$harm'], ['4', 'stemming for arabic languages arabic$$languages'], ['5', 'audio based animal recognition audio$$based animal$$recognition'], ['6', 'comparison different retrieval models retrieval$$models'], ['7', 'cache architecture cache$$architecture'], ['8', 'document scoping formula document$$scoping'], ['9', 'pseudo relevance feedback pseudo$$relevance relevance$$feedback'], ['10', 'how to represent natural conversations in word nets natural$$conversations word$$nets'], ['11', 'algorithm acceleration with nvidia cuda compute unified device architecture nvidia$$cuda compute$$unified unified$$device device$$architecture'], ['12', 'mention of algorithm'], ['13', 'at least three authors at$$least three$$authors'], ['14', 'german domain german$$domain'], ['15', 'mention of open source open$$source'], ['16', 'inclusion of text mining text$$mining'], ['17', 'the ethics of artificial intelligence artificial$$intelligence'], ['19', 'machine learning for more relevant results machine$$learning relevant$$results'], ['20', 'crawling websites using machine learning machine$$learning'], ['21', 'recommenders influence on users'], ['22', 'search engine caching effects search$$engine caching$$effects'], ['23', 'consumer product reviews consumer$$product product$$reviews'], ['24', 'limitations machine learning machine$$learning'], ['25', 'medicine related research medicine$$related related$$research'], ['26', 'natural language processing natural$$language language$$processing'], ['27', 'graph based ranking graph$$based based$$ranking'], ['28', 'medical studies that use information retrieval information$$retrieval'], ['29', 'information retrieval on different language sources information$$retrieval language$$sources'], ['30', 'papers that compare multiple information retrieval methods information$$retrieval'], ['31', 'risks of information retrieval in social media information$$retrieval social$$media'], ['32', 'actual experiments that strengthen theoretical knowledge theoretical$$knowledge'], ['33', 'fake news detection fake$$news'], ['34', 'multimedia retrieval multimedia$$retrieval'], ['35', 'processing natural language for information retrieval natural$$language information$$retrieval'], ['36', 'recommendation systems recommendation$$systems'], ['37', 'personalised search in e commerce personalised$$search e$$commerce'], ['38', 'sentiment analysis sentiment$$analysis'], ['39', 'informational retrieval using neural networks informational$$retrieval neural$$networks'], ['40', 'query log analysis query$$log log$$analysis'], ['41', 'entity recognition entity$$recognition'], ['42', 'relevance assessments'], ['43', 'deep neural networks deep$$neural neural$$networks'], ['44', 'information retrieval information$$retrieval'], ['45', 'analysis for android apps android$$apps'], ['46', 'the university of amsterdam university$$of of$$amsterdam'], ['47', 'neural ranking for ecommerce product search neural$$ranking product$$search'], ['48', 'web pages evolution web$$pages'], ['49', 'exhaustivity of index'], ['50', 'query optimization query$$optimization'], ['51', 'cosine similarity vector cosine$$similarity'], ['52', 'reverse indexing reverse$$indexing'], ['53', 'index compression techniques index$$compression compression$$techniques'], ['54', 'search engine optimization with query logs search$$engine engine$$optimization query$$logs'], ['55', 'bm25 best match 25 best$$match'], ['56', 'what makes natural language processing natural natural$$language language$$processing'], ['57', 'principle of a information retrieval indexing information$$retrieval'], ['58', 'architecture of web search engine web$$search search$$engine'], ['59', 'what is ahp analytic hierarchy process analytic$$hierarchy hierarchy$$process'], ['60', 'what is information retrieval information$$retrieval'], ['61', 'efficient retrieval algorithms retrieval$$algorithms'], ['62', 'how to avoid spam results spam$$results'], ['63', 'information retrieval with algorithms information$$retrieval'], ['64', 'misspellings in queries'], ['65', 'information in different language different$$language'], ['66', 'abbreviations in queries'], ['67', 'lemmatization algorithms'], ['68', 'filter ad rich documents ad$$rich'], ['18', 'advancements in information retrieval information$$retrieval']]\n"
     ]
    }
   ],
   "source": [
    "expanded_queries_list = querys.values.tolist()\n",
    "for i in qid_list:\n",
    "    \n",
    "\n",
    "    determine_ngrams = f\"\"\" \n",
    "    You are an scientific expert in the domain of Information Retrieval and linguistics. Your task is to detect whether\n",
    "    a given query, which is given as a text below delimited by triple quotes, contains bigrams. This means, you should check for all \n",
    "    bigrams in the query, if they are an existing term consisting of multiple words. Then, your answer should be the original query \n",
    "    with all the bigrams you found appended in the format word1$$word2. Your answer should only include the query and the bigrams, no additional information.\n",
    "    This means that when there are no existing bigrams in the query, your answer should just be the original query. You should not wrap your answer in quotation marks.\n",
    "    For example given a query 'usage of machine learning in image recognition' you should answer\n",
    "      'usage of machine learning in image recognition machine$$learning image$$recognition'.\n",
    "\n",
    "    query: '''{get_query_by_qid(querys, i)}'''\n",
    "    \"\"\"\n",
    "    answer = ask_gpt(prompt=determine_ngrams)\n",
    "    set_query_by_qid(querys, i, answer)\n",
    "\n",
    "expanded_queries_list = querys.values.tolist()\n",
    "print(expanded_queries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 'retrieval system improving effectiveness retrieval$$system'), ('2', 'machine learning language identification machine$$learning language$$identification'), ('3', 'social media detect self harm social$$media self$$harm'), ('4', 'stemming for arabic languages arabic$$languages'), ('5', 'audio based animal recognition audio$$based animal$$recognition'), ('6', 'comparison different retrieval models retrieval$$models'), ('7', 'cache architecture cache$$architecture'), ('8', 'document scoping formula document$$scoping'), ('9', 'pseudo relevance feedback pseudo$$relevance relevance$$feedback'), ('10', 'how to represent natural conversations in word nets natural$$conversations word$$nets'), ('11', 'algorithm acceleration with nvidia cuda compute unified device architecture nvidia$$cuda compute$$unified unified$$device device$$architecture'), ('12', 'mention of algorithm'), ('13', 'at least three authors at$$least three$$authors'), ('14', 'german domain german$$domain'), ('15', 'mention of open source open$$source'), ('16', 'inclusion of text mining text$$mining'), ('17', 'the ethics of artificial intelligence artificial$$intelligence'), ('19', 'machine learning for more relevant results machine$$learning relevant$$results'), ('20', 'crawling websites using machine learning machine$$learning'), ('21', 'recommenders influence on users'), ('22', 'search engine caching effects search$$engine caching$$effects'), ('23', 'consumer product reviews consumer$$product product$$reviews'), ('24', 'limitations machine learning machine$$learning'), ('25', 'medicine related research medicine$$related related$$research'), ('26', 'natural language processing natural$$language language$$processing'), ('27', 'graph based ranking graph$$based based$$ranking'), ('28', 'medical studies that use information retrieval information$$retrieval'), ('29', 'information retrieval on different language sources information$$retrieval language$$sources'), ('30', 'papers that compare multiple information retrieval methods information$$retrieval'), ('31', 'risks of information retrieval in social media information$$retrieval social$$media'), ('32', 'actual experiments that strengthen theoretical knowledge theoretical$$knowledge'), ('33', 'fake news detection fake$$news'), ('34', 'multimedia retrieval multimedia$$retrieval'), ('35', 'processing natural language for information retrieval natural$$language information$$retrieval'), ('36', 'recommendation systems recommendation$$systems'), ('37', 'personalised search in e commerce personalised$$search e$$commerce'), ('38', 'sentiment analysis sentiment$$analysis'), ('39', 'informational retrieval using neural networks informational$$retrieval neural$$networks'), ('40', 'query log analysis query$$log log$$analysis'), ('41', 'entity recognition entity$$recognition'), ('42', 'relevance assessments'), ('43', 'deep neural networks deep$$neural neural$$networks'), ('44', 'information retrieval information$$retrieval'), ('45', 'analysis for android apps android$$apps'), ('46', 'the university of amsterdam university$$of of$$amsterdam'), ('47', 'neural ranking for ecommerce product search neural$$ranking product$$search'), ('48', 'web pages evolution web$$pages'), ('49', 'exhaustivity of index'), ('50', 'query optimization query$$optimization'), ('51', 'cosine similarity vector cosine$$similarity'), ('52', 'reverse indexing reverse$$indexing'), ('53', 'index compression techniques index$$compression compression$$techniques'), ('54', 'search engine optimization with query logs search$$engine engine$$optimization query$$logs'), ('55', 'bm25 best match 25 best$$match'), ('56', 'what makes natural language processing natural natural$$language language$$processing'), ('57', 'principle of a information retrieval indexing information$$retrieval'), ('58', 'architecture of web search engine web$$search search$$engine'), ('59', 'what is ahp analytic hierarchy process analytic$$hierarchy hierarchy$$process'), ('60', 'what is information retrieval information$$retrieval'), ('61', 'efficient retrieval algorithms retrieval$$algorithms'), ('62', 'how to avoid spam results spam$$results'), ('63', 'information retrieval with algorithms information$$retrieval'), ('64', 'misspellings in queries'), ('65', 'information in different language different$$language'), ('66', 'abbreviations in queries'), ('67', 'lemmatization algorithms'), ('68', 'filter ad rich documents ad$$rich'), ('18', 'advancements in information retrieval information$$retrieval')]\n"
     ]
    }
   ],
   "source": [
    "data_expd = [(sublist[0], sublist[1]) for sublist in expanded_queries_list]\n",
    "print(data_expd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expanded_queries = [['1', 'retrieval system improving effectiveness', 'retrieval system improving effectiveness retrieval$$system'], ['2', 'machine learning language identification', 'machine learning language identification machine$$learning language$$identification'], ['3', 'social media detect self harm', 'social media detect self harm social$$media self$$harm'], ['4', 'stemming for arabic languages', 'stemming for arabic languages arabic$$languages'], ['5', 'audio based animal recognition', 'audio based animal recognition audio$$based animal$$recognition'], ['6', 'comparison different retrieval models', 'comparison different retrieval models retrieval$$models'], ['7', 'cache architecture', 'cache architecture'], ['8', 'document scoping formula', 'document scoping formula document$$scoping'], ['9', 'pseudo relevance feedback', 'pseudo relevance feedback pseudo$$relevance$$feedback pseudo$$relevance relevance$$feedback'], ['10', 'how to represent natural conversations in word nets', 'how to represent natural conversations in word nets natural$$conversations word$$nets'], ['11', 'algorithm acceleration with nvidia cuda', 'algorithm acceleration with nvidia cuda nvidia$$cuda'], ['12', 'mention of algorithm', 'mention of algorithm'], ['13', 'at least three authors', 'at least three authors three$$authors'], ['14', 'german domain', 'german domain german$$domain'], ['15', 'mention of open source', 'mention of open source open$$source'], ['16', 'inclusion of text mining', 'inclusion of text mining text$$mining'], ['17', 'the ethics of artificial intelligence', 'the ethics of artificial intelligence artificial$$intelligence'], ['19', 'machine learning for more relevant results', 'machine learning for more relevant results machine$$learning relevant$$results'], ['20', 'crawling websites using machine learning', 'crawling websites using machine learning machine$$learning'], ['21', 'recommenders influence on users', 'recommenders influence on users'], ['22', 'search engine caching effects', 'search engine caching effects search$$engine caching$$effects'], ['23', 'consumer product reviews', 'consumer product reviews consumer$$product product$$reviews'], ['24', 'limitations machine learning', 'limitations machine learning machine$$learning'], ['25', 'medicine related research', 'medicine related research medicine$$related related$$research'], ['26', 'natural language processing', 'natural language processing natural$$language$$processing natural$$language'], ['27', 'graph based ranking', 'graph based ranking graph$$based based$$ranking'], ['28', 'medical studies that use information retrieval', 'medical studies that use information retrieval information$$retrieval'], ['29', 'information retrieval on different language sources', 'information retrieval on different language sources information$$retrieval language$$sources'], ['30', 'papers that compare multiple information retrieval methods', 'papers that compare multiple information retrieval methods information$$retrieval'], ['31', 'risks of information retrieval in social media', 'risks of information retrieval in social media information$$retrieval social$$media'], ['32', 'actual experiments that strengthen theoretical knowledge', 'actual experiments that strengthen theoretical knowledge theoretical$$knowledge'], ['33', 'fake news detection', 'fake news detection fake$$news news$$detection'], ['34', 'multimedia retrieval', 'multimedia retrieval multimedia$$retrieval'], ['35', 'processing natural language for information retrieval', 'processing natural language for information retrieval natural$$language information$$retrieval'], ['36', 'recommendation systems', 'recommendation systems recommendation$$systems'], ['37', 'personalised search in e commerce', 'personalised search in e commerce personalised$$search e$$commerce'], ['38', 'sentiment analysis', 'sentiment analysis sentiment$$analysis'], ['39', 'informational retrieval using neural networks', 'informational retrieval using neural networks informational$$retrieval neural$$networks'], ['40', 'query log analysis', 'query log analysis query$$log log$$analysis'], ['41', 'entity recognition', 'entity recognition entity$$recognition'], ['42', 'relevance assessments', 'relevance assessments relevance$$assessments'], ['43', 'deep neural networks', 'deep neural networks deep$$neural neural$$networks'], ['44', 'information retrieval', 'information retrieval information$$retrieval'], ['45', 'analysis for android apps', 'analysis for android apps android$$apps'], ['46', 'the university of amsterdam', 'the university of amsterdam university$$of of$$amsterdam'], ['47', 'neural ranking for ecommerce product search', 'neural ranking for ecommerce product search neural$$ranking product$$search'], ['48', 'web pages evolution', 'web pages evolution web$$pages'], ['49', 'exhaustivity of index', 'exhaustivity of index'], ['50', 'query optimization', 'query optimization query$$optimization'], ['51', 'cosine similarity vector', 'cosine similarity vector cosine$$similarity'], ['52', 'reverse indexing', 'reverse indexing reverse$$indexing'], ['53', 'index compression techniques', 'index compression techniques index$$compression compression$$techniques'], ['54', 'search engine optimization with query logs', 'search engine optimization with query logs search$$engine engine$$optimization query$$logs'], ['55', 'bm25', 'bm25'], ['56', 'what makes natural language processing natural', 'what makes natural language processing natural natural$$language$$processing natural$$language'], ['57', 'principle of a information retrieval indexing', 'principle of a information retrieval indexing information$$retrieval'], ['58', 'architecture of web search engine', 'architecture of web search engine web$$search search$$engine'], ['59', 'what is ahp', 'what is ahp'], ['60', 'what is information retrieval', 'what is information retrieval information$$retrieval'], ['61', 'efficient retrieval algorithms', 'efficient retrieval algorithms retrieval$$algorithms'], ['62', 'how to avoid spam results', 'how to avoid spam results spam$$results'], ['63', 'information retrieval with algorithms', 'information retrieval with algorithms information$$retrieval'], ['64', 'misspellings in queries', 'misspellings in queries'], ['65', 'information in different language', 'information in different language different$$language'], ['66', 'abbreviations in queries', 'abbreviations in queries'], ['67', 'lemmatization algorithms', 'lemmatization algorithms'], ['68', 'filter ad rich documents', 'filter ad rich documents ad$$rich'], ['18', 'advancements in information retrieval', 'advancements in information retrieval information$$retrieval']]\n",
    "#print(expanded_queries)\n",
    "\n",
    "#data_expd = [(sublist[0], sublist[1]) for sublist in expanded_queries] # Extract qid and query from each sublist\n",
    "\n",
    "\n",
    "df_expd = pd.DataFrame(data_expd, columns=['qid', 'query']) # Create pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>62</td>\n",
       "      <td>how to avoid spam results spam$$results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>63</td>\n",
       "      <td>information retrieval with algorithms informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>64</td>\n",
       "      <td>misspellings in queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>65</td>\n",
       "      <td>information in different language different$$l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>66</td>\n",
       "      <td>abbreviations in queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>67</td>\n",
       "      <td>lemmatization algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>68</td>\n",
       "      <td>filter ad rich documents ad$$rich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>18</td>\n",
       "      <td>advancements in information retrieval informat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid                                              query\n",
       "60  62            how to avoid spam results spam$$results\n",
       "61  63  information retrieval with algorithms informat...\n",
       "62  64                            misspellings in queries\n",
       "63  65  information in different language different$$l...\n",
       "64  66                           abbreviations in queries\n",
       "65  67                           lemmatization algorithms\n",
       "66  68                  filter ad rich documents ad$$rich\n",
       "67  18  advancements in information retrieval informat..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_expd[60:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_with_dollar_signs(text, return_as_list=False, keep_dollar_signs=False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    if keep_dollar_signs:      \n",
    "        text = text.replace('$$', 'DOUBLEDOLLARNGRAMS')                 # Replace double dollar signs with a unique placeholder\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)             # Remove all non-alphanumeric characters except spaces    \n",
    "        cleaned_text = cleaned_text.replace('DOUBLEDOLLARNGRAMS', '$$') # Replace placeholder back to double dollar signs\n",
    "    else:\n",
    "        \n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)             # Remove all non-alphanumeric characters except spaces\n",
    "    \n",
    "    if return_as_list:\n",
    "        word_list = cleaned_text.split()\n",
    "        return word_list\n",
    "    else:\n",
    "        return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_clean_text = pt.rewrite.tokenise(lambda query: clean_text_with_dollar_signs(query, return_as_list=True, keep_dollar_signs=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remvoe stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure NLTK data directory is set correctly\n",
    "nltk.data.path.append(\"/usr/local/nltk_data\")\n",
    "\n",
    "# Download 'stopwords' corpus to the specified directory\n",
    "nltk.download('stopwords', download_dir=\"/usr/nltk_data\")\n",
    "\n",
    "# Get the set of stopwords for the English language\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_with_dollar_signs(text, return_as_list=False):\n",
    "    words = text.split()\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if '$$' in word:\n",
    "            parts = word.split('$$')\n",
    "            skip_word = False\n",
    "            for part in parts:\n",
    "                if part.lower() in stop_words:\n",
    "                    skip_word = True\n",
    "                    break  # If any part is a stopword, skip the entire word\n",
    "            if not skip_word:\n",
    "                filtered_words.append(word)\n",
    "        else:\n",
    "            if word.lower() not in stop_words:\n",
    "                filtered_words.append(word)\n",
    "    \n",
    "    if return_as_list:\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_remove_stopwords = pt.rewrite.tokenise(lambda query: remove_stopwords_with_dollar_signs(query, return_as_list=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text_with_dollar_signs(text, return_as_list=False):\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        if '$$' in word:\n",
    "            parts = word.split('$$')\n",
    "            stemmed_parts = [stemmer.stem(part) for part in parts]\n",
    "            stemmed_word = '$$'.join(stemmed_parts)\n",
    "        else:\n",
    "            stemmed_word = stemmer.stem(word)   # Stem the word normally\n",
    "        \n",
    "        stemmed_words.append(stemmed_word)\n",
    "\n",
    "    if return_as_list:\n",
    "        return stemmed_words\n",
    "    else:\n",
    "        return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_stem_text = pt.rewrite.tokenise(lambda query: stem_text_with_dollar_signs(query, return_as_list=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_factory = pt.IndexFactory.of(\"./ngramindex/data.properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This transformer will do the retrieval using bm25, and explicitly not apply any stemming and stopword removal\n",
    "bm25 = pt.BatchRetrieve(index_factory, wmodel=\"BM25\", verbose = True, properties={\"termpipelines\" : \"\"}, controls={\"bm25.b\": 0.2})#, \"bm25.k_1\": 0.1})\n",
    "\n",
    "# This is our retrieval pipeline\n",
    "retr_pipeline = transf_clean_text >> transf_remove_stopwords >> transf_stem_text >> bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25):   0%|          | 0/68 [00:00<?, ?q/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25): 100%|██████████| 68/68 [00:02<00:00, 24.28q/s]\n"
     ]
    }
   ],
   "source": [
    "run = retr_pipeline(df_expd) #queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"./runs/fullrun\".\n",
      "Done. run file is stored under \"./runs/fullrun/run.txt\".\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='ngrams', default_output='./runs/fullrun')\n",
    "\n",
    "#import os\n",
    "#os.rename('../runs/run.txt', '../runs/runngram.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
