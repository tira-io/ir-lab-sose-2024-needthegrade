{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import re\n",
    "import openai\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI() #connect to OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt(prompt, model=\"gpt-4\", temperature=0):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "querys = pt_dataset.get_topics('query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_integers_from_qid(df):\n",
    "    # Convert the qid column to integers\n",
    "    return df['qid'].astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_by_qid(df, qid):\n",
    "    # Convert the qid to string and find the corresponding query\n",
    "    result = df.loc[df['qid'] == str(qid), 'query']\n",
    "    return result.iloc[0] if not result.empty else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_query_by_qid(df, qid, new_query):\n",
    "    # Convert the qid to string and update the query\n",
    "    df.loc[df['qid'] == str(qid), 'query'] = new_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_list = extract_integers_from_qid(querys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_queries_list = querys.values.tolist()\n",
    "for i in qid_list:\n",
    "    \n",
    "\n",
    "    determine_ngrams = f\"\"\" \n",
    "    You are an scientific expert in the domain of Information Retrieval and linguistics. Your task is to detect whether\n",
    "    a given query, which is given as a text below delimited by triple quotes, contains bigrams. This means, you should check for all \n",
    "    bigrams in the query, if they are an existing term consisting of multiple words. Then, your answer should be the original query \n",
    "    with all the bigrams you found appended in the format word1$$word2. Your answer should only include the query and the bigrams, no additional information.\n",
    "    This means that when there are no existing bigrams in the query, your answer should just be the original query. You should not wrap your answer in quotation marks.\n",
    "    For example given a query 'usage of machine learning in image recognition' you should answer\n",
    "      'usage of machine learning in image recognition machine$$learning image$$recognition'.\n",
    "\n",
    "    query: '''{get_query_by_qid(querys, i)}'''\n",
    "    \"\"\"\n",
    "    answer = ask_gpt(prompt=determine_ngrams)\n",
    "    set_query_by_qid(querys, i, answer)\n",
    "\n",
    "expanded_queries_list = querys.values.tolist()\n",
    "print(expanded_queries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_expd = [(sublist[0], sublist[1]) for sublist in expanded_queries_list]\n",
    "print(data_expd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'retrieval system improving effectiveness', 'retrieval system improving effectiveness retrieval$$system'], ['2', 'machine learning language identification', 'machine learning language identification machine$$learning language$$identification'], ['3', 'social media detect self harm', 'social media detect self harm social$$media self$$harm'], ['4', 'stemming for arabic languages', 'stemming for arabic languages arabic$$languages'], ['5', 'audio based animal recognition', 'audio based animal recognition audio$$based animal$$recognition'], ['6', 'comparison different retrieval models', 'comparison different retrieval models retrieval$$models'], ['7', 'cache architecture', 'cache architecture'], ['8', 'document scoping formula', 'document scoping formula document$$scoping'], ['9', 'pseudo relevance feedback', 'pseudo relevance feedback pseudo$$relevance$$feedback pseudo$$relevance relevance$$feedback'], ['10', 'how to represent natural conversations in word nets', 'how to represent natural conversations in word nets natural$$conversations word$$nets'], ['11', 'algorithm acceleration with nvidia cuda', 'algorithm acceleration with nvidia cuda nvidia$$cuda'], ['12', 'mention of algorithm', 'mention of algorithm'], ['13', 'at least three authors', 'at least three authors three$$authors'], ['14', 'german domain', 'german domain german$$domain'], ['15', 'mention of open source', 'mention of open source open$$source'], ['16', 'inclusion of text mining', 'inclusion of text mining text$$mining'], ['17', 'the ethics of artificial intelligence', 'the ethics of artificial intelligence artificial$$intelligence'], ['19', 'machine learning for more relevant results', 'machine learning for more relevant results machine$$learning relevant$$results'], ['20', 'crawling websites using machine learning', 'crawling websites using machine learning machine$$learning'], ['21', 'recommenders influence on users', 'recommenders influence on users'], ['22', 'search engine caching effects', 'search engine caching effects search$$engine caching$$effects'], ['23', 'consumer product reviews', 'consumer product reviews consumer$$product product$$reviews'], ['24', 'limitations machine learning', 'limitations machine learning machine$$learning'], ['25', 'medicine related research', 'medicine related research medicine$$related related$$research'], ['26', 'natural language processing', 'natural language processing natural$$language$$processing natural$$language'], ['27', 'graph based ranking', 'graph based ranking graph$$based based$$ranking'], ['28', 'medical studies that use information retrieval', 'medical studies that use information retrieval information$$retrieval'], ['29', 'information retrieval on different language sources', 'information retrieval on different language sources information$$retrieval language$$sources'], ['30', 'papers that compare multiple information retrieval methods', 'papers that compare multiple information retrieval methods information$$retrieval'], ['31', 'risks of information retrieval in social media', 'risks of information retrieval in social media information$$retrieval social$$media'], ['32', 'actual experiments that strengthen theoretical knowledge', 'actual experiments that strengthen theoretical knowledge theoretical$$knowledge'], ['33', 'fake news detection', 'fake news detection fake$$news news$$detection'], ['34', 'multimedia retrieval', 'multimedia retrieval multimedia$$retrieval'], ['35', 'processing natural language for information retrieval', 'processing natural language for information retrieval natural$$language information$$retrieval'], ['36', 'recommendation systems', 'recommendation systems recommendation$$systems'], ['37', 'personalised search in e commerce', 'personalised search in e commerce personalised$$search e$$commerce'], ['38', 'sentiment analysis', 'sentiment analysis sentiment$$analysis'], ['39', 'informational retrieval using neural networks', 'informational retrieval using neural networks informational$$retrieval neural$$networks'], ['40', 'query log analysis', 'query log analysis query$$log log$$analysis'], ['41', 'entity recognition', 'entity recognition entity$$recognition'], ['42', 'relevance assessments', 'relevance assessments relevance$$assessments'], ['43', 'deep neural networks', 'deep neural networks deep$$neural neural$$networks'], ['44', 'information retrieval', 'information retrieval information$$retrieval'], ['45', 'analysis for android apps', 'analysis for android apps android$$apps'], ['46', 'the university of amsterdam', 'the university of amsterdam university$$of of$$amsterdam'], ['47', 'neural ranking for ecommerce product search', 'neural ranking for ecommerce product search neural$$ranking product$$search'], ['48', 'web pages evolution', 'web pages evolution web$$pages'], ['49', 'exhaustivity of index', 'exhaustivity of index'], ['50', 'query optimization', 'query optimization query$$optimization'], ['51', 'cosine similarity vector', 'cosine similarity vector cosine$$similarity'], ['52', 'reverse indexing', 'reverse indexing reverse$$indexing'], ['53', 'index compression techniques', 'index compression techniques index$$compression compression$$techniques'], ['54', 'search engine optimization with query logs', 'search engine optimization with query logs search$$engine engine$$optimization query$$logs'], ['55', 'bm25', 'bm25'], ['56', 'what makes natural language processing natural', 'what makes natural language processing natural natural$$language$$processing natural$$language'], ['57', 'principle of a information retrieval indexing', 'principle of a information retrieval indexing information$$retrieval'], ['58', 'architecture of web search engine', 'architecture of web search engine web$$search search$$engine'], ['59', 'what is ahp', 'what is ahp'], ['60', 'what is information retrieval', 'what is information retrieval information$$retrieval'], ['61', 'efficient retrieval algorithms', 'efficient retrieval algorithms retrieval$$algorithms'], ['62', 'how to avoid spam results', 'how to avoid spam results spam$$results'], ['63', 'information retrieval with algorithms', 'information retrieval with algorithms information$$retrieval'], ['64', 'misspellings in queries', 'misspellings in queries'], ['65', 'information in different language', 'information in different language different$$language'], ['66', 'abbreviations in queries', 'abbreviations in queries'], ['67', 'lemmatization algorithms', 'lemmatization algorithms'], ['68', 'filter ad rich documents', 'filter ad rich documents ad$$rich'], ['18', 'advancements in information retrieval', 'advancements in information retrieval information$$retrieval']]\n"
     ]
    }
   ],
   "source": [
    "expanded_queries = [['1', 'retrieval system improving effectiveness', 'retrieval system improving effectiveness retrieval$$system'], ['2', 'machine learning language identification', 'machine learning language identification machine$$learning language$$identification'], ['3', 'social media detect self harm', 'social media detect self harm social$$media self$$harm'], ['4', 'stemming for arabic languages', 'stemming for arabic languages arabic$$languages'], ['5', 'audio based animal recognition', 'audio based animal recognition audio$$based animal$$recognition'], ['6', 'comparison different retrieval models', 'comparison different retrieval models retrieval$$models'], ['7', 'cache architecture', 'cache architecture'], ['8', 'document scoping formula', 'document scoping formula document$$scoping'], ['9', 'pseudo relevance feedback', 'pseudo relevance feedback pseudo$$relevance$$feedback pseudo$$relevance relevance$$feedback'], ['10', 'how to represent natural conversations in word nets', 'how to represent natural conversations in word nets natural$$conversations word$$nets'], ['11', 'algorithm acceleration with nvidia cuda', 'algorithm acceleration with nvidia cuda nvidia$$cuda'], ['12', 'mention of algorithm', 'mention of algorithm'], ['13', 'at least three authors', 'at least three authors three$$authors'], ['14', 'german domain', 'german domain german$$domain'], ['15', 'mention of open source', 'mention of open source open$$source'], ['16', 'inclusion of text mining', 'inclusion of text mining text$$mining'], ['17', 'the ethics of artificial intelligence', 'the ethics of artificial intelligence artificial$$intelligence'], ['19', 'machine learning for more relevant results', 'machine learning for more relevant results machine$$learning relevant$$results'], ['20', 'crawling websites using machine learning', 'crawling websites using machine learning machine$$learning'], ['21', 'recommenders influence on users', 'recommenders influence on users'], ['22', 'search engine caching effects', 'search engine caching effects search$$engine caching$$effects'], ['23', 'consumer product reviews', 'consumer product reviews consumer$$product product$$reviews'], ['24', 'limitations machine learning', 'limitations machine learning machine$$learning'], ['25', 'medicine related research', 'medicine related research medicine$$related related$$research'], ['26', 'natural language processing', 'natural language processing natural$$language$$processing natural$$language'], ['27', 'graph based ranking', 'graph based ranking graph$$based based$$ranking'], ['28', 'medical studies that use information retrieval', 'medical studies that use information retrieval information$$retrieval'], ['29', 'information retrieval on different language sources', 'information retrieval on different language sources information$$retrieval language$$sources'], ['30', 'papers that compare multiple information retrieval methods', 'papers that compare multiple information retrieval methods information$$retrieval'], ['31', 'risks of information retrieval in social media', 'risks of information retrieval in social media information$$retrieval social$$media'], ['32', 'actual experiments that strengthen theoretical knowledge', 'actual experiments that strengthen theoretical knowledge theoretical$$knowledge'], ['33', 'fake news detection', 'fake news detection fake$$news news$$detection'], ['34', 'multimedia retrieval', 'multimedia retrieval multimedia$$retrieval'], ['35', 'processing natural language for information retrieval', 'processing natural language for information retrieval natural$$language information$$retrieval'], ['36', 'recommendation systems', 'recommendation systems recommendation$$systems'], ['37', 'personalised search in e commerce', 'personalised search in e commerce personalised$$search e$$commerce'], ['38', 'sentiment analysis', 'sentiment analysis sentiment$$analysis'], ['39', 'informational retrieval using neural networks', 'informational retrieval using neural networks informational$$retrieval neural$$networks'], ['40', 'query log analysis', 'query log analysis query$$log log$$analysis'], ['41', 'entity recognition', 'entity recognition entity$$recognition'], ['42', 'relevance assessments', 'relevance assessments relevance$$assessments'], ['43', 'deep neural networks', 'deep neural networks deep$$neural neural$$networks'], ['44', 'information retrieval', 'information retrieval information$$retrieval'], ['45', 'analysis for android apps', 'analysis for android apps android$$apps'], ['46', 'the university of amsterdam', 'the university of amsterdam university$$of of$$amsterdam'], ['47', 'neural ranking for ecommerce product search', 'neural ranking for ecommerce product search neural$$ranking product$$search'], ['48', 'web pages evolution', 'web pages evolution web$$pages'], ['49', 'exhaustivity of index', 'exhaustivity of index'], ['50', 'query optimization', 'query optimization query$$optimization'], ['51', 'cosine similarity vector', 'cosine similarity vector cosine$$similarity'], ['52', 'reverse indexing', 'reverse indexing reverse$$indexing'], ['53', 'index compression techniques', 'index compression techniques index$$compression compression$$techniques'], ['54', 'search engine optimization with query logs', 'search engine optimization with query logs search$$engine engine$$optimization query$$logs'], ['55', 'bm25', 'bm25'], ['56', 'what makes natural language processing natural', 'what makes natural language processing natural natural$$language$$processing natural$$language'], ['57', 'principle of a information retrieval indexing', 'principle of a information retrieval indexing information$$retrieval'], ['58', 'architecture of web search engine', 'architecture of web search engine web$$search search$$engine'], ['59', 'what is ahp', 'what is ahp'], ['60', 'what is information retrieval', 'what is information retrieval information$$retrieval'], ['61', 'efficient retrieval algorithms', 'efficient retrieval algorithms retrieval$$algorithms'], ['62', 'how to avoid spam results', 'how to avoid spam results spam$$results'], ['63', 'information retrieval with algorithms', 'information retrieval with algorithms information$$retrieval'], ['64', 'misspellings in queries', 'misspellings in queries'], ['65', 'information in different language', 'information in different language different$$language'], ['66', 'abbreviations in queries', 'abbreviations in queries'], ['67', 'lemmatization algorithms', 'lemmatization algorithms'], ['68', 'filter ad rich documents', 'filter ad rich documents ad$$rich'], ['18', 'advancements in information retrieval', 'advancements in information retrieval information$$retrieval']]\n",
    "print(expanded_queries)\n",
    "\n",
    "data_expd = [(sublist[0], sublist[2]) for sublist in expanded_queries] # Extract qid and query from each sublist\n",
    "\n",
    "\n",
    "df_expd = pd.DataFrame(data_expd, columns=['qid', 'query']) # Create pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>62</td>\n",
       "      <td>how to avoid spam results spam$$results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>63</td>\n",
       "      <td>information retrieval with algorithms informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>64</td>\n",
       "      <td>misspellings in queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>65</td>\n",
       "      <td>information in different language different$$l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>66</td>\n",
       "      <td>abbreviations in queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>67</td>\n",
       "      <td>lemmatization algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>68</td>\n",
       "      <td>filter ad rich documents ad$$rich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>18</td>\n",
       "      <td>advancements in information retrieval informat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid                                              query\n",
       "60  62            how to avoid spam results spam$$results\n",
       "61  63  information retrieval with algorithms informat...\n",
       "62  64                            misspellings in queries\n",
       "63  65  information in different language different$$l...\n",
       "64  66                           abbreviations in queries\n",
       "65  67                           lemmatization algorithms\n",
       "66  68                  filter ad rich documents ad$$rich\n",
       "67  18  advancements in information retrieval informat..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_expd[60:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_with_dollar_signs(text, return_as_list=False, keep_dollar_signs=False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    if keep_dollar_signs:      \n",
    "        text = text.replace('$$', 'DOUBLEDOLLARNGRAMS')                 # Replace double dollar signs with a unique placeholder\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)             # Remove all non-alphanumeric characters except spaces    \n",
    "        cleaned_text = cleaned_text.replace('DOUBLEDOLLARNGRAMS', '$$') # Replace placeholder back to double dollar signs\n",
    "    else:\n",
    "        \n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)             # Remove all non-alphanumeric characters except spaces\n",
    "    \n",
    "    if return_as_list:\n",
    "        word_list = cleaned_text.split()\n",
    "        return word_list\n",
    "    else:\n",
    "        return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_clean_text = pt.rewrite.tokenise(lambda query: clean_text_with_dollar_signs(query, return_as_list=True, keep_dollar_signs=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remvoe stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure NLTK data directory is set correctly\n",
    "nltk.data.path.append(\"/usr/local/nltk_data\")\n",
    "\n",
    "# Download 'stopwords' corpus to the specified directory\n",
    "nltk.download('stopwords', download_dir=\"/usr/nltk_data\")\n",
    "\n",
    "# Get the set of stopwords for the English language\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_with_dollar_signs(text, return_as_list=False):\n",
    "    words = text.split()\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if '$$' in word:\n",
    "            parts = word.split('$$')\n",
    "            skip_word = False\n",
    "            for part in parts:\n",
    "                if part.lower() in stop_words:\n",
    "                    skip_word = True\n",
    "                    break  # If any part is a stopword, skip the entire word\n",
    "            if not skip_word:\n",
    "                filtered_words.append(word)\n",
    "        else:\n",
    "            if word.lower() not in stop_words:\n",
    "                filtered_words.append(word)\n",
    "    \n",
    "    if return_as_list:\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_remove_stopwords = pt.rewrite.tokenise(lambda query: remove_stopwords_with_dollar_signs(query, return_as_list=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text_with_dollar_signs(text, return_as_list=False):\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        if '$$' in word:\n",
    "            parts = word.split('$$')\n",
    "            stemmed_parts = [stemmer.stem(part) for part in parts]\n",
    "            stemmed_word = '$$'.join(stemmed_parts)\n",
    "        else:\n",
    "            stemmed_word = stemmer.stem(word)   # Stem the word normally\n",
    "        \n",
    "        stemmed_words.append(stemmed_word)\n",
    "\n",
    "    if return_as_list:\n",
    "        return stemmed_words\n",
    "    else:\n",
    "        return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_stem_text = pt.rewrite.tokenise(lambda query: stem_text_with_dollar_signs(query, return_as_list=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_factory = pt.IndexFactory.of(\"./ngramindex/data.properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This transformer will do the retrieval using bm25, and explicitly not apply any stemming and stopword removal\n",
    "bm25 = pt.BatchRetrieve(index_factory, wmodel=\"BM25\", verbose = True, properties={\"termpipelines\" : \"\"}, controls={\"bm25.b\": 0.2})#, \"bm25.k_1\": 0.1})\n",
    "\n",
    "# This is our retrieval pipeline\n",
    "retr_pipeline = transf_clean_text >> transf_remove_stopwords >> transf_stem_text >> bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25):  15%|█▍        | 10/68 [00:00<00:01, 42.46q/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25): 100%|██████████| 68/68 [00:01<00:00, 49.19q/s]\n"
     ]
    }
   ],
   "source": [
    "run = retr_pipeline(df_expd) #queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"./runs/\".\n",
      "Done. run file is stored under \"./runs//run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='ngrams', default_output='./runs/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
