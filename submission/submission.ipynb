{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import re\n",
    "import openai\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 4.00kiB [00:00, 14.0MiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_runs/ir-lab-sose-2024/ir-acl-anthology-20240504-training/needthegrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "ir_query_interpretation = tira.pt.transform_queries('ir-lab-sose-2024/needthegrade/ir-query-interpretation', pt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/ir-acl-anthology-20240504-inputs.zip?download=1\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download:   0%|          | 0.00/39.4M [00:00<?, ?iB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 39.4M/39.4M [00:00<00:00, 45.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_datasets/ir-lab-sose-2024/ir-acl-anthology-20240504-training/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:03<00:00, 32125.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 126958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs =  pt_dataset.get_corpus_iter()\n",
    "docs = list(docs)\n",
    "count = sum(1 for _ in docs)\n",
    "docs = docs[:126959]\n",
    "print(\"Number of documents:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that removes all special characters from a String, and returns either a String or a list of all words\n",
    "def clean_text(text, return_as_list = False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text) #remove non-alphanumeric characters, except spaces\n",
    "    if return_as_list:\n",
    "        word_list = cleaned_text.split()\n",
    "        return word_list\n",
    "    else:\n",
    "        return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Ensure NLTK data directory is set correctly\n",
    "nltk.data.path.append(\"/usr/local/nltk_data\")\n",
    "\n",
    "# Download 'stopwords' corpus to the specified directory\n",
    "nltk.download('stopwords', download_dir=\"/usr/nltk_data\")\n",
    "\n",
    "# Get the set of stopwords for the English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text, return_as_list = False):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Join the filtered words back into a single string\n",
    "\n",
    "    if return_as_list:\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_text(text, return_as_list = False):\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    if return_as_list:\n",
    "        return stemmed_words\n",
    "    # Join the stemmed words back into a single string\n",
    "    else:\n",
    "        return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our ngram tokenizer. It takes a string and returns a dict of all ngrams, where each ngram is seperated by $$ so it will be parsed as one token\n",
    "\n",
    "def tokenize_ngrams_to_dict(text, n1=1, n2=2):\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text.split(' ')\n",
    "    words = [word for word in words if len(''.join(format(ord(c), '08b') for c in word)) <= 60]\n",
    "\n",
    "    # Initialize an empty Counter to hold all n-grams\n",
    "    all_ngram_counts = Counter()\n",
    "    \n",
    "    # Loop through each n from n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Update the Counter with the current n-grams\n",
    "        all_ngram_counts.update(ngrams)\n",
    "    \n",
    "    return dict(all_ngram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'docno': 'O02-2002', 'toks': {'studi': 1, 'word': 7, 'similar': 8, 'use': 3, 'context': 5, 'vector': 3, 'model': 2, 'need': 1, 'measur': 2, 'process': 1, 'natur': 1, 'languag': 1, 'especi': 1, 'general': 1, 'classif': 1, 'exampl': 1, 'base': 3, 'usual': 1, 'two': 1, 'defin': 1, 'accord': 3, 'distanc': 1, 'semant': 6, 'class': 2, 'less': 1, 'consid': 1, 'syntact': 5, 'ie': 2, 'howev': 1, 'real': 1, 'applic': 1, 'requir': 1, 'weight': 1, 'differ': 1, 'mixtur': 1, 'paper': 1, 'propos': 1, 'relat': 1, 'co': 2, 'occurr': 2, 'adopt': 1, 'inform': 1, 'theoret': 1, 'solv': 1, 'problem': 1, 'data': 1, 'spars': 1, 'precis': 1, 'featur': 2, 'deriv': 1, 'pars': 1, 'environ': 1, 'adjust': 1, 'idf': 1, 'invers': 1, 'valu': 2, 'agglom': 1, 'cluster': 1, 'appli': 1, 'group': 2, 'turn': 1, 'togeth': 1, 'studi$$word': 1, 'word$$similar': 4, 'similar$$use': 1, 'use$$context': 1, 'context$$vector': 3, 'vector$$model': 1, 'model$$need': 1, 'need$$measur': 1, 'measur$$word': 1, 'similar$$process': 1, 'process$$natur': 1, 'natur$$languag': 1, 'languag$$especi': 1, 'especi$$use': 1, 'use$$general': 1, 'general$$classif': 1, 'classif$$exampl': 1, 'exampl$$base': 1, 'base$$usual': 1, 'usual$$measur': 1, 'measur$$similar': 1, 'similar$$two': 1, 'two$$word': 1, 'word$$defin': 1, 'defin$$accord': 1, 'accord$$distanc': 1, 'distanc$$semant': 1, 'semant$$class': 2, 'class$$semant': 1, 'semant$$less': 1, 'less$$semant': 1, 'semant$$base': 1, 'base$$consid': 1, 'consid$$syntact': 1, 'syntact$$ie': 1, 'ie$$howev': 1, 'howev$$real': 1, 'real$$applic': 1, 'applic$$semant': 1, 'semant$$syntact': 1, 'syntact$$similar': 1, 'similar$$requir': 1, 'requir$$weight': 1, 'weight$$differ': 1, 'differ$$word': 1, 'similar$$base': 1, 'base$$context': 1, 'vector$$mixtur': 1, 'mixtur$$syntact': 1, 'syntact$$semant': 2, 'semant$$ie': 1, 'ie$$paper': 1, 'paper$$propos': 1, 'propos$$use': 1, 'use$$syntact': 1, 'syntact$$relat': 1, 'relat$$co': 1, 'co$$occurr': 2, 'occurr$$context': 2, 'vector$$adopt': 1, 'adopt$$inform': 1, 'inform$$theoret': 1, 'theoret$$model': 1, 'model$$solv': 1, 'solv$$problem': 1, 'problem$$data': 1, 'data$$spars': 1, 'spars$$precis': 1, 'precis$$co': 1, 'context$$featur': 2, 'featur$$deriv': 1, 'deriv$$pars': 1, 'pars$$environ': 1, 'environ$$word': 1, 'word$$context': 1, 'featur$$adjust': 1, 'adjust$$accord': 1, 'accord$$idf': 1, 'idf$$invers': 1, 'invers$$valu': 1, 'valu$$agglom': 1, 'agglom$$cluster': 1, 'cluster$$appli': 1, 'appli$$group': 1, 'group$$similar': 1, 'similar$$word': 1, 'word$$accord': 1, 'accord$$similar': 1, 'similar$$valu': 1, 'valu$$turn': 1, 'turn$$word': 1, 'similar$$syntact': 1, 'class$$group': 1, 'group$$togeth': 1}}\n",
      "{'docno': 'L02-1310', 'toks': {'larg': 1, 'sens': 1, 'tag': 1, 'corpora': 1, 'larg$$sens': 1, 'sens$$tag': 1, 'tag$$corpora': 1}}\n",
      "{'docno': 'R13-1042', 'toks': {'use': 2, 'pairwis': 2, 'email': 6, 'classif': 2, 'thread': 6, 'task': 1, 'separ': 1, 'convers': 1, 'whose': 1, 'distort': 1, 'lost': 1, 'paper': 1, 'perform': 2, 'text': 4, 'similar': 5, 'measur': 1, 'non': 1, 'quot': 1, 'show': 1, 'content': 2, 'metric': 2, 'style': 1, 'class': 2, 'balanc': 1, 'set': 1, 'ii': 1, 'featur': 2, 'depend': 1, 'semant': 2, 'corpus': 4, 'still': 1, 'effect': 1, 'even': 1, 'control': 1, 'make': 1, 'avail': 1, 'enron': 2, 'newli': 1, 'extract': 1, '70': 1, '178': 1, 'use$$pairwis': 1, 'pairwis$$email': 1, 'email$$classif': 1, 'classif$$email': 1, 'email$$thread': 2, 'thread$$thread': 1, 'thread$$task': 1, 'task$$separ': 1, 'separ$$convers': 1, 'convers$$whose': 1, 'whose$$thread': 1, 'thread$$distort': 1, 'distort$$lost': 1, 'lost$$paper': 1, 'paper$$perform': 1, 'perform$$email': 1, 'thread$$pairwis': 1, 'pairwis$$classif': 1, 'classif$$use': 1, 'use$$text': 1, 'text$$similar': 3, 'similar$$measur': 1, 'measur$$non': 1, 'non$$quot': 1, 'quot$$text': 1, 'text$$email': 1, 'email$$show': 1, 'show$$content': 1, 'content$$text': 1, 'similar$$metric': 2, 'metric$$style': 1, 'style$$text': 1, 'metric$$class': 1, 'class$$balanc': 1, 'balanc$$class': 1, 'class$$set': 1, 'set$$ii': 1, 'ii$$featur': 1, 'featur$$perform': 1, 'perform$$depend': 1, 'depend$$semant': 1, 'semant$$similar': 2, 'similar$$corpus': 1, 'corpus$$content': 1, 'content$$featur': 1, 'featur$$still': 1, 'still$$effect': 1, 'effect$$even': 1, 'even$$control': 1, 'control$$semant': 1, 'similar$$make': 1, 'make$$avail': 1, 'avail$$enron': 1, 'enron$$thread': 1, 'thread$$corpus': 1, 'corpus$$newli': 1, 'newli$$extract': 1, 'extract$$corpus': 1, 'corpus$$70': 1, '70$$178': 1, '178$$thread': 1, 'thread$$email': 1, 'email$$enron': 1, 'enron$$email': 1, 'email$$corpus': 1}}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "        if 'text' in doc:\n",
    "            doc['text'] = clean_text(doc['text'])\n",
    "            doc['text'] = remove_stopwords(doc['text'])\n",
    "            doc['text'] = stem_text(doc['text'])\n",
    "            \n",
    "            doc_1gram = tokenize_ngrams_to_dict(doc['text'], n1=1, n2=2) # Apply n-gram tokenization to the dataset\n",
    "\n",
    "            doc['toks'] = doc_1gram # create new toks field for tokenfrequency\n",
    "            del doc['text']  #This will delete the 'text' field from the documents\n",
    "    \n",
    "for i, doc in enumerate(docs):\n",
    "     if i == 3:\n",
    "           break\n",
    "     print(doc)\n",
    "\n",
    "#remove all empty documents\n",
    "docs = [d for d in docs if any(k != '' for k in d['toks'].keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 126824\n",
      "Number of terms: 1735213\n",
      "Number of postings: 11740755\n",
      "Number of fields: 0\n",
      "Number of tokens: 14688164\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n",
      "0\n",
      "9406\n",
      "0$$0\n",
      "84\n",
      "0$$00\n",
      "11\n",
      "0$$000\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Initialize the IterDictIndexer with pretokenised set to True\n",
    "iter_indexer = pt.IterDictIndexer(\"./ngramindex\", overwrite=True, meta={'docno': 35}, pretokenised=True, verbose = True, type = pt.index.IndexingType.SINGLEPASS)\n",
    "\n",
    "# Index our pretokenized documents\n",
    "index_ref = iter_indexer.index(docs)\n",
    "\n",
    "index_ngram = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "#Print some stats about our index\n",
    "print(index_ngram.getCollectionStatistics())\n",
    "\n",
    "# Access the MetaIndex and Lexicon\n",
    "meta = index_ngram.getMetaIndex()\n",
    "lexicon = index_ngram.getLexicon()\n",
    "\n",
    "\n",
    "i = 0\n",
    "for term, le in index_ngram.getLexicon():\n",
    "    i = i+1\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(term) \n",
    "    print(le.getFrequency())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABBREVIATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_with_dollar_signs(text, return_as_list=False, keep_dollar_signs=False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    if keep_dollar_signs:      \n",
    "        text = text.replace('$$', 'DOUBLEDOLLARNGRAMS')                 # Replace double dollar signs with a unique placeholder\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)             # Remove all non-alphanumeric characters except spaces    \n",
    "        cleaned_text = cleaned_text.replace('DOUBLEDOLLARNGRAMS', '$$') # Replace placeholder back to double dollar signs\n",
    "    else:\n",
    "        \n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)             # Remove all non-alphanumeric characters except spaces\n",
    "    \n",
    "    if return_as_list:\n",
    "        word_list = cleaned_text.split()\n",
    "        return word_list\n",
    "    else:\n",
    "        return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_clean_text = pt.rewrite.tokenise(lambda query: clean_text_with_dollar_signs(query, return_as_list=True, keep_dollar_signs=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remvoe stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure NLTK data directory is set correctly\n",
    "nltk.data.path.append(\"/usr/local/nltk_data\")\n",
    "\n",
    "# Download 'stopwords' corpus to the specified directory\n",
    "nltk.download('stopwords', download_dir=\"/usr/nltk_data\")\n",
    "\n",
    "# Get the set of stopwords for the English language\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_with_dollar_signs(text, return_as_list=False):\n",
    "    words = text.split()\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if '$$' in word:\n",
    "            parts = word.split('$$')\n",
    "            skip_word = False\n",
    "            for part in parts:\n",
    "                if part.lower() in stop_words:\n",
    "                    skip_word = True\n",
    "                    break  # If any part is a stopword, skip the entire word\n",
    "            if not skip_word:\n",
    "                filtered_words.append(word)\n",
    "        else:\n",
    "            if word.lower() not in stop_words:\n",
    "                filtered_words.append(word)\n",
    "    \n",
    "    if return_as_list:\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_remove_stopwords = pt.rewrite.tokenise(lambda query: remove_stopwords_with_dollar_signs(query, return_as_list=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text_with_dollar_signs(text, return_as_list=False):\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        if '$$' in word:\n",
    "            parts = word.split('$$')\n",
    "            stemmed_parts = [stemmer.stem(part) for part in parts]\n",
    "            stemmed_word = '$$'.join(stemmed_parts)\n",
    "        else:\n",
    "            stemmed_word = stemmer.stem(word)   # Stem the word normally\n",
    "        \n",
    "        stemmed_words.append(stemmed_word)\n",
    "\n",
    "    if return_as_list:\n",
    "        return stemmed_words\n",
    "    else:\n",
    "        return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_stem_text = pt.rewrite.tokenise(lambda query: stem_text_with_dollar_signs(query, return_as_list=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_factory = pt.IndexFactory.of(\"./ngramindex/data.properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This transformer will do the retrieval using bm25, and explicitly not apply any stemming and stopword removal\n",
    "bm25 = pt.BatchRetrieve(index_factory, wmodel=\"BM25\", verbose = True, properties={\"termpipelines\" : \"\"}, controls={\"bm25.b\": 0.2})#, \"bm25.k_1\": 0.1})\n",
    "\n",
    "# This is our retrieval pipeline\n",
    "retr_pipeline = transf_clean_text >> transf_remove_stopwords >> transf_stem_text >> bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/ir-acl-anthology-20240504-truth.zip?download=1\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 29.6k/29.6k [00:00<00:00, 1.44MiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_datasets/ir-lab-sose-2024/ir-acl-anthology-20240504-training/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25): 100%|██████████| 68/68 [00:02<00:00, 27.42q/s]\n"
     ]
    }
   ],
   "source": [
    "run = retr_pipeline(ir_query_interpretation(pt_dataset.get_topics())) #queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>query_2</th>\n",
       "      <th>description</th>\n",
       "      <th>narrative</th>\n",
       "      <th>original_query</th>\n",
       "      <th>is_abbreviations</th>\n",
       "      <th>expanded_abbreviation</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>query_1</th>\n",
       "      <th>query_0</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>94795</td>\n",
       "      <td>2004.cikm_conference-2004.47</td>\n",
       "      <td>0</td>\n",
       "      <td>16.287685</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>retrieval system improving effectiveness retri...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>124670</td>\n",
       "      <td>2006.ipm_journal-ir0volumeA42A3.2</td>\n",
       "      <td>1</td>\n",
       "      <td>15.362926</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>retrieval system improving effectiveness retri...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>83395</td>\n",
       "      <td>1997.sigirconf_conference-97.36</td>\n",
       "      <td>2</td>\n",
       "      <td>15.220951</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>retrieval system improving effectiveness retri...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>81597</td>\n",
       "      <td>2018.sigirconf_conference-2018.234</td>\n",
       "      <td>3</td>\n",
       "      <td>15.035347</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>retrieval system improving effectiveness retri...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>82044</td>\n",
       "      <td>2007.sigirconf_conference-2007.212</td>\n",
       "      <td>4</td>\n",
       "      <td>15.028518</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>retrieval system improving effectiveness retri...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>82438</td>\n",
       "      <td>1998.sigirconf_conference-98.39</td>\n",
       "      <td>5</td>\n",
       "      <td>15.008899</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>retrieval system improving effectiveness retri...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>122357</td>\n",
       "      <td>2010.sigirjournals_journal-ir0volu</td>\n",
       "      <td>6</td>\n",
       "      <td>14.928209</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>retrieval system improving effectiveness retri...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>125684</td>\n",
       "      <td>2005.ipm_journal-ir0volumeA41A5.11</td>\n",
       "      <td>7</td>\n",
       "      <td>14.908299</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>retrieval system improving effectiveness retri...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>84816</td>\n",
       "      <td>2016.ntcir_conference-2016.90</td>\n",
       "      <td>8</td>\n",
       "      <td>14.725026</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>retrieval system improving effectiveness retri...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>94352</td>\n",
       "      <td>2008.cikm_conference-2008.183</td>\n",
       "      <td>9</td>\n",
       "      <td>14.694944</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>retrieval system improving effectiveness retri...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                               docno  rank      score  \\\n",
       "0   1   94795        2004.cikm_conference-2004.47     0  16.287685   \n",
       "1   1  124670   2006.ipm_journal-ir0volumeA42A3.2     1  15.362926   \n",
       "2   1   83395     1997.sigirconf_conference-97.36     2  15.220951   \n",
       "3   1   81597  2018.sigirconf_conference-2018.234     3  15.035347   \n",
       "4   1   82044  2007.sigirconf_conference-2007.212     4  15.028518   \n",
       "5   1   82438     1998.sigirconf_conference-98.39     5  15.008899   \n",
       "6   1  122357  2010.sigirjournals_journal-ir0volu     6  14.928209   \n",
       "7   1  125684  2005.ipm_journal-ir0volumeA41A5.11     7  14.908299   \n",
       "8   1   84816       2016.ntcir_conference-2016.90     8  14.725026   \n",
       "9   1   94352       2008.cikm_conference-2008.183     9  14.694944   \n",
       "\n",
       "                                       text  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                      title  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                    query_2  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                         description  \\\n",
       "0  What papers focus on improving the effectivene...   \n",
       "1  What papers focus on improving the effectivene...   \n",
       "2  What papers focus on improving the effectivene...   \n",
       "3  What papers focus on improving the effectivene...   \n",
       "4  What papers focus on improving the effectivene...   \n",
       "5  What papers focus on improving the effectivene...   \n",
       "6  What papers focus on improving the effectivene...   \n",
       "7  What papers focus on improving the effectivene...   \n",
       "8  What papers focus on improving the effectivene...   \n",
       "9  What papers focus on improving the effectivene...   \n",
       "\n",
       "                                           narrative  \\\n",
       "0  Relevant papers include research on what makes...   \n",
       "1  Relevant papers include research on what makes...   \n",
       "2  Relevant papers include research on what makes...   \n",
       "3  Relevant papers include research on what makes...   \n",
       "4  Relevant papers include research on what makes...   \n",
       "5  Relevant papers include research on what makes...   \n",
       "6  Relevant papers include research on what makes...   \n",
       "7  Relevant papers include research on what makes...   \n",
       "8  Relevant papers include research on what makes...   \n",
       "9  Relevant papers include research on what makes...   \n",
       "\n",
       "                             original_query  is_abbreviations  \\\n",
       "0  retrieval system improving effectiveness             False   \n",
       "1  retrieval system improving effectiveness             False   \n",
       "2  retrieval system improving effectiveness             False   \n",
       "3  retrieval system improving effectiveness             False   \n",
       "4  retrieval system improving effectiveness             False   \n",
       "5  retrieval system improving effectiveness             False   \n",
       "6  retrieval system improving effectiveness             False   \n",
       "7  retrieval system improving effectiveness             False   \n",
       "8  retrieval system improving effectiveness             False   \n",
       "9  retrieval system improving effectiveness             False   \n",
       "\n",
       "  expanded_abbreviation                                             ngrams  \\\n",
       "0                  None  retrieval system improving effectiveness retri...   \n",
       "1                  None  retrieval system improving effectiveness retri...   \n",
       "2                  None  retrieval system improving effectiveness retri...   \n",
       "3                  None  retrieval system improving effectiveness retri...   \n",
       "4                  None  retrieval system improving effectiveness retri...   \n",
       "5                  None  retrieval system improving effectiveness retri...   \n",
       "6                  None  retrieval system improving effectiveness retri...   \n",
       "7                  None  retrieval system improving effectiveness retri...   \n",
       "8                  None  retrieval system improving effectiveness retri...   \n",
       "9                  None  retrieval system improving effectiveness retri...   \n",
       "\n",
       "                                    query_1  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                    query_0                         query  \n",
       "0  retrieval system improving effectiveness  retriev system improv effect  \n",
       "1  retrieval system improving effectiveness  retriev system improv effect  \n",
       "2  retrieval system improving effectiveness  retriev system improv effect  \n",
       "3  retrieval system improving effectiveness  retriev system improv effect  \n",
       "4  retrieval system improving effectiveness  retriev system improv effect  \n",
       "5  retrieval system improving effectiveness  retriev system improv effect  \n",
       "6  retrieval system improving effectiveness  retriev system improv effect  \n",
       "7  retrieval system improving effectiveness  retriev system improv effect  \n",
       "8  retrieval system improving effectiveness  retriev system improv effect  \n",
       "9  retrieval system improving effectiveness  retriev system improv effect  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"./runs/fullrun/submission\".\n",
      "Done. run file is stored under \"./runs/fullrun/submission/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='ngrams', default_output='./runs/fullrun/submission')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
