{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing complete: <org.terrier.querying.IndexRef at 0x7bc84235bf10 jclass=org/terrier/querying/IndexRef jself=<LocalRef obj=0x59d3357a0cb8 at 0x7bc843bb49f0>>\n",
      "Number of documents: 2\n",
      "Number of terms: 4\n",
      "Number of postings: 4\n",
      "Number of fields: 0\n",
      "Number of tokens: 4\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n",
      "brown fox\n",
      "1\n",
      "do goldfish\n",
      "1\n",
      "goldfish grow\n",
      "1\n",
      "quick brown\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize PyTerrier\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    {'docno': 'd1', 'text': 'do goldfish grow'}, #fragzeicehn rausparsen vorher\n",
    "    {'docno': 'd2', 'text': 'quick brown fox'}\n",
    "]\n",
    "\n",
    "# Function to tokenize text into n-grams\n",
    "def tokenize_ngrams(text, n=2):\n",
    "    tokens = text.split()\n",
    "    ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return ngrams\n",
    "\n",
    "# Apply n-gram tokenization to the dataset\n",
    "for doc in dataset:\n",
    "    ngram_tokens = tokenize_ngrams(doc['text'], n=2)\n",
    "    token_counts = Counter(ngram_tokens)\n",
    "    doc['toks'] = token_counts\n",
    "    del doc['text']  # Remove the 'text' field as it's not needed anymore\n",
    "\n",
    "# Initialize the IterDictIndexer with pretokenised set to True\n",
    "iter_indexer = pt.IterDictIndexer(\"./pretokindex\", meta={'docno': 20}, pretokenised=True)\n",
    "index_ref = iter_indexer.index(dataset)\n",
    "print(f\"Indexing complete: {index_ref}\")\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "print(index.getCollectionStatistics())\n",
    "for term, le in index.getLexicon():\n",
    "    print(term) \n",
    "    print(le.getFrequency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indexref' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m topics\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize the BatchRetrieve with the indexed reference and BM25\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m bm25 \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mBatchRetrieve(\u001b[43mindexref\u001b[49m, wmodel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBM25\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Create a query pipeline that includes the n-gram tokenization in matchop format\u001b[39;00m\n\u001b[1;32m     20\u001b[0m matchop_tokenize \u001b[38;5;241m=\u001b[39m MatchopQueryTransform(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'indexref' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to convert n-grams into matchop query language\n",
    "def ngram_to_matchop(query, n=2):\n",
    "    ngrams = tokenize_ngrams(query, n)\n",
    "    matchop_query = ' '.join([f'#od1({ngram})' for ngram in ngrams])\n",
    "    return matchop_query\n",
    "\n",
    "# Define the MatchopQueryTransform transformer\n",
    "class MatchopQueryTransform(pt.Transformer):\n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "\n",
    "    def transform(self, topics):\n",
    "        topics['query'] = topics['query'].apply(lambda x: ngram_to_matchop(x, self.n))\n",
    "        return topics\n",
    "\n",
    "# Initialize the BatchRetrieve with the indexed reference and BM25\n",
    "bm25 = pt.BatchRetrieve(indexref, wmodel=\"BM25\")\n",
    "\n",
    "# Create a query pipeline that includes the n-gram tokenization in matchop format\n",
    "matchop_tokenize = MatchopQueryTransform(n=1)\n",
    "query_toks = pt.apply.query(lambda row: ngram_to_matchop(row['query'], n=2))\n",
    "retr_pipe = query_toks >> bm25\n",
    "\n",
    "# Sample queries\n",
    "queries = pd.DataFrame([\n",
    "    {'qid': 'q1', 'query': 'do goldfish grow'},\n",
    "    {'qid': 'q2', 'query': 'quick brown fox'}\n",
    "])\n",
    "\n",
    "# Retrieve results\n",
    "run = retr_pipe.transform(queries)\n",
    "print(run.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [docid, docno, rank, score, qid, query]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "query = '\"do goldfish\"'\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "run = bm25.search(query)\n",
    "print(run.head(10))\n",
    "# Retrieve documents\n",
    "#results = index.search(query)\n",
    "\n",
    "# Print retrieved documents\n",
    "#for doc in results:\n",
    " #   print(\"Document:\", doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset and the Index\n",
    "\n",
    "The type of the index object that we load is `<class 'jnius.reflect.org.terrier.structures.Index'>`, in fact a [Java class](http://terrier.org/docs/v3.6/javadoc/org/terrier/structures/Index.html) wrapped into Python. However, you do not need to worry about this: at this point, we will simply use the provided Index object to run procedures defined in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom tokenizer method.\n",
    "This method takes a String as an argument and returns a List of all tokens, including ngrams between 1 and 3 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Iterator for our dataset. This is an Iterator where every element is a dict which contains a text and a docno.\n",
    "See the example print below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Retrieval Pipeline\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ValueError() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNow we do the retrieval...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#run = bm25(test_dataset.get_topics('text'))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#run = bm25([{\"qid\": \"0\", \"query\" : \"retrieval systems\"}])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print('Done. Here are the first 10 entries of the run')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a query pipeline that includes the n-gram tokenization\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#ngram_tokenize = NgramTokenizeTransform(n=2)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m retr_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mngram_tokenize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbm25\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Sample queries\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/transformer.py:232\u001b[0m, in \u001b[0;36mTransformer.__rrshift__\u001b[0;34m(self, left)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rrshift__\u001b[39m(\u001b[38;5;28mself\u001b[39m, left) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComposedPipeline\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mComposedPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matchpy/expressions/expressions.py:284\u001b[0m, in \u001b[0;36m_OperationMeta.__call__\u001b[0;34m(cls, variable_name, *operands)\u001b[0m\n\u001b[1;32m    282\u001b[0m operation \u001b[38;5;241m=\u001b[39m Expression\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39munpacked_args_to_init:\n\u001b[0;32m--> 284\u001b[0m     \u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     operation\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39moperands, variable_name\u001b[38;5;241m=\u001b[39mvariable_name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:29\u001b[0m, in \u001b[0;36mNAryTransformerBase.__init__\u001b[0;34m(self, operands, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(operands\u001b[38;5;241m=\u001b[39moperands, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     28\u001b[0m models \u001b[38;5;241m=\u001b[39m operands\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:29\u001b[0m, in \u001b[0;36mNAryTransformerBase.__init__.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(operands\u001b[38;5;241m=\u001b[39moperands, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     28\u001b[0m models \u001b[38;5;241m=\u001b[39m operands\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m( \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[43mget_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m, models) )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/transformer.py:42\u001b[0m, in \u001b[0;36mget_transformer\u001b[0;34m(v, stacklevel)\u001b[0m\n\u001b[1;32m     40\u001b[0m     warn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoercion of a dataframe into a transformer is deprecated; use a pt.Transformer.from_df() instead\u001b[39m\u001b[38;5;124m'\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SourceTransformer(v)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;43;01mValueError\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPassed parameter \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m of type \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m cannot be coerced into a transformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;167;43;01mDeprecationWarning\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: ValueError() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "#run = bm25(test_dataset.get_topics('text'))\n",
    "#run = bm25([{\"qid\": \"0\", \"query\" : \"retrieval systems\"}])\n",
    "#print('Done. Here are the first 10 entries of the run')\n",
    "#run.head(10)\n",
    "\n",
    "\n",
    "# tokenize ngram the queries\n",
    "# Create a query pipeline that includes the n-gram tokenization\n",
    "#ngram_tokenize = NgramTokenizeTransform(n=2)\n",
    "retr_pipe = ngram_tokenize >> bm25\n",
    "\n",
    "import pandas as pd\n",
    "# Sample queries\n",
    "queries = pd.DataFrame([\n",
    "    {'qid': 'q1', 'query': 'do goldfish grow?'},\n",
    "    {'qid': 'q2', 'query': 'quick brown fox'}\n",
    "])\n",
    "\n",
    "# Retrieve results\n",
    "run = retr_pipe(queries)\n",
    "run.head(10)\n",
    "results = retr_pipe.transform(queries)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes for docid 0: {'docno': '0', 'text': ''}\n",
      "Attributes for docid 1: {'docno': '1', 'text': ''}\n",
      "Attributes for docid 2: {'docno': '2', 'text': ''}\n"
     ]
    }
   ],
   "source": [
    "#index = pt.IndexFactory.of(index_ref)\n",
    "meta = index.getMetaIndex()\n",
    "\n",
    "# List of metadata keys\n",
    "meta_keys = ['docno', 'text']  # Adjust this list based on your meta fields\n",
    "\n",
    "# Function to print document attributes\n",
    "def print_doc_attributes(docid):\n",
    "    attributes = {key: meta.getItem(key, docid) for key in meta_keys}\n",
    "    print(f\"Attributes for docid {docid}: {attributes}\")\n",
    "\n",
    "# Example: Print attributes for the first 5 documents\n",
    "for docid in range(3):\n",
    "    print_doc_attributes(docid)\n",
    "\n",
    "# Function to print document attributes by docno\n",
    "def print_doc_attributes_by_docno(docno):\n",
    "    try:\n",
    "        docid = meta.getDocument(\"docno\", docno)\n",
    "        print_doc_attributes(docid)\n",
    "    except KeyError:\n",
    "        print(f\"Document with docno {docno} not found.\")\n",
    "\n",
    "# List of specific docnos to retrieve\n",
    "#docnos = ['W05-0704']\n",
    "\n",
    "# Retrieve and print attributes for each specified docno\n",
    "#for docno in docnos:\n",
    " #   print_doc_attributes_by_docno(docno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
