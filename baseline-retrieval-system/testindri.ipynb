{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing complete: <org.terrier.querying.IndexRef at 0x7ab28e9147c0 jclass=org/terrier/querying/IndexRef jself=<LocalRef obj=0x611d04e01f80 at 0x7ab28e89fb50>>\n",
      "Number of documents: 5\n",
      "Number of terms: 6\n",
      "Number of postings: 14\n",
      "Number of fields: 0\n",
      "Number of tokens: 14\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor term, le in index.getLexicon():\\n    print(term) \\n    print(le.getFrequency())\\n    '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize PyTerrier\n",
    "if not pt.started():\n",
    "  pt.init()\n",
    "\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    {'docno': 'd1', 'text': 'do goldfish grow'}, #fragzeicehn rausparsen vorher\n",
    "    {'docno': 'd2', 'text': 'quick brown fox'},\n",
    "        {'docno': 'd3', 'text': 'quick white fox'},\n",
    "                {'docno': 'd4', 'text': 'white quick fox'}\n",
    "]\n",
    "\n",
    "#dataset2 = [[\"d1\", \"do goldfish grow\"], [\"d2\", \"quick brown fox\"], [\"brown quick fox\"], [\"d3\", \"quick white fox\"], [\"d4\", \"white quick fox\"]]\n",
    "docs_df = pd.DataFrame([\n",
    "    [\"d1\", \"do goldfish grow\"], \n",
    "    [\"d2\", \"quick brown fox\"], \n",
    "    [\"d3\", \"brown quick fox\"], \n",
    "    [\"d4\", \"quick white fox\"],\n",
    "    [\"d5\", \"white quick fox\"]\n",
    "    ], columns=[\"docno\", \"text\"])\n",
    "\n",
    "# Function to tokenize text into n-grams\n",
    "def tokenize_ngrams(text, n=2):\n",
    "    tokens = text.split()\n",
    "    ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return ngrams\n",
    "'''\n",
    "# Apply n-gram tokenization to the dataset\n",
    "for doc in dataset:\n",
    "    ngram_tokens1 = tokenize_ngrams(doc['text'], n=1)\n",
    "    ngram_tokens2 = tokenize_ngrams(doc['text'], n=2)\n",
    "    ngram_tokens = ngram_tokens1 + ngram_tokens2\n",
    "    token_counts = Counter(ngram_tokens)\n",
    "    doc['toks'] = token_counts\n",
    "    del doc['text']  # Remove the 'text' field as it's not needed anymore\n",
    "'''\n",
    "# Initialize the IterDictIndexer with pretokenised set to True\n",
    "indexer = pt.DFIndexer(\"./pretokindex\", overwrite=True) #blocks = true\n",
    "index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n",
    "print(f\"Indexing complete: {index_ref}\")\n",
    "index_ref.toString()\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "print(index.getCollectionStatistics())\n",
    "'''\n",
    "for term, le in index.getLexicon():\n",
    "    print(term) \n",
    "    print(le.getFrequency())\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m matchop_query\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define the MatchopQueryTransform transformer\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMatchopQueryTransform\u001b[39;00m(\u001b[43mpt\u001b[49m\u001b[38;5;241m.\u001b[39mTransformer):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m=\u001b[39m n\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to convert n-grams into matchop query language\n",
    "def ngram_to_matchop(query, n=2):\n",
    "    ngrams = tokenize_ngrams(query, n)\n",
    "    matchop_query = ' '.join([f'#od1({ngram})' for ngram in ngrams])\n",
    "    return matchop_query\n",
    "\n",
    "# Define the MatchopQueryTransform transformer\n",
    "class MatchopQueryTransform(pt.Transformer):\n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "\n",
    "    def transform(self, topics):\n",
    "        topics['query'] = topics['query'].apply(lambda x: ngram_to_matchop(x, self.n))\n",
    "        return topics\n",
    "\n",
    "# Initialize the BatchRetrieve with the indexed reference and BM25\n",
    "bm25 = pt.BatchRetrieve(indexref, wmodel=\"BM25\")\n",
    "\n",
    "# Create a query pipeline that includes the n-gram tokenization in matchop format\n",
    "matchop_tokenize = MatchopQueryTransform(n=1)\n",
    "query_toks = pt.apply.query(lambda row: ngram_to_matchop(row['query'], n=1))\n",
    "retr_pipe = query_toks >> bm25\n",
    "\n",
    "# Sample queries\n",
    "queries = pd.DataFrame([\n",
    "    {'qid': 'q1', 'query': 'do goldfish grow'},\n",
    "    {'qid': 'q2', 'query': 'quick brown fox'}\n",
    "])\n",
    "\n",
    "# Retrieve results\n",
    "run = retr_pipe.transform(queries)\n",
    "print(run.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:13:53.148 [main] ERROR org.terrier.structures.Index - This index (/workspaces/ir-lab-sose-2024-needthegrade/baseline-retrieval-system/pretokindex/data.properties) doesnt have an index structure called lexicon: property index.lexicon.class not found\n",
      "20:13:53.150 [main] ERROR org.terrier.structures.Index - Valid structures are: [meta-inputstream, lexicon-valuefactory, document, meta, direct, direct-inputstream, document-inputstream, document-factory, lexicon-keyfactory]\n",
      "20:13:53.152 [main] ERROR org.terrier.structures.Index - This index (/workspaces/ir-lab-sose-2024-needthegrade/baseline-retrieval-system/pretokindex/data.properties) doesnt have an index structure called inverted: property index.inverted.class not found\n",
      "20:13:53.153 [main] ERROR org.terrier.structures.Index - Valid structures are: [meta-inputstream, lexicon-valuefactory, document, meta, direct, direct-inputstream, document-inputstream, document-factory, lexicon-keyfactory]\n",
      "20:13:53.155 [main] ERROR org.terrier.structures.Index - This index (/workspaces/ir-lab-sose-2024-needthegrade/baseline-retrieval-system/pretokindex/data.properties) doesnt have an index structure called lexicon: property index.lexicon.class not found\n",
      "20:13:53.157 [main] ERROR org.terrier.structures.Index - Valid structures are: [meta-inputstream, lexicon-valuefactory, document, meta, direct, direct-inputstream, document-inputstream, document-factory, lexicon-keyfactory]\n",
      "20:13:53.158 [main] ERROR org.terrier.structures.Index - This index (/workspaces/ir-lab-sose-2024-needthegrade/baseline-retrieval-system/pretokindex/data.properties) doesnt have an index structure called inverted: property index.inverted.class not found\n",
      "20:13:53.160 [main] ERROR org.terrier.structures.Index - Valid structures are: [meta-inputstream, lexicon-valuefactory, document, meta, direct, direct-inputstream, document-inputstream, document-factory, lexicon-keyfactory]\n",
      "20:13:53.162 [main] ERROR org.terrier.structures.Index - This index (/workspaces/ir-lab-sose-2024-needthegrade/baseline-retrieval-system/pretokindex/data.properties) doesnt have an index structure called lexicon: property index.lexicon.class not found\n",
      "20:13:53.164 [main] ERROR org.terrier.structures.Index - Valid structures are: [meta-inputstream, lexicon-valuefactory, document, meta, direct, direct-inputstream, document-inputstream, document-factory, lexicon-keyfactory]\n"
     ]
    },
    {
     "ename": "JavaException",
     "evalue": "JVM exception occurred: java.lang.NullPointerException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJavaException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrow\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m bm25 \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mBatchRetrieve(index)\n\u001b[0;32m----> 3\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mbm25\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(run\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Retrieve documents\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#results = index.search(query)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Print retrieved documents\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#for doc in results:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m  \u001b[38;5;66;03m#   print(\"Document:\", doc)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/transformer.py:169\u001b[0m, in \u001b[0;36mTransformer.search\u001b[0;34m(self, query, qid, sort)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m    168\u001b[0m queryDf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([[qid, query]], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 169\u001b[0m rtr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueryDf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rtr\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rtr\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    171\u001b[0m     rtr \u001b[38;5;241m=\u001b[39m rtr\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mTrue\u001b[39;00m,\u001b[38;5;28;01mTrue\u001b[39;00m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/batchretrieve.py:441\u001b[0m, in \u001b[0;36mBatchRetrieve.transform\u001b[0;34m(self, queries)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28miter\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m), total\u001b[38;5;241m=\u001b[39mqueries\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m:\n\u001b[0;32m--> 441\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocno_provided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocno_provided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocid_provided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocid_provided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores_provided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores_provided\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m         results\u001b[38;5;241m.\u001b[39mextend(res)\n\u001b[1;32m    444\u001b[0m res_dt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocid\u001b[39m\u001b[38;5;124m'\u001b[39m ] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/batchretrieve.py:352\u001b[0m, in \u001b[0;36mBatchRetrieve._retrieve_one\u001b[0;34m(self, row, input_results, docno_provided, docid_provided, scores_provided)\u001b[0m\n\u001b[1;32m    349\u001b[0m     srq\u001b[38;5;241m.\u001b[39msetControl(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatching\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.terrier.matching.ScoringMatching\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m srq\u001b[38;5;241m.\u001b[39mgetControl(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatching\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# now ask Terrier to run the request\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunSearchRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m result \u001b[38;5;241m=\u001b[39m srq\u001b[38;5;241m.\u001b[39mgetResults()\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# check we got all of the expected metadata (if the resultset has a size at all)\u001b[39;00m\n",
      "File \u001b[0;32mjnius/jnius_export_class.pxi:877\u001b[0m, in \u001b[0;36mjnius.JavaMethod.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_export_class.pxi:971\u001b[0m, in \u001b[0;36mjnius.JavaMethod.call_method\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_utils.pxi:79\u001b[0m, in \u001b[0;36mjnius.check_exception\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mJavaException\u001b[0m: JVM exception occurred: java.lang.NullPointerException"
     ]
    }
   ],
   "source": [
    "query = 'grow'\n",
    "bm25 = pt.BatchRetrieve(index)\n",
    "run = bm25.search(query)\n",
    "print(run.head(10))\n",
    "# Retrieve documents\n",
    "#results = index.search(query)\n",
    "\n",
    "# Print retrieved documents\n",
    "#for doc in results:\n",
    " #   print(\"Document:\", doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset and the Index\n",
    "\n",
    "The type of the index object that we load is `<class 'jnius.reflect.org.terrier.structures.Index'>`, in fact a [Java class](http://terrier.org/docs/v3.6/javadoc/org/terrier/structures/Index.html) wrapped into Python. However, you do not need to worry about this: at this point, we will simply use the provided Index object to run procedures defined in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom tokenizer method.\n",
    "This method takes a String as an argument and returns a List of all tokens, including ngrams between 1 and 3 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Iterator for our dataset. This is an Iterator where every element is a dict which contains a text and a docno.\n",
    "See the example print below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Retrieval Pipeline\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ValueError() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNow we do the retrieval...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#run = bm25(test_dataset.get_topics('text'))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#run = bm25([{\"qid\": \"0\", \"query\" : \"retrieval systems\"}])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print('Done. Here are the first 10 entries of the run')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a query pipeline that includes the n-gram tokenization\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#ngram_tokenize = NgramTokenizeTransform(n=2)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m retr_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mngram_tokenize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbm25\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Sample queries\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/transformer.py:232\u001b[0m, in \u001b[0;36mTransformer.__rrshift__\u001b[0;34m(self, left)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rrshift__\u001b[39m(\u001b[38;5;28mself\u001b[39m, left) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComposedPipeline\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mComposedPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matchpy/expressions/expressions.py:284\u001b[0m, in \u001b[0;36m_OperationMeta.__call__\u001b[0;34m(cls, variable_name, *operands)\u001b[0m\n\u001b[1;32m    282\u001b[0m operation \u001b[38;5;241m=\u001b[39m Expression\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39munpacked_args_to_init:\n\u001b[0;32m--> 284\u001b[0m     \u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     operation\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39moperands, variable_name\u001b[38;5;241m=\u001b[39mvariable_name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:29\u001b[0m, in \u001b[0;36mNAryTransformerBase.__init__\u001b[0;34m(self, operands, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(operands\u001b[38;5;241m=\u001b[39moperands, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     28\u001b[0m models \u001b[38;5;241m=\u001b[39m operands\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:29\u001b[0m, in \u001b[0;36mNAryTransformerBase.__init__.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(operands\u001b[38;5;241m=\u001b[39moperands, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     28\u001b[0m models \u001b[38;5;241m=\u001b[39m operands\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m( \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[43mget_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m, models) )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/transformer.py:42\u001b[0m, in \u001b[0;36mget_transformer\u001b[0;34m(v, stacklevel)\u001b[0m\n\u001b[1;32m     40\u001b[0m     warn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoercion of a dataframe into a transformer is deprecated; use a pt.Transformer.from_df() instead\u001b[39m\u001b[38;5;124m'\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SourceTransformer(v)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;43;01mValueError\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPassed parameter \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m of type \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m cannot be coerced into a transformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;167;43;01mDeprecationWarning\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: ValueError() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "#run = bm25(test_dataset.get_topics('text'))\n",
    "#run = bm25([{\"qid\": \"0\", \"query\" : \"retrieval systems\"}])\n",
    "#print('Done. Here are the first 10 entries of the run')\n",
    "#run.head(10)\n",
    "\n",
    "\n",
    "# tokenize ngram the queries\n",
    "# Create a query pipeline that includes the n-gram tokenization\n",
    "#ngram_tokenize = NgramTokenizeTransform(n=2)\n",
    "retr_pipe = ngram_tokenize >> bm25\n",
    "\n",
    "import pandas as pd\n",
    "# Sample queries\n",
    "queries = pd.DataFrame([\n",
    "    {'qid': 'q1', 'query': 'do goldfish grow?'},\n",
    "    {'qid': 'q2', 'query': 'quick brown fox'}\n",
    "])\n",
    "\n",
    "# Retrieve results\n",
    "run = retr_pipe(queries)\n",
    "run.head(10)\n",
    "results = retr_pipe.transform(queries)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes for docid 0: {'docno': '0', 'text': ''}\n",
      "Attributes for docid 1: {'docno': '1', 'text': ''}\n",
      "Attributes for docid 2: {'docno': '2', 'text': ''}\n"
     ]
    }
   ],
   "source": [
    "#index = pt.IndexFactory.of(index_ref)\n",
    "meta = index.getMetaIndex()\n",
    "\n",
    "# List of metadata keys\n",
    "meta_keys = ['docno', 'text']  # Adjust this list based on your meta fields\n",
    "\n",
    "# Function to print document attributes\n",
    "def print_doc_attributes(docid):\n",
    "    attributes = {key: meta.getItem(key, docid) for key in meta_keys}\n",
    "    print(f\"Attributes for docid {docid}: {attributes}\")\n",
    "\n",
    "# Example: Print attributes for the first 5 documents\n",
    "for docid in range(3):\n",
    "    print_doc_attributes(docid)\n",
    "\n",
    "# Function to print document attributes by docno\n",
    "def print_doc_attributes_by_docno(docno):\n",
    "    try:\n",
    "        docid = meta.getDocument(\"docno\", docno)\n",
    "        print_doc_attributes(docid)\n",
    "    except KeyError:\n",
    "        print(f\"Document with docno {docno} not found.\")\n",
    "\n",
    "# List of specific docnos to retrieve\n",
    "#docnos = ['W05-0704']\n",
    "\n",
    "# Retrieve and print attributes for each specified docno\n",
    "#for docno in docnos:\n",
    " #   print_doc_attributes_by_docno(docno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
