{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing complete: <org.terrier.querying.IndexRef at 0x76c1f4e528e0 jclass=org/terrier/querying/IndexRef jself=<LocalRef obj=0x5ae7c75c8b28 at 0x76c1e2bb4530>>\n",
      "Number of documents: 3\n",
      "Number of terms: 8\n",
      "Number of postings: 8\n",
      "Number of fields: 0\n",
      "Number of tokens: 8\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n",
      "a$$brown\n",
      "1\n",
      "a$$quick\n",
      "1\n",
      "brown$$fox\n",
      "1\n",
      "brown$$quick\n",
      "1\n",
      "do$$goldfish\n",
      "1\n",
      "goldfish$$grow?\n",
      "1\n",
      "quick$$brown\n",
      "1\n",
      "quick$$fox\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import re\n",
    "# Initialize PyTerrier\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    {'docno': 'd1', 'text': 'do goldfish grow?'},\n",
    "    {'docno': 'd2', 'text': 'a quick brown fox'},\n",
    "    {'docno' : 'd3', 'text' : 'a brown quick fox'}\n",
    "]\n",
    "\n",
    "# Function to tokenize text into n-grams\n",
    "def tokenize_ngrams(text, n=2):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), token_pattern=r'\\b\\w+\\b')\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    ngrams = vectorizer.get_feature_names_out()\n",
    "    counts = X.toarray().flatten()\n",
    "    return dict(zip(ngrams, counts))\n",
    "\n",
    "def tokenize_ngrams_dollar_sign(text, n=2):\n",
    "    # Replace spaces with dollar signs\n",
    "    text_with_dollar_signs = re.sub(r'\\s+', '$', text)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text_with_dollar_signs.split('$')\n",
    "    \n",
    "    # Generate n-grams manually\n",
    "    ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    \n",
    "    # Count occurrences of each n-gram\n",
    "    ngram_counts = Counter(ngrams)\n",
    "    \n",
    "    return dict(ngram_counts)\n",
    "\n",
    "\n",
    "# Apply n-gram tokenization to the dataset\n",
    "for doc in dataset:\n",
    "    doc['toks'] = tokenize_ngrams_dollar_sign(doc['text'], n=2)\n",
    "    del doc['text']  # Remove the 'text' field as it's not needed anymore\n",
    "\n",
    "# Initialize the IterDictIndexer with pretokenised set to True\n",
    "iter_indexer = pt.IterDictIndexer(\"./pretokindex\", meta={'docno': 20}, pretokenised=True)\n",
    "\n",
    "# Index the pretokenized dataset\n",
    "index_ref = iter_indexer.index(dataset)\n",
    "\n",
    "print(f\"Indexing complete: {index_ref}\")\n",
    "# Now you can use the index_ref as usual\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "print(index.getCollectionStatistics())\n",
    "for term, le in index.getLexicon():\n",
    "    print(term) \n",
    "    print(le.getFrequency())\n",
    "\n",
    "# Access the MetaIndex and Lexicon\n",
    "meta = index.getMetaIndex()\n",
    "lexicon = index.getLexicon()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful method for getting all ngrams in a String, returns a list of the ngrams\n",
    "# Not currently used anywhere\n",
    "def tokenize_ngrams_dollar_sign_query(text, n=2):\n",
    "    tokens = text.split()\n",
    "    ngrams = ['$$'.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "['brown$$quick quick$$fox']\n",
      "  qid  docid docno  rank     score          query_0                    query\n",
      "0   1      2    d3     0  1.088135  brown quick fox  brown$$quick quick$$fox\n"
     ]
    }
   ],
   "source": [
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", verbose = True)\n",
    "\n",
    "#query for ngrams by typing '\"word1 word2\"' will only return documents where \"word1 word2\" appear in this constellation (document where they dont appear like this will not get ranked)\n",
    "#query = 'brown fox'\n",
    "query = 'brown quick fox'\n",
    "tokenise_query_ngram = pt.rewrite.tokenise(lambda query: tokenize_ngrams_dollar_sign_query(query))\n",
    "print()\n",
    "poortokenisation = tokenise_query_ngram >> pt.BatchRetrieve(index_ref)\n",
    "print()\n",
    "df = pd.DataFrame([{\"query\": query}])\n",
    "transformed_df = tokenise_query_ngram.transform(df)\n",
    "print(transformed_df[\"query\"].values)\n",
    "#TODO put multiple queries into list and run on all queries\n",
    "#TODO load queries from Tira\n",
    "#TODO add ngrams to the queries like shown, but only for real ngrams. perhaps use LLMs to find out which ngrams are real phrases from the english language and only put those\n",
    "# or perhaps just weight all the ngrams but dont make them necessary\n",
    "\n",
    "run = poortokenisation.search(query)\n",
    "print(run.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to tokenize query into n-grams\n",
    "def query_tokenize_ngrams(query, n=2):\n",
    "    return list(tokenize_ngrams(query, n=n).keys())\n",
    "\n",
    "# Define the retrieval pipeline\n",
    "class NgramTokenizeTransform:\n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "\n",
    "    def transform(self, queries):\n",
    "        def ngrams(text, n):\n",
    "            tokens = text.split()\n",
    "            return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        \n",
    "        queries['query'] = queries['query'].apply(lambda x: ' '.join(ngrams(x, self.n)))\n",
    "        return queries\n",
    "\n",
    "ngram_tokenize = NgramTokenizeTransform(n=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset and the Index\n",
    "\n",
    "The type of the index object that we load is `<class 'jnius.reflect.org.terrier.structures.Index'>`, in fact a [Java class](http://terrier.org/docs/v3.6/javadoc/org/terrier/structures/Index.html) wrapped into Python. However, you do not need to worry about this: at this point, we will simply use the provided Index object to run procedures defined in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom tokenizer method.\n",
    "This method takes a String as an argument and returns a List of all tokens, including ngrams between 1 and 3 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Iterator for our dataset. This is an Iterator where every element is a dict which contains a text and a docno.\n",
    "See the example print below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Retrieval Pipeline\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ValueError() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNow we do the retrieval...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#run = bm25(test_dataset.get_topics('text'))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#run = bm25([{\"qid\": \"0\", \"query\" : \"retrieval systems\"}])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print('Done. Here are the first 10 entries of the run')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a query pipeline that includes the n-gram tokenization\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#ngram_tokenize = NgramTokenizeTransform(n=2)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m retr_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mngram_tokenize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbm25\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Sample queries\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/transformer.py:232\u001b[0m, in \u001b[0;36mTransformer.__rrshift__\u001b[0;34m(self, left)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rrshift__\u001b[39m(\u001b[38;5;28mself\u001b[39m, left) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComposedPipeline\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mComposedPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matchpy/expressions/expressions.py:284\u001b[0m, in \u001b[0;36m_OperationMeta.__call__\u001b[0;34m(cls, variable_name, *operands)\u001b[0m\n\u001b[1;32m    282\u001b[0m operation \u001b[38;5;241m=\u001b[39m Expression\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39munpacked_args_to_init:\n\u001b[0;32m--> 284\u001b[0m     \u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     operation\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39moperands, variable_name\u001b[38;5;241m=\u001b[39mvariable_name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:29\u001b[0m, in \u001b[0;36mNAryTransformerBase.__init__\u001b[0;34m(self, operands, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(operands\u001b[38;5;241m=\u001b[39moperands, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     28\u001b[0m models \u001b[38;5;241m=\u001b[39m operands\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:29\u001b[0m, in \u001b[0;36mNAryTransformerBase.__init__.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(operands\u001b[38;5;241m=\u001b[39moperands, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     28\u001b[0m models \u001b[38;5;241m=\u001b[39m operands\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m( \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[43mget_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m, models) )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/transformer.py:42\u001b[0m, in \u001b[0;36mget_transformer\u001b[0;34m(v, stacklevel)\u001b[0m\n\u001b[1;32m     40\u001b[0m     warn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoercion of a dataframe into a transformer is deprecated; use a pt.Transformer.from_df() instead\u001b[39m\u001b[38;5;124m'\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SourceTransformer(v)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;43;01mValueError\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPassed parameter \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m of type \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m cannot be coerced into a transformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;167;43;01mDeprecationWarning\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: ValueError() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "#run = bm25(test_dataset.get_topics('text'))\n",
    "#run = bm25([{\"qid\": \"0\", \"query\" : \"retrieval systems\"}])\n",
    "#print('Done. Here are the first 10 entries of the run')\n",
    "#run.head(10)\n",
    "\n",
    "\n",
    "# tokenize ngram the queries\n",
    "# Create a query pipeline that includes the n-gram tokenization\n",
    "#ngram_tokenize = NgramTokenizeTransform(n=2)\n",
    "retr_pipe = ngram_tokenize >> bm25\n",
    "\n",
    "import pandas as pd\n",
    "# Sample queries\n",
    "queries = pd.DataFrame([\n",
    "    {'qid': 'q1', 'query': 'do goldfish grow?'},\n",
    "    {'qid': 'q2', 'query': 'quick brown fox'}\n",
    "])\n",
    "\n",
    "# Retrieve results\n",
    "run = retr_pipe(queries)\n",
    "run.head(10)\n",
    "results = retr_pipe.transform(queries)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes for docid 0: {'docno': '0', 'text': ''}\n",
      "Attributes for docid 1: {'docno': '1', 'text': ''}\n",
      "Attributes for docid 2: {'docno': '2', 'text': ''}\n"
     ]
    }
   ],
   "source": [
    "#index = pt.IndexFactory.of(index_ref)\n",
    "meta = index.getMetaIndex()\n",
    "\n",
    "# List of metadata keys\n",
    "meta_keys = ['docno', 'text']  # Adjust this list based on your meta fields\n",
    "\n",
    "# Function to print document attributes\n",
    "def print_doc_attributes(docid):\n",
    "    attributes = {key: meta.getItem(key, docid) for key in meta_keys}\n",
    "    print(f\"Attributes for docid {docid}: {attributes}\")\n",
    "\n",
    "# Example: Print attributes for the first 5 documents\n",
    "for docid in range(3):\n",
    "    print_doc_attributes(docid)\n",
    "\n",
    "# Function to print document attributes by docno\n",
    "def print_doc_attributes_by_docno(docno):\n",
    "    try:\n",
    "        docid = meta.getDocument(\"docno\", docno)\n",
    "        print_doc_attributes(docid)\n",
    "    except KeyError:\n",
    "        print(f\"Document with docno {docno} not found.\")\n",
    "\n",
    "# List of specific docnos to retrieve\n",
    "#docnos = ['W05-0704']\n",
    "\n",
    "# Retrieve and print attributes for each specified docno\n",
    "#for docno in docnos:\n",
    " #   print_doc_attributes_by_docno(docno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
