{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from tira.third_party_integrations import persist_and_normalize_run,  ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start pyterrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyTerrier\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "\n",
    "#start Tira\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dataset and Indexing\n",
    "First, we get the Dataset from pyterrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:   0%|          | 0/126958 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#Get dataset\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "#Create Iterator of our dataset\n",
    "docs =  pt_dataset.get_corpus_iter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we delete all but the first n documents. \n",
    "We need to do so as otherwise the notebook will crash.\n",
    "\n",
    "TODO: Use full dataset and try it in Tira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 59070.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'A Study on Word Similarity using Context Vector Models\\n\\n\\n There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.', 'docno': 'O02-2002'}\n",
      "{'text': 'Bootstrapping Large Sense Tagged Corpora', 'docno': 'L02-1310'}\n",
      "{'text': 'Headerless, Quoteless, but not Hopeless? Using Pairwise Email Classification to Disentangle Email Threads\\n\\n\\n Thread disentanglement is the task of separating out conversations whose thread structure is implicit, distorted, or lost. In this paper, we perform email thread disentanglement through pairwise classification, using text similarity measures on non-quoted texts in emails. We show that i) content text similarity metrics outperform style and structure text similarity metrics in both a class-balanced and class-imbalanced setting, and ii) although feature performance is dependent on the semantic similarity of the corpus, content features are still effective even when controlling for semantic similarity. We make available the Enron Threads Corpus, a newly-extracted corpus of 70,178 multiemail threads with emails from the Enron Email Corpus.', 'docno': 'R13-1042'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "#Create List from our docs iterator\n",
    "docs = list(docs)\n",
    "#cutoff the list after the first 500 documents\n",
    "docs = docs[:500]\n",
    "\n",
    "#Print some example documents\n",
    "for i,doc in enumerate(docs):\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(doc)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we process the documents in multiple steps.\n",
    "First, we remove all special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'A Study on Word Similarity using Context Vector Models    There is a need to measure word similarity when processing natural languages  especially when using generalization  classification  or example  based approaches  Usually  measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy   The taxonomy approaches are more or less semantic  based that do not consider syntactic similarit ies  However  in real applications  both semantic and syntactic similarities are required and weighted differently  Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies  In this paper  we propose using only syntactic related co occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision  The probabilistic distribution of co occurrence context features is derived by parsing the contextual environment of each word   and all the context features are adjusted according to their IDF  inverse document frequency  values  The agglomerative clustering algorithm is applied to group similar words according to their similarity values  It turns out that words with similar syntactic categories and semantic classes are grouped together ', 'docno': 'O02-2002'}\n",
      "{'text': 'Bootstrapping Large Sense Tagged Corpora', 'docno': 'L02-1310'}\n",
      "{'text': 'Headerless  Quoteless  but not Hopeless  Using Pairwise Email Classification to Disentangle Email Threads    Thread disentanglement is the task of separating out conversations whose thread structure is implicit  distorted  or lost  In this paper  we perform email thread disentanglement through pairwise classification  using text similarity measures on non quoted texts in emails  We show that i  content text similarity metrics outperform style and structure text similarity metrics in both a class balanced and class imbalanced setting  and ii  although feature performance is dependent on the semantic similarity of the corpus  content features are still effective even when controlling for semantic similarity  We make available the Enron Threads Corpus  a newly extracted corpus of 70 178 multiemail threads with emails from the Enron Email Corpus ', 'docno': 'R13-1042'}\n"
     ]
    }
   ],
   "source": [
    "#Method that removes all special characters from a String\n",
    "def clean_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Use regular expression to remove non-alphanumeric characters, except spaces\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "\n",
    "#Method that removes all special characters from the text fields of a list of documents\n",
    "#TODO change it so the name of the \"text\" field is passed as parameter\n",
    "def clean_documents(documents):\n",
    "    for document in documents:\n",
    "        if 'text' in document:\n",
    "            document['text'] = clean_text(document['text'])\n",
    "    return documents\n",
    "\n",
    "#Apply that method to our document list\n",
    "clean_documents(docs)\n",
    "for i,doc in enumerate(docs):\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a method to remove all stopwords from our document text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the set of stopwords for the English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Join the filtered words back into a single string\n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a method to apply a Snowball Stemmer to the document text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef remove_stopwords_from_text(text):\\n    # Tokenize the text\\n    words = word_tokenize(text)\\n\\n    #filtered_and_stemmed_text = [stemmer.stem(word) for word in words not in stop_words]\\n\\n    # Filter out the stopwords\\n    filtered_and_stemmed_text = [word for word in words if word.lower() not in stop_words]\\n    # Reconstruct the string from the filtered words\\n    filtered_text = ' '.join(filtered_and_stemmed_text)\\n    return filtered_text\\n\""
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    # Join the stemmed words back into a single string\n",
    "    return ' '.join(stemmed_words)\n",
    "'''\n",
    "def remove_stopwords_from_text(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    #filtered_and_stemmed_text = [stemmer.stem(word) for word in words not in stop_words]\n",
    "\n",
    "    # Filter out the stopwords\n",
    "    filtered_and_stemmed_text = [word for word in words if word.lower() not in stop_words]\n",
    "    # Reconstruct the string from the filtered words\n",
    "    filtered_text = ' '.join(filtered_and_stemmed_text)\n",
    "    return filtered_text\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'studi word similar use context vector model need measur word similar process natur languag especi use general classif exampl base approach usual measur similar two word defin accord distanc semant class semant taxonomi taxonomi approach less semant base consid syntact similarit ie howev real applic semant syntact similar requir weight differ word similar base context vector mixtur syntact semant similarit ie paper propos use syntact relat co occurr context vector adopt inform theoret model solv problem data spars characterist precis probabilist distribut co occurr context featur deriv pars contextu environ word context featur adjust accord idf invers document frequenc valu agglom cluster algorithm appli group similar word accord similar valu turn word similar syntact categori semant class group togeth', 'docno': 'O02-2002'}\n",
      "{'text': 'bootstrap larg sens tag corpora', 'docno': 'L02-1310'}\n",
      "{'text': 'headerless quoteless hopeless use pairwis email classif disentangl email thread thread disentangl task separ convers whose thread structur implicit distort lost paper perform email thread disentangl pairwis classif use text similar measur non quot text email show content text similar metric outperform style structur text similar metric class balanc class imbalanc set ii although featur perform depend semant similar corpus content featur still effect even control semant similar make avail enron thread corpus newli extract corpus 70 178 multiemail thread email enron email corpus', 'docno': 'R13-1042'}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "        if 'text' in doc:\n",
    "            doc['text'] = remove_stopwords(doc['text'])\n",
    "            doc['text'] = stem_text(doc['text'])\n",
    "for i, doc in enumerate(docs):\n",
    "     if i == 3:\n",
    "           break\n",
    "     print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define our ngram tokeniser method. We will use monograms, bigrams, and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our ngram tokenizer. It takes a string and returns a dict of all ngrams, where each ngram is seperated by $$ so it will be parsed as one token\n",
    "\n",
    "def tokenize_ngrams_to_dict(text, n1=1, n2=3):\n",
    "    # Replace spaces with dollar signs\n",
    "    #text_with_dollar_signs = re.sub(r'\\s+', '$', text)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    #words = text_with_dollar_signs.split('$')\n",
    "    words = text.split(' ')\n",
    "    # Initialize an empty Counter to hold all n-grams\n",
    "    all_ngram_counts = Counter()\n",
    "    \n",
    "    # Loop through each n from n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Update the Counter with the current n-grams\n",
    "        all_ngram_counts.update(ngrams)\n",
    "    \n",
    "    return dict(all_ngram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply n-gram tokenization to the dataset\n",
    "#This will delete the 'text' field from the documents,\n",
    "# and create a new 'toks' field which contains the tokens with their frequencies\n",
    "for i,doc in enumerate(docs):\n",
    "    #if i == 1000: #limit to 1000 for now since kernel crashes if performed on whole set of docs\n",
    "     #   break\n",
    "\n",
    "    doc_1gram = tokenize_ngrams_to_dict(doc['text'], n1=1, n2=3)\n",
    "\n",
    "    doc['toks'] = doc_1gram\n",
    "    del doc['text']  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'docno': 'O02-2002', 'toks': {'studi': 1, 'word': 7, 'similar': 8, 'use': 3, 'context': 5, 'vector': 3, 'model': 2, 'need': 1, 'measur': 2, 'process': 1, 'natur': 1, 'languag': 1, 'especi': 1, 'general': 1, 'classif': 1, 'exampl': 1, 'base': 3, 'approach': 2, 'usual': 1, 'two': 1, 'defin': 1, 'accord': 3, 'distanc': 1, 'semant': 6, 'class': 2, 'taxonomi': 2, 'less': 1, 'consid': 1, 'syntact': 5, 'similarit': 2, 'ie': 2, 'howev': 1, 'real': 1, 'applic': 1, 'requir': 1, 'weight': 1, 'differ': 1, 'mixtur': 1, 'paper': 1, 'propos': 1, 'relat': 1, 'co': 2, 'occurr': 2, 'adopt': 1, 'inform': 1, 'theoret': 1, 'solv': 1, 'problem': 1, 'data': 1, 'spars': 1, 'characterist': 1, 'precis': 1, 'probabilist': 1, 'distribut': 1, 'featur': 2, 'deriv': 1, 'pars': 1, 'contextu': 1, 'environ': 1, 'adjust': 1, 'idf': 1, 'invers': 1, 'document': 1, 'frequenc': 1, 'valu': 2, 'agglom': 1, 'cluster': 1, 'algorithm': 1, 'appli': 1, 'group': 2, 'turn': 1, 'categori': 1, 'togeth': 1, 'studi$$word': 1, 'word$$similar': 4, 'similar$$use': 1, 'use$$context': 1, 'context$$vector': 3, 'vector$$model': 1, 'model$$need': 1, 'need$$measur': 1, 'measur$$word': 1, 'similar$$process': 1, 'process$$natur': 1, 'natur$$languag': 1, 'languag$$especi': 1, 'especi$$use': 1, 'use$$general': 1, 'general$$classif': 1, 'classif$$exampl': 1, 'exampl$$base': 1, 'base$$approach': 1, 'approach$$usual': 1, 'usual$$measur': 1, 'measur$$similar': 1, 'similar$$two': 1, 'two$$word': 1, 'word$$defin': 1, 'defin$$accord': 1, 'accord$$distanc': 1, 'distanc$$semant': 1, 'semant$$class': 2, 'class$$semant': 1, 'semant$$taxonomi': 1, 'taxonomi$$taxonomi': 1, 'taxonomi$$approach': 1, 'approach$$less': 1, 'less$$semant': 1, 'semant$$base': 1, 'base$$consid': 1, 'consid$$syntact': 1, 'syntact$$similarit': 1, 'similarit$$ie': 2, 'ie$$howev': 1, 'howev$$real': 1, 'real$$applic': 1, 'applic$$semant': 1, 'semant$$syntact': 1, 'syntact$$similar': 1, 'similar$$requir': 1, 'requir$$weight': 1, 'weight$$differ': 1, 'differ$$word': 1, 'similar$$base': 1, 'base$$context': 1, 'vector$$mixtur': 1, 'mixtur$$syntact': 1, 'syntact$$semant': 1, 'semant$$similarit': 1, 'ie$$paper': 1, 'paper$$propos': 1, 'propos$$use': 1, 'use$$syntact': 1, 'syntact$$relat': 1, 'relat$$co': 1, 'co$$occurr': 2, 'occurr$$context': 2, 'vector$$adopt': 1, 'adopt$$inform': 1, 'inform$$theoret': 1, 'theoret$$model': 1, 'model$$solv': 1, 'solv$$problem': 1, 'problem$$data': 1, 'data$$spars': 1, 'spars$$characterist': 1, 'characterist$$precis': 1, 'precis$$probabilist': 1, 'probabilist$$distribut': 1, 'distribut$$co': 1, 'context$$featur': 2, 'featur$$deriv': 1, 'deriv$$pars': 1, 'pars$$contextu': 1, 'contextu$$environ': 1, 'environ$$word': 1, 'word$$context': 1, 'featur$$adjust': 1, 'adjust$$accord': 1, 'accord$$idf': 1, 'idf$$invers': 1, 'invers$$document': 1, 'document$$frequenc': 1, 'frequenc$$valu': 1, 'valu$$agglom': 1, 'agglom$$cluster': 1, 'cluster$$algorithm': 1, 'algorithm$$appli': 1, 'appli$$group': 1, 'group$$similar': 1, 'similar$$word': 1, 'word$$accord': 1, 'accord$$similar': 1, 'similar$$valu': 1, 'valu$$turn': 1, 'turn$$word': 1, 'similar$$syntact': 1, 'syntact$$categori': 1, 'categori$$semant': 1, 'class$$group': 1, 'group$$togeth': 1, 'studi$$word$$similar': 1, 'word$$similar$$use': 1, 'similar$$use$$context': 1, 'use$$context$$vector': 1, 'context$$vector$$model': 1, 'vector$$model$$need': 1, 'model$$need$$measur': 1, 'need$$measur$$word': 1, 'measur$$word$$similar': 1, 'word$$similar$$process': 1, 'similar$$process$$natur': 1, 'process$$natur$$languag': 1, 'natur$$languag$$especi': 1, 'languag$$especi$$use': 1, 'especi$$use$$general': 1, 'use$$general$$classif': 1, 'general$$classif$$exampl': 1, 'classif$$exampl$$base': 1, 'exampl$$base$$approach': 1, 'base$$approach$$usual': 1, 'approach$$usual$$measur': 1, 'usual$$measur$$similar': 1, 'measur$$similar$$two': 1, 'similar$$two$$word': 1, 'two$$word$$defin': 1, 'word$$defin$$accord': 1, 'defin$$accord$$distanc': 1, 'accord$$distanc$$semant': 1, 'distanc$$semant$$class': 1, 'semant$$class$$semant': 1, 'class$$semant$$taxonomi': 1, 'semant$$taxonomi$$taxonomi': 1, 'taxonomi$$taxonomi$$approach': 1, 'taxonomi$$approach$$less': 1, 'approach$$less$$semant': 1, 'less$$semant$$base': 1, 'semant$$base$$consid': 1, 'base$$consid$$syntact': 1, 'consid$$syntact$$similarit': 1, 'syntact$$similarit$$ie': 1, 'similarit$$ie$$howev': 1, 'ie$$howev$$real': 1, 'howev$$real$$applic': 1, 'real$$applic$$semant': 1, 'applic$$semant$$syntact': 1, 'semant$$syntact$$similar': 1, 'syntact$$similar$$requir': 1, 'similar$$requir$$weight': 1, 'requir$$weight$$differ': 1, 'weight$$differ$$word': 1, 'differ$$word$$similar': 1, 'word$$similar$$base': 1, 'similar$$base$$context': 1, 'base$$context$$vector': 1, 'context$$vector$$mixtur': 1, 'vector$$mixtur$$syntact': 1, 'mixtur$$syntact$$semant': 1, 'syntact$$semant$$similarit': 1, 'semant$$similarit$$ie': 1, 'similarit$$ie$$paper': 1, 'ie$$paper$$propos': 1, 'paper$$propos$$use': 1, 'propos$$use$$syntact': 1, 'use$$syntact$$relat': 1, 'syntact$$relat$$co': 1, 'relat$$co$$occurr': 1, 'co$$occurr$$context': 2, 'occurr$$context$$vector': 1, 'context$$vector$$adopt': 1, 'vector$$adopt$$inform': 1, 'adopt$$inform$$theoret': 1, 'inform$$theoret$$model': 1, 'theoret$$model$$solv': 1, 'model$$solv$$problem': 1, 'solv$$problem$$data': 1, 'problem$$data$$spars': 1, 'data$$spars$$characterist': 1, 'spars$$characterist$$precis': 1, 'characterist$$precis$$probabilist': 1, 'precis$$probabilist$$distribut': 1, 'probabilist$$distribut$$co': 1, 'distribut$$co$$occurr': 1, 'occurr$$context$$featur': 1, 'context$$featur$$deriv': 1, 'featur$$deriv$$pars': 1, 'deriv$$pars$$contextu': 1, 'pars$$contextu$$environ': 1, 'contextu$$environ$$word': 1, 'environ$$word$$context': 1, 'word$$context$$featur': 1, 'context$$featur$$adjust': 1, 'featur$$adjust$$accord': 1, 'adjust$$accord$$idf': 1, 'accord$$idf$$invers': 1, 'idf$$invers$$document': 1, 'invers$$document$$frequenc': 1, 'document$$frequenc$$valu': 1, 'frequenc$$valu$$agglom': 1, 'valu$$agglom$$cluster': 1, 'agglom$$cluster$$algorithm': 1, 'cluster$$algorithm$$appli': 1, 'algorithm$$appli$$group': 1, 'appli$$group$$similar': 1, 'group$$similar$$word': 1, 'similar$$word$$accord': 1, 'word$$accord$$similar': 1, 'accord$$similar$$valu': 1, 'similar$$valu$$turn': 1, 'valu$$turn$$word': 1, 'turn$$word$$similar': 1, 'word$$similar$$syntact': 1, 'similar$$syntact$$categori': 1, 'syntact$$categori$$semant': 1, 'categori$$semant$$class': 1, 'semant$$class$$group': 1, 'class$$group$$togeth': 1}}\n",
      "{'docno': 'L02-1310', 'toks': {'bootstrap': 1, 'larg': 1, 'sens': 1, 'tag': 1, 'corpora': 1, 'bootstrap$$larg': 1, 'larg$$sens': 1, 'sens$$tag': 1, 'tag$$corpora': 1, 'bootstrap$$larg$$sens': 1, 'larg$$sens$$tag': 1, 'sens$$tag$$corpora': 1}}\n",
      "{'docno': 'R13-1042', 'toks': {'headerless': 1, 'quoteless': 1, 'hopeless': 1, 'use': 2, 'pairwis': 2, 'email': 6, 'classif': 2, 'disentangl': 3, 'thread': 6, 'task': 1, 'separ': 1, 'convers': 1, 'whose': 1, 'structur': 2, 'implicit': 1, 'distort': 1, 'lost': 1, 'paper': 1, 'perform': 2, 'text': 4, 'similar': 5, 'measur': 1, 'non': 1, 'quot': 1, 'show': 1, 'content': 2, 'metric': 2, 'outperform': 1, 'style': 1, 'class': 2, 'balanc': 1, 'imbalanc': 1, 'set': 1, 'ii': 1, 'although': 1, 'featur': 2, 'depend': 1, 'semant': 2, 'corpus': 4, 'still': 1, 'effect': 1, 'even': 1, 'control': 1, 'make': 1, 'avail': 1, 'enron': 2, 'newli': 1, 'extract': 1, '70': 1, '178': 1, 'multiemail': 1, 'headerless$$quoteless': 1, 'quoteless$$hopeless': 1, 'hopeless$$use': 1, 'use$$pairwis': 1, 'pairwis$$email': 1, 'email$$classif': 1, 'classif$$disentangl': 1, 'disentangl$$email': 1, 'email$$thread': 2, 'thread$$thread': 1, 'thread$$disentangl': 2, 'disentangl$$task': 1, 'task$$separ': 1, 'separ$$convers': 1, 'convers$$whose': 1, 'whose$$thread': 1, 'thread$$structur': 1, 'structur$$implicit': 1, 'implicit$$distort': 1, 'distort$$lost': 1, 'lost$$paper': 1, 'paper$$perform': 1, 'perform$$email': 1, 'disentangl$$pairwis': 1, 'pairwis$$classif': 1, 'classif$$use': 1, 'use$$text': 1, 'text$$similar': 3, 'similar$$measur': 1, 'measur$$non': 1, 'non$$quot': 1, 'quot$$text': 1, 'text$$email': 1, 'email$$show': 1, 'show$$content': 1, 'content$$text': 1, 'similar$$metric': 2, 'metric$$outperform': 1, 'outperform$$style': 1, 'style$$structur': 1, 'structur$$text': 1, 'metric$$class': 1, 'class$$balanc': 1, 'balanc$$class': 1, 'class$$imbalanc': 1, 'imbalanc$$set': 1, 'set$$ii': 1, 'ii$$although': 1, 'although$$featur': 1, 'featur$$perform': 1, 'perform$$depend': 1, 'depend$$semant': 1, 'semant$$similar': 2, 'similar$$corpus': 1, 'corpus$$content': 1, 'content$$featur': 1, 'featur$$still': 1, 'still$$effect': 1, 'effect$$even': 1, 'even$$control': 1, 'control$$semant': 1, 'similar$$make': 1, 'make$$avail': 1, 'avail$$enron': 1, 'enron$$thread': 1, 'thread$$corpus': 1, 'corpus$$newli': 1, 'newli$$extract': 1, 'extract$$corpus': 1, 'corpus$$70': 1, '70$$178': 1, '178$$multiemail': 1, 'multiemail$$thread': 1, 'thread$$email': 1, 'email$$enron': 1, 'enron$$email': 1, 'email$$corpus': 1, 'headerless$$quoteless$$hopeless': 1, 'quoteless$$hopeless$$use': 1, 'hopeless$$use$$pairwis': 1, 'use$$pairwis$$email': 1, 'pairwis$$email$$classif': 1, 'email$$classif$$disentangl': 1, 'classif$$disentangl$$email': 1, 'disentangl$$email$$thread': 1, 'email$$thread$$thread': 1, 'thread$$thread$$disentangl': 1, 'thread$$disentangl$$task': 1, 'disentangl$$task$$separ': 1, 'task$$separ$$convers': 1, 'separ$$convers$$whose': 1, 'convers$$whose$$thread': 1, 'whose$$thread$$structur': 1, 'thread$$structur$$implicit': 1, 'structur$$implicit$$distort': 1, 'implicit$$distort$$lost': 1, 'distort$$lost$$paper': 1, 'lost$$paper$$perform': 1, 'paper$$perform$$email': 1, 'perform$$email$$thread': 1, 'email$$thread$$disentangl': 1, 'thread$$disentangl$$pairwis': 1, 'disentangl$$pairwis$$classif': 1, 'pairwis$$classif$$use': 1, 'classif$$use$$text': 1, 'use$$text$$similar': 1, 'text$$similar$$measur': 1, 'similar$$measur$$non': 1, 'measur$$non$$quot': 1, 'non$$quot$$text': 1, 'quot$$text$$email': 1, 'text$$email$$show': 1, 'email$$show$$content': 1, 'show$$content$$text': 1, 'content$$text$$similar': 1, 'text$$similar$$metric': 2, 'similar$$metric$$outperform': 1, 'metric$$outperform$$style': 1, 'outperform$$style$$structur': 1, 'style$$structur$$text': 1, 'structur$$text$$similar': 1, 'similar$$metric$$class': 1, 'metric$$class$$balanc': 1, 'class$$balanc$$class': 1, 'balanc$$class$$imbalanc': 1, 'class$$imbalanc$$set': 1, 'imbalanc$$set$$ii': 1, 'set$$ii$$although': 1, 'ii$$although$$featur': 1, 'although$$featur$$perform': 1, 'featur$$perform$$depend': 1, 'perform$$depend$$semant': 1, 'depend$$semant$$similar': 1, 'semant$$similar$$corpus': 1, 'similar$$corpus$$content': 1, 'corpus$$content$$featur': 1, 'content$$featur$$still': 1, 'featur$$still$$effect': 1, 'still$$effect$$even': 1, 'effect$$even$$control': 1, 'even$$control$$semant': 1, 'control$$semant$$similar': 1, 'semant$$similar$$make': 1, 'similar$$make$$avail': 1, 'make$$avail$$enron': 1, 'avail$$enron$$thread': 1, 'enron$$thread$$corpus': 1, 'thread$$corpus$$newli': 1, 'corpus$$newli$$extract': 1, 'newli$$extract$$corpus': 1, 'extract$$corpus$$70': 1, 'corpus$$70$$178': 1, '70$$178$$multiemail': 1, '178$$multiemail$$thread': 1, 'multiemail$$thread$$email': 1, 'thread$$email$$enron': 1, 'email$$enron$$email': 1, 'enron$$email$$corpus': 1}}\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    if i == 3:\n",
    "        break    \n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our Index. \n",
    "We will use an IterDictIndexer with pretokenised=True, as we already created the tokens manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 500\n",
      "Number of terms: 60550\n",
      "Number of postings: 88887\n",
      "Number of fields: 0\n",
      "Number of tokens: 102993\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n",
      "0\n",
      "47\n",
      "0$$2\n",
      "2\n",
      "0$$2$$0\n",
      "1\n",
      "0$$2$$rather\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the IterDictIndexer with pretokenised set to True\n",
    "iter_indexer = pt.IterDictIndexer(\"./ngramindex\", overwrite=True, meta={'docno': 20}, pretokenised=True)\n",
    "\n",
    "# Index our pretokenized documents\n",
    "index_ref = iter_indexer.index(docs)\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "#Print some stats about our index\n",
    "print(index.getCollectionStatistics())\n",
    "\n",
    "# Access the MetaIndex and Lexicon\n",
    "meta = index.getMetaIndex()\n",
    "lexicon = index.getLexicon()\n",
    "\n",
    "#Print some example terms from the index. We see that numbers arent removed\n",
    "i = 0\n",
    "for term, le in index.getLexicon():\n",
    "    i = i+1\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(term) \n",
    "    print(le.getFrequency())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the retrieval pipeline\n",
    "First, we define a new method that works exactly like the index ngram tokeniser method, just that it will return a list of tokens instead of a dictionary, and it will not count each token.\n",
    "\n",
    "This method will be used as one of the transformers for our retrieval pipeline and it is responsible for tokenising the query in the same format as we tokenized our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a string and returns a list with all ngrams\n",
    "def tokenize_ngrams_to_list(text, n1=1, n2=3):\n",
    "    words = text.split()\n",
    "\n",
    "    # Initialize an empty list to hold all n-grams\n",
    "    all_ngrams = []\n",
    "    \n",
    "    # Loop through each n between n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Add all current ngrams to the all_ngrams list\n",
    "        all_ngrams.extend(ngrams)\n",
    "    \n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can finally define the actual retrieval pipeline. It consists of two steps; first, we tokenize the queries into ngrams, and after that, we apply bm25 to the query with our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This transformer will tokenise the queries into the ngrams\n",
    "tokenise_query_ngram = pt.rewrite.tokenise(lambda query: tokenize_ngrams_to_list(query))\n",
    "\n",
    "# This transformer will do the retrieval using bm25, and explicitly not apply any stemming and stopword removal\n",
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", verbose = True, properties={\"termpipelines\" : \"\"})\n",
    "\n",
    "# This is our retrieval pipeline\n",
    "retr_pipeline = tokenise_query_ngram >> bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n",
      "Index(['qid', 'text', 'title', 'query', 'description', 'narrative'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>machine learning language identification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>social media detect self harm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                                     query\n",
       "0   1  retrieval system improving effectiveness\n",
       "1   2  machine learning language identification\n",
       "2   3             social media detect self harm"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at some example queries\n",
    "print(pt_dataset.get_topics().columns)\n",
    "pt_dataset.get_topics('query').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our methods for processing the queries work on lists. So, we will first convert the query Dataframe into a list, then process it, and finally convert it back into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n",
      "[{'qid': '1', 'query': 'retrieval system improving effectiveness'}, {'qid': '2', 'query': 'machine learning language identification'}, {'qid': '3', 'query': 'social media detect self harm'}, {'qid': '4', 'query': 'stemming for arabic languages'}, {'qid': '5', 'query': 'audio based animal recognition'}, {'qid': '6', 'query': 'comparison different retrieval models'}, {'qid': '7', 'query': 'cache architecture'}, {'qid': '8', 'query': 'document scoping formula'}, {'qid': '9', 'query': 'pseudo relevance feedback'}, {'qid': '10', 'query': 'how to represent natural conversations in word nets'}, {'qid': '11', 'query': 'algorithm acceleration with nvidia cuda'}, {'qid': '12', 'query': 'mention of algorithm'}, {'qid': '13', 'query': 'at least three authors'}, {'qid': '14', 'query': 'german domain'}, {'qid': '15', 'query': 'mention of open source'}, {'qid': '16', 'query': 'inclusion of text mining'}, {'qid': '17', 'query': 'the ethics of artificial intelligence'}, {'qid': '19', 'query': 'machine learning for more relevant results'}, {'qid': '20', 'query': 'crawling websites using machine learning'}, {'qid': '21', 'query': 'recommenders influence on users'}, {'qid': '22', 'query': 'search engine caching effects'}, {'qid': '23', 'query': 'consumer product reviews'}, {'qid': '24', 'query': 'limitations machine learning'}, {'qid': '25', 'query': 'medicine related research'}, {'qid': '26', 'query': 'natural language processing'}, {'qid': '27', 'query': 'graph based ranking'}, {'qid': '28', 'query': 'medical studies that use information retrieval'}, {'qid': '29', 'query': 'information retrieval on different language sources'}, {'qid': '30', 'query': 'papers that compare multiple information retrieval methods'}, {'qid': '31', 'query': 'risks of information retrieval in social media'}, {'qid': '32', 'query': 'actual experiments that strengthen theoretical knowledge'}, {'qid': '33', 'query': 'fake news detection'}, {'qid': '34', 'query': 'multimedia retrieval'}, {'qid': '35', 'query': 'processing natural language for information retrieval'}, {'qid': '36', 'query': 'recommendation systems'}, {'qid': '37', 'query': 'personalised search in e commerce'}, {'qid': '38', 'query': 'sentiment analysis'}, {'qid': '39', 'query': 'informational retrieval using neural networks'}, {'qid': '40', 'query': 'query log analysis'}, {'qid': '41', 'query': 'entity recognition'}, {'qid': '42', 'query': 'relevance assessments'}, {'qid': '43', 'query': 'deep neural networks'}, {'qid': '44', 'query': 'information retrieval'}, {'qid': '45', 'query': 'analysis for android apps'}, {'qid': '46', 'query': 'the university of amsterdam'}, {'qid': '47', 'query': 'neural ranking for ecommerce product search'}, {'qid': '48', 'query': 'web pages evolution'}, {'qid': '49', 'query': 'exhaustivity of index'}, {'qid': '50', 'query': 'query optimization'}, {'qid': '51', 'query': 'cosine similarity vector'}, {'qid': '52', 'query': 'reverse indexing'}, {'qid': '53', 'query': 'index compression techniques'}, {'qid': '54', 'query': 'search engine optimization with query logs'}, {'qid': '55', 'query': 'bm25'}, {'qid': '56', 'query': 'what makes natural language processing natural'}, {'qid': '57', 'query': 'principle of a information retrieval indexing'}, {'qid': '58', 'query': 'architecture of web search engine'}, {'qid': '59', 'query': 'what is ahp'}, {'qid': '60', 'query': 'what is information retrieval'}, {'qid': '61', 'query': 'efficient retrieval algorithms'}, {'qid': '62', 'query': 'how to avoid spam results'}, {'qid': '63', 'query': 'information retrieval with algorithms'}, {'qid': '64', 'query': 'misspellings in queries'}, {'qid': '65', 'query': 'information in different language'}, {'qid': '66', 'query': 'abbreviations in queries'}, {'qid': '67', 'query': 'lemmatization algorithms'}, {'qid': '68', 'query': 'filter ad rich documents'}, {'qid': '18', 'query': 'advancements in information retrieval'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(pt_dataset.get_topics())\n",
    "if 'query' not in df.columns:\n",
    "    df['query'] = df['text']\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "queries = df[['qid', 'query']].to_dict(orient='records')\n",
    "\n",
    "# Print the result\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qid': '1', 'query': 'retriev system improv effect'}\n",
      "{'qid': '2', 'query': 'machin learn languag identif'}\n",
      "{'qid': '3', 'query': 'social media detect self harm'}\n",
      "{'qid': '4', 'query': 'stem arab languag'}\n",
      "{'qid': '5', 'query': 'audio base anim recognit'}\n",
      "{'qid': '6', 'query': 'comparison differ retriev model'}\n",
      "{'qid': '7', 'query': 'cach architectur'}\n",
      "{'qid': '8', 'query': 'document scope formula'}\n",
      "{'qid': '9', 'query': 'pseudo relev feedback'}\n",
      "{'qid': '10', 'query': 'repres natur convers word net'}\n",
      "{'qid': '11', 'query': 'algorithm acceler nvidia cuda'}\n",
      "{'qid': '12', 'query': 'mention algorithm'}\n",
      "{'qid': '13', 'query': 'least three author'}\n",
      "{'qid': '14', 'query': 'german domain'}\n",
      "{'qid': '15', 'query': 'mention open sourc'}\n",
      "{'qid': '16', 'query': 'inclus text mine'}\n",
      "{'qid': '17', 'query': 'ethic artifici intellig'}\n",
      "{'qid': '19', 'query': 'machin learn relev result'}\n",
      "{'qid': '20', 'query': 'crawl websit use machin learn'}\n",
      "{'qid': '21', 'query': 'recommend influenc user'}\n",
      "{'qid': '22', 'query': 'search engin cach effect'}\n",
      "{'qid': '23', 'query': 'consum product review'}\n",
      "{'qid': '24', 'query': 'limit machin learn'}\n",
      "{'qid': '25', 'query': 'medicin relat research'}\n",
      "{'qid': '26', 'query': 'natur languag process'}\n",
      "{'qid': '27', 'query': 'graph base rank'}\n",
      "{'qid': '28', 'query': 'medic studi use inform retriev'}\n",
      "{'qid': '29', 'query': 'inform retriev differ languag sourc'}\n",
      "{'qid': '30', 'query': 'paper compar multipl inform retriev method'}\n",
      "{'qid': '31', 'query': 'risk inform retriev social media'}\n",
      "{'qid': '32', 'query': 'actual experi strengthen theoret knowledg'}\n",
      "{'qid': '33', 'query': 'fake news detect'}\n",
      "{'qid': '34', 'query': 'multimedia retriev'}\n",
      "{'qid': '35', 'query': 'process natur languag inform retriev'}\n",
      "{'qid': '36', 'query': 'recommend system'}\n",
      "{'qid': '37', 'query': 'personalis search e commerc'}\n",
      "{'qid': '38', 'query': 'sentiment analysi'}\n",
      "{'qid': '39', 'query': 'inform retriev use neural network'}\n",
      "{'qid': '40', 'query': 'queri log analysi'}\n",
      "{'qid': '41', 'query': 'entiti recognit'}\n",
      "{'qid': '42', 'query': 'relev assess'}\n",
      "{'qid': '43', 'query': 'deep neural network'}\n",
      "{'qid': '44', 'query': 'inform retriev'}\n",
      "{'qid': '45', 'query': 'analysi android app'}\n",
      "{'qid': '46', 'query': 'univers amsterdam'}\n",
      "{'qid': '47', 'query': 'neural rank ecommerc product search'}\n",
      "{'qid': '48', 'query': 'web page evolut'}\n",
      "{'qid': '49', 'query': 'exhaust index'}\n",
      "{'qid': '50', 'query': 'queri optim'}\n",
      "{'qid': '51', 'query': 'cosin similar vector'}\n",
      "{'qid': '52', 'query': 'revers index'}\n",
      "{'qid': '53', 'query': 'index compress techniqu'}\n",
      "{'qid': '54', 'query': 'search engin optim queri log'}\n",
      "{'qid': '55', 'query': 'bm25'}\n",
      "{'qid': '56', 'query': 'make natur languag process natur'}\n",
      "{'qid': '57', 'query': 'principl inform retriev index'}\n",
      "{'qid': '58', 'query': 'architectur web search engin'}\n",
      "{'qid': '59', 'query': 'ahp'}\n",
      "{'qid': '60', 'query': 'inform retriev'}\n",
      "{'qid': '61', 'query': 'effici retriev algorithm'}\n",
      "{'qid': '62', 'query': 'avoid spam result'}\n",
      "{'qid': '63', 'query': 'inform retriev algorithm'}\n",
      "{'qid': '64', 'query': 'misspel queri'}\n",
      "{'qid': '65', 'query': 'inform differ languag'}\n",
      "{'qid': '66', 'query': 'abbrevi queri'}\n",
      "{'qid': '67', 'query': 'lemmat algorithm'}\n",
      "{'qid': '68', 'query': 'filter ad rich document'}\n",
      "{'qid': '18', 'query': 'advanc inform retriev'}\n"
     ]
    }
   ],
   "source": [
    "#TODO sonderzeichen aus query löschen\n",
    "def clean_queries(queries):\n",
    "    for query in queries:\n",
    "        if 'query' in query:\n",
    "            query['query'] = clean_text(query['query'])\n",
    "clean_queries(queries)\n",
    "for query in queries:\n",
    "        if 'query' in query:\n",
    "            query['query'] = remove_stopwords(query['query'])\n",
    "            query['query'] = stem_text(query['query'])\n",
    "'''\n",
    "for i,query in enumerate(queries):\n",
    "   \n",
    "\n",
    "    query_ngram = tokenize_ngrams_to_dict(query['query'], n1=1, n2=3)\n",
    "\n",
    "    query['query'] = query_ngram\n",
    "  '''\n",
    "for query in queries:\n",
    "     print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25):   0%|          | 0/68 [00:00<?, ?q/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25): 100%|██████████| 68/68 [00:00<00:00, 215.35q/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Here are the first 10 entries of the run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query_0</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>414</td>\n",
       "      <td>S07-1088</td>\n",
       "      <td>0</td>\n",
       "      <td>17.367886</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>308</td>\n",
       "      <td>D19-3006</td>\n",
       "      <td>1</td>\n",
       "      <td>12.982733</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>264</td>\n",
       "      <td>2012.iwslt-evaluati</td>\n",
       "      <td>2</td>\n",
       "      <td>12.363764</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>341</td>\n",
       "      <td>P05-1007</td>\n",
       "      <td>3</td>\n",
       "      <td>12.021729</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>C10-2174</td>\n",
       "      <td>4</td>\n",
       "      <td>11.991876</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>2021.emnlp-main.148</td>\n",
       "      <td>5</td>\n",
       "      <td>9.380649</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "      <td>L16-1093</td>\n",
       "      <td>6</td>\n",
       "      <td>9.101785</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>R13-1056</td>\n",
       "      <td>7</td>\n",
       "      <td>9.022026</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>366</td>\n",
       "      <td>2009.mtsummit-poste</td>\n",
       "      <td>8</td>\n",
       "      <td>8.699967</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>475</td>\n",
       "      <td>W18-5026</td>\n",
       "      <td>9</td>\n",
       "      <td>7.870108</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid                docno  rank      score  \\\n",
       "0   1    414             S07-1088     0  17.367886   \n",
       "1   1    308             D19-3006     1  12.982733   \n",
       "2   1    264  2012.iwslt-evaluati     2  12.363764   \n",
       "3   1    341             P05-1007     3  12.021729   \n",
       "4   1    121             C10-2174     4  11.991876   \n",
       "5   1    306  2021.emnlp-main.148     5   9.380649   \n",
       "6   1    360             L16-1093     6   9.101785   \n",
       "7   1     44             R13-1056     7   9.022026   \n",
       "8   1    366  2009.mtsummit-poste     8   8.699967   \n",
       "9   1    475             W18-5026     9   7.870108   \n",
       "\n",
       "                        query_0  \\\n",
       "0  retriev system improv effect   \n",
       "1  retriev system improv effect   \n",
       "2  retriev system improv effect   \n",
       "3  retriev system improv effect   \n",
       "4  retriev system improv effect   \n",
       "5  retriev system improv effect   \n",
       "6  retriev system improv effect   \n",
       "7  retriev system improv effect   \n",
       "8  retriev system improv effect   \n",
       "9  retriev system improv effect   \n",
       "\n",
       "                                               query  \n",
       "0  retriev system improv effect retriev$$system s...  \n",
       "1  retriev system improv effect retriev$$system s...  \n",
       "2  retriev system improv effect retriev$$system s...  \n",
       "3  retriev system improv effect retriev$$system s...  \n",
       "4  retriev system improv effect retriev$$system s...  \n",
       "5  retriev system improv effect retriev$$system s...  \n",
       "6  retriev system improv effect retriev$$system s...  \n",
       "7  retriev system improv effect retriev$$system s...  \n",
       "8  retriev system improv effect retriev$$system s...  \n",
       "9  retriev system improv effect retriev$$system s...  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "run = retr_pipeline(queries)\n",
    "\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "run.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queries = [\\n    {\\'qid\\':1 , \\'query\\':\\'machine learning\\'},\\n    {\\'qid\\':2 , \\'query\\':\\'natural language processing techniques\\'}\\n]\\n\\n# Print the new query representation with ngrams included. This is how our query will get passed to bm25\\ndf = pd.DataFrame(queries)\\nprint(df)\\ntransformed_df = tokenise_query_ngram.transform(df)\\nprint(\"Transformed:\")\\nprint(transformed_df)\\n'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''queries = [\n",
    "    {'qid':1 , 'query':'machine learning'},\n",
    "    {'qid':2 , 'query':'natural language processing techniques'}\n",
    "]\n",
    "\n",
    "# Print the new query representation with ngrams included. This is how our query will get passed to bm25\n",
    "df = pd.DataFrame(queries)\n",
    "print(df)\n",
    "transformed_df = tokenise_query_ngram.transform(df)\n",
    "print(\"Transformed:\")\n",
    "print(transformed_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for index, row in transformed_df.iterrows():\\n    query_id = row[\\'qid\\']\\n    query_text = row[\\'query\\']\\n    print(\"test\")\\n    print(f\"Processing query ID {query_id} with text: {query_text}\")\\n    \\n    # Execute the search\\n    results = retr_pipeline.search(query_text)\\n    \\n    # Print or process the results\\n    print(f\"Results for query ID {query_id}:\")\\n    print(results)'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''for index, row in transformed_df.iterrows():\n",
    "    query_id = row['qid']\n",
    "    query_text = row['query']\n",
    "    print(\"test\")\n",
    "    print(f\"Processing query ID {query_id} with text: {query_text}\")\n",
    "    \n",
    "    # Execute the search\n",
    "    results = retr_pipeline.search(query_text)\n",
    "    \n",
    "    # Print or process the results\n",
    "    print(f\"Results for query ID {query_id}:\")\n",
    "    print(results)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
