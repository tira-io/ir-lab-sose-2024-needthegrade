{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tira in /usr/local/lib/python3.10/dist-packages (0.0.132)\n",
      "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
      "Requirement already satisfied: python-terrier in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Requirement already satisfied: requests==2.*,>=2.26 in /usr/local/lib/python3.10/dist-packages (from tira) (2.31.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tira) (2.1.3)\n",
      "Requirement already satisfied: docker==6.*,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tira) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tira) (4.66.1)\n",
      "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (23.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (2.1.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (1.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.3.2)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.3)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (1.26.2)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.3.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (6.0.1)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.12)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.6)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.12.2)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.3.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.6)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (3.2.3)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.2)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.9.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.7)\n",
      "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: matchpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.0)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: typish>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.5)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /usr/local/lib/python3.10/dist-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.10/dist-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (2.1.3)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->python-terrier) (3.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (0.5.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->python-terrier) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# You only need to execute this cell if you are using Google Golab.\n",
    "# If you use GitHub Codespaces, everything is already installed.\n",
    "!pip3 install tira ir-datasets python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "#from pyterrier.index import IterDictIndexer\n",
    "import pandas as pd #extra import\n",
    "from sklearn.feature_extraction.text import CountVectorizer #extra import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "VM is already running, can't set classpath/options; VM started at  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1077, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 529, in dispatch_queue\n    await self.process_one()\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 518, in process_one\n    await dispatch(*args)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 424, in dispatch_shell\n    await result\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 766, in execute_request\n    reply_content = await reply_content\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n    res = shell.run_cell(\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n    result = runner(coro)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_2818/338050841.py\", line 5, in <module>\n    from pyterrier.index import IterDictIndexer\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.10/dist-packages/pyterrier/index.py\", line 5, in <module>\n    from jnius import autoclass, PythonJavaClass, java_method, cast\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.10/dist-packages/jnius/__init__.py\", line 45, in <module>\n    from .reflect import *  # noqa\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.10/dist-packages/jnius/reflect.py\", line 19, in <module>\n    class Class(JavaClass, metaclass=MetaJavaClass):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mensure_pyterrier_is_loaded\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m tira \u001b[38;5;241m=\u001b[39m Client()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tira/third_party_integrations.py:30\u001b[0m, in \u001b[0;36mensure_pyterrier_is_loaded\u001b[0;34m(boot_packages, packages, patch_ir_datasets)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pt\u001b[38;5;241m.\u001b[39mstarted():\n\u001b[1;32m     29\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart PyTerrier with version=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpt_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, helper_version=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpt_helper_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, no_download=True\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpt_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhelper_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpt_helper_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboot_packages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mboot_packages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpackages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/__init__.py:122\u001b[0m, in \u001b[0;36minit\u001b[0;34m(version, mem, packages, jvm_opts, redirect_io, logging, home_dir, boot_packages, tqdm, no_download, helper_version)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjnius_config\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m jar \u001b[38;5;129;01min\u001b[39;00m classpathTrJars:\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mjnius_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_classpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jvm_opts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m jvm_opts:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jnius_config.py:57\u001b[0m, in \u001b[0;36madd_classpath\u001b[0;34m(*path)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_classpath\u001b[39m(\u001b[38;5;241m*\u001b[39mpath):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    Appends items to the classpath for the JVM to use.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    Replaces any existing classpath, overriding the CLASSPATH environment variable.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mcheck_vm_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m classpath\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m classpath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jnius_config.py:20\u001b[0m, in \u001b[0;36mcheck_vm_running\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raises a ValueError if the VM is already running.\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vm_running:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVM is already running, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt set classpath/options; VM started at\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m vm_started_at)\n",
      "\u001b[0;31mValueError\u001b[0m: VM is already running, can't set classpath/options; VM started at  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1077, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 529, in dispatch_queue\n    await self.process_one()\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 518, in process_one\n    await dispatch(*args)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 424, in dispatch_shell\n    await result\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 766, in execute_request\n    reply_content = await reply_content\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n    res = shell.run_cell(\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n    result = runner(coro)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_2818/338050841.py\", line 5, in <module>\n    from pyterrier.index import IterDictIndexer\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.10/dist-packages/pyterrier/index.py\", line 5, in <module>\n    from jnius import autoclass, PythonJavaClass, java_method, cast\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.10/dist-packages/jnius/__init__.py\", line 45, in <module>\n    from .reflect import *  # noqa\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.10/dist-packages/jnius/reflect.py\", line 19, in <module>\n    class Class(JavaClass, metaclass=MetaJavaClass):\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset and the Index\n",
    "\n",
    "The type of the index object that we load is `<class 'jnius.reflect.org.terrier.structures.Index'>`, in fact a [Java class](http://terrier.org/docs/v3.6/javadoc/org/terrier/structures/Index.html) wrapped into Python. However, you do not need to worry about this: at this point, we will simply use the provided Index object to run procedures defined in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRDSDataset('ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n"
     ]
    }
   ],
   "source": [
    "# Datenset laden\n",
    "# Lade den Datensatz in test_dataset\n",
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "\n",
    "test_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom tokenizer method.\n",
    "This method takes a String as an argument and returns a List of all tokens, including ngrams between 1 and 3 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['testing', 'of', 'information', 'retrieval', 'systems', 'testing of', 'of information', 'information retrieval', 'retrieval systems', 'testing of information', 'of information retrieval', 'information retrieval systems']\n"
     ]
    }
   ],
   "source": [
    "# Methode, die einen String einliest und eine Liste aller ngrams in der Range ausgibt. 1grams: alle Wörter einzeln. 2grams: alle paare aus benachbarten etc.\n",
    "def ngram_tokenizer(text, ngram_range=(1, 3)):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, token_pattern=r'\\b\\w+\\b', analyzer='word')\n",
    "    analyze = vectorizer.build_analyzer()\n",
    "    return analyze(text)\n",
    "\n",
    "#Beispiel, wie ein text getokenized wird\n",
    "print(ngram_tokenizer(\"Testing of Information Retrieval Systems\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Iterator for our dataset. This is an Iterator where every element is a dict which contains a text and a docno.\n",
    "See the example print below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:   8%|▊         | 9737/126958 [00:00<00:02, 49278.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Bootstrapping Large Sense Tagged Corpora', 'docno': 'L02-1310'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 54531.28it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Erstelle einen Iterator von Dicts von unserem Dataset\n",
    "# Return type : Iterator[Dict[str, Any]]\n",
    "corpus_iter = test_dataset.get_corpus_iter()\n",
    "\n",
    "#Print example element of corpus_iter.\n",
    "#Corpus_iter is an iterator where all elements are dicts that look like this\n",
    "for doc in corpus_iter:\n",
    "     if doc.get(\"docno\") == \"L02-1310\":\n",
    "          print(doc)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erstelle custom Indexer, der IterDictIndexer extended\n",
    "class CustomTokenizerIterDictIndexer(IterDictIndexer):\n",
    "    # Constructor, der den Konstruktor der Superklasse aufruft und die Argumente weitergibt\n",
    "    def __init__(self, index_path, tokenizer):\n",
    "        super().__init__(index_path) # übergebe Parameter an Konstruktor der Superklasse\n",
    "        self.tokenizer = tokenizer  # Da man den tokenizer nicht als Parameter übergeben kann, muss man ihn so von hand ändern\n",
    "\n",
    "    def _document_to_dict(self, doc):\n",
    "        \"\"\"\n",
    "        Convert a document into a dictionary representation.\n",
    "        Override this method to include custom tokenization.\n",
    "        \"\"\"\n",
    "        if isinstance(doc, dict):\n",
    "            doc_dict = doc\n",
    "        else:\n",
    "            doc_dict = {\n",
    "                \"docno\": str(doc[0]),\n",
    "                \"text\": doc[1]\n",
    "            }\n",
    "        \n",
    "        # Tokenize the text using the custom tokenizer\n",
    "        tokens = self.tokenizer(doc_dict[\"text\"])\n",
    "        \n",
    "        # Create a dictionary with tokenized terms\n",
    "        doc_dict[\"text\"] = \" \".join(tokens)\n",
    "        return doc_dict\n",
    "\n",
    "    def index(self, iter, fields=('text',), **kwargs):\n",
    "        \"\"\"\n",
    "        Override the index method if necessary.\n",
    "        \"\"\"\n",
    "        return super().index(iter, fields, **kwargs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:   0%|          | 0/126958 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the custom indexer\n",
    "index_path = \"./ngram_test_index1\"\n",
    "ngram_indexer = CustomTokenizerIterDictIndexer(index_path, ngram_tokenizer) \n",
    "\n",
    "\n",
    "#indexer = pt.IterDictIndexer('./index_ngram3', meta=['docno', 'text'], meta_lengths=[20, 4096])\n",
    "#index_ref = indexer.index(transformed_corpus_iter, fields=['text'])\n",
    "\n",
    "docs = test_dataset.get_corpus_iter()\n",
    "\n",
    "docs =  [\n",
    "    {\"docno\": \"1\", \"text\": \"Information Retrieval Systems\"},\n",
    "    {\"docno\": \"2\", \"text\": \"Systems Retrieval Information\"}\n",
    "]\n",
    "index_ref = ngram_indexer.index(docs)   #,  fields=['text'])\n",
    "\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   qid                                     query\n",
      "0    1  retrieval system improving effectiveness\n",
      "1    2  machine learning language identification\n",
      "2    3             social media detect self harm\n",
      "3    4             stemming for arabic languages\n",
      "4    5            audio based animal recognition\n",
      "..  ..                                       ...\n",
      "63  65         information in different language\n",
      "64  66                  abbreviations in queries\n",
      "65  67                  lemmatization algorithms\n",
      "66  68                  filter ad rich documents\n",
      "67  18     advancements in information retrieval\n",
      "\n",
      "[68 rows x 2 columns]\n",
      "Number of documents: 5\n",
      "Number of terms: 4\n",
      "Number of postings: 10\n",
      "Number of fields: 1\n",
      "Number of tokens: 16\n",
      "Field names: [text]\n",
      "Positions:   false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.get_topics('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "JavaException",
     "evalue": "JVM exception occurred: org/terrier/structures/indexing/DocumentPostingList java.lang.NoClassDefFoundError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJavaException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#index = tira.pt.index('ir-lab-sose-2024-needthegrade/baseline-retrieval-system/index_ngram2', test_dataset)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyterrier\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyterrier\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IterDictIndexer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_tokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m        Custom tokenizer method that lowercases and splits text by spaces.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/index.py:1265\u001b[0m\n\u001b[1;32m   1262\u001b[0m         lastdoc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection\u001b[38;5;241m.\u001b[39mgetDocument()\n\u001b[1;32m   1263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m lastdoc\n\u001b[0;32m-> 1265\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDocListIterator\u001b[39;00m(PythonJavaClass):\n\u001b[1;32m   1266\u001b[0m     dpl_class \u001b[38;5;241m=\u001b[39m autoclass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.terrier.structures.indexing.DocumentPostingList\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1267\u001b[0m     tuple_class \u001b[38;5;241m=\u001b[39m autoclass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.terrier.structures.collections.MapEntry\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/index.py:1266\u001b[0m, in \u001b[0;36mDocListIterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDocListIterator\u001b[39;00m(PythonJavaClass):\n\u001b[0;32m-> 1266\u001b[0m     dpl_class \u001b[38;5;241m=\u001b[39m \u001b[43mautoclass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.terrier.structures.indexing.DocumentPostingList\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1267\u001b[0m     tuple_class \u001b[38;5;241m=\u001b[39m autoclass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.terrier.structures.collections.MapEntry\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1268\u001b[0m     __javainterfaces__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1269\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava/util/Iterator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1270\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jnius/reflect.py:209\u001b[0m, in \u001b[0;36mautoclass\u001b[0;34m(clsname, include_protected, include_private)\u001b[0m\n\u001b[1;32m    206\u001b[0m cls_start_packagename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(clsname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# c = Class.forName(clsname)\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mfind_javaclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclsname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJava class \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(c))\n",
      "File \u001b[0;32mjnius/jnius_export_func.pxi:22\u001b[0m, in \u001b[0;36mjnius.find_javaclass\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_utils.pxi:79\u001b[0m, in \u001b[0;36mjnius.check_exception\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mJavaException\u001b[0m: JVM exception occurred: org/terrier/structures/indexing/DocumentPostingList java.lang.NoClassDefFoundError"
     ]
    }
   ],
   "source": [
    "\n",
    "#index = tira.pt.index('ir-lab-sose-2024-needthegrade/baseline-retrieval-system/index_ngram2', test_dataset)\n",
    "\n",
    "import pyterrier as pt\n",
    "from pyterrier.index import IterDictIndexer\n",
    "\n",
    "\n",
    "def custom_tokenizer(self, text):\n",
    "        \"\"\"\n",
    "        Custom tokenizer method that lowercases and splits text by spaces.\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(ngram_range=(2,2), token_pattern=r'\\b\\w+\\b', analyzer='word')\n",
    "        analyze = vectorizer.build_analyzer()\n",
    "        return analyze(text)\n",
    "def ngram_tokenizer(text, ngram_range=(2,2)):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, token_pattern=r'\\b\\w+\\b', analyzer='word')\n",
    "    analyze = vectorizer.build_analyzer()\n",
    "    return analyze(text)\n",
    "\n",
    "class CustomTokenizerIterDictIndexer(IterDictIndexer):\n",
    "    def __init__(self, index_path, overwrite, tokenizer):\n",
    "        super().__init__(index_path, overwrite)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "\n",
    "    def get_text_dict(self, doc):\n",
    "        text = doc['text']\n",
    "        tokens = custom_tokenizer(text)\n",
    "        return {'text': ' '.join(tokens)}\n",
    "    \n",
    "\n",
    "    def _document_to_dict(self, doc):\n",
    "        \"\"\"\n",
    "        Convert a document into a dictionary representation.\n",
    "        Override this method to include custom tokenization.\n",
    "        \"\"\"\n",
    "        if isinstance(doc, dict):\n",
    "            doc_dict = doc\n",
    "        else:\n",
    "            doc_dict = {\n",
    "                \"docno\": str(doc[0]),\n",
    "                \"text\": doc[1]\n",
    "            }\n",
    "        \n",
    "        # Tokenize the text using the custom tokenizer\n",
    "        tokens = self.custom_tokenizer(doc_dict[\"text\"])\n",
    "        \n",
    "        # Create a dictionary with tokenized terms\n",
    "        doc_dict[\"text\"] = \" \".join(tokens)\n",
    "        return doc_dict\n",
    "\n",
    "    def index(self, iter, fields=('text',), **kwargs):\n",
    "        \"\"\"\n",
    "        Override the index method if necessary.\n",
    "        \"\"\"\n",
    "        return super().index(iter, fields, **kwargs)\n",
    "\n",
    "# Initialize PyTerrier\n",
    "\n",
    "\n",
    "# Create an instance of the custom indexer\n",
    "index_path = \"./my_custom_index\"\n",
    "custom_indexer = CustomTokenizerIterDictIndexer(index_path, overwrite = True, tokenizer=ngram_tokenizer)\n",
    "\n",
    "# Example documents to index\n",
    "docs = [\n",
    "    {\"docno\": \"0\", \"text\": \"information retrieval systems for about a million times i try to do the system for the retrieval pipeline, why doesnt it work\"},\n",
    "    {\"docno\": \"1\", \"text\": \"\"}\n",
    "]\n",
    "\n",
    "# Index the documents\n",
    "index_ref = custom_indexer.index(docs)\n",
    "\n",
    "# Now you can use the index_ref as usual\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "print(index.getCollectionStatistics())\n",
    "for term, le in index.getLexicon():\n",
    "    print(term) \n",
    "    print(le.getFrequency())\n",
    "\n",
    "# Access the MetaIndex and Lexicon\n",
    "meta = index.getMetaIndex()\n",
    "lexicon = index.getLexicon()\n",
    "inverted = index.getInvertedIndex()\n",
    "print(inverted)\n",
    "print(inverted)\n",
    "\n",
    "\n",
    "# Print indexed terms for each document\n",
    "for doc in docs:\n",
    "    doc_id = meta.getDocument(\"docno\", doc[\"docno\"])\n",
    "    print(f\"Document {doc['docno']} index entries:\")\n",
    "    for termid in index.getInvertedIndex().getLexicon().getLexiconEntry(termid):\n",
    "        term = lexicon.getLexiconEntry(termid).getKey()\n",
    "        freq = index.getDocumentIndex()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Retrieval Pipeline\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, we have a short look at the first three topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>machine learning language identification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>social media detect self harm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                                     query\n",
       "0   1  retrieval system improving effectiveness\n",
       "1   2  machine learning language identification\n",
       "2   3             social media detect self harm"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('First, we have a short look at the first three topics:')\n",
    "\n",
    "test_dataset.get_topics('text').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n",
      "Done. Here are the first 10 entries of the run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.394619</td>\n",
       "      <td>retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.394619</td>\n",
       "      <td>retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.573390</td>\n",
       "      <td>retrieval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid docno  rank     score      query\n",
       "0   0      0     0     0 -0.394619  retrieval\n",
       "1   0      4     4     1 -0.394619  retrieval\n",
       "2   0      1     1     2 -0.573390  retrieval"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "#run = bm25(test_dataset.get_topics('text'))\n",
    "run = bm25([{\"qid\": \"0\", \"query\" : \"retrieval\"}])\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "run.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes for docid 0: {'docno': '0', 'text': ''}\n",
      "Attributes for docid 1: {'docno': '1', 'text': ''}\n",
      "Attributes for docid 2: {'docno': '2', 'text': ''}\n",
      "Attributes for docid 3: {'docno': '3', 'text': ''}\n"
     ]
    }
   ],
   "source": [
    "#index = pt.IndexFactory.of(index_ref)\n",
    "meta = index.getMetaIndex()\n",
    "\n",
    "# List of metadata keys\n",
    "meta_keys = ['docno', 'text']  # Adjust this list based on your meta fields\n",
    "\n",
    "# Function to print document attributes\n",
    "def print_doc_attributes(docid):\n",
    "    attributes = {key: meta.getItem(key, docid) for key in meta_keys}\n",
    "    print(f\"Attributes for docid {docid}: {attributes}\")\n",
    "\n",
    "# Example: Print attributes for the first 5 documents\n",
    "for docid in range(4):\n",
    "    print_doc_attributes(docid)\n",
    "\n",
    "# Function to print document attributes by docno\n",
    "def print_doc_attributes_by_docno(docno):\n",
    "    try:\n",
    "        docid = meta.getDocument(\"docno\", docno)\n",
    "        print_doc_attributes(docid)\n",
    "    except KeyError:\n",
    "        print(f\"Document with docno {docno} not found.\")\n",
    "\n",
    "# List of specific docnos to retrieve\n",
    "#docnos = ['W05-0704']\n",
    "\n",
    "# Retrieve and print attributes for each specified docno\n",
    "#for docno in docnos:\n",
    " #   print_doc_attributes_by_docno(docno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
