{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tira in /usr/local/lib/python3.10/dist-packages (0.0.132)\n",
      "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
      "Requirement already satisfied: python-terrier in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Requirement already satisfied: requests==2.*,>=2.26 in /usr/local/lib/python3.10/dist-packages (from tira) (2.31.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tira) (2.1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tira) (4.66.1)\n",
      "Requirement already satisfied: docker==6.*,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tira) (6.1.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (2.1.0)\n",
      "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (23.2)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (1.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.6)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.9.3)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (1.26.2)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.3.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.6)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.3)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.12.2)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.12)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (3.2.3)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.6)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (6.0.1)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.3.2)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.4)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.7)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.0)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: matchpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
      "Requirement already satisfied: typish>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.5)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /usr/local/lib/python3.10/dist-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.10/dist-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (2.1.3)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->python-terrier) (3.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (0.5.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->python-terrier) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# You only need to execute this cell if you are using Google Golab.\n",
    "# If you use GitHub Codespaces, everything is already installed.\n",
    "!pip3 install tira ir-datasets python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import pandas as pd #extra import\n",
    "from sklearn.feature_extraction.text import CountVectorizer #extra import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset and the Index\n",
    "\n",
    "The type of the index object that we load is `<class 'jnius.reflect.org.terrier.structures.Index'>`, in fact a [Java class](http://terrier.org/docs/v3.6/javadoc/org/terrier/structures/Index.html) wrapped into Python. However, you do not need to worry about this: at this point, we will simply use the provided Index object to run procedures defined in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRDSDataset('ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n"
     ]
    }
   ],
   "source": [
    "# Datenset laden\n",
    "# Lade den Datensatz in test_dataset\n",
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "\n",
    "test_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom tokenizer method.\n",
    "This method takes a String as an argument and returns a List of all tokens, including ngrams between 1 and 3 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['testing', 'of', 'information', 'retrieval', 'systems', 'testing of', 'of information', 'information retrieval', 'retrieval systems', 'testing of information', 'of information retrieval', 'information retrieval systems']\n"
     ]
    }
   ],
   "source": [
    "# Methode, die einen String einliest und eine Liste aller ngrams in der Range ausgibt. 1grams: alle Wörter einzeln. 2grams: alle paare aus benachbarten etc.\n",
    "def ngram_tokenizer(text, ngram_range=(1, 3)):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, token_pattern=r'\\b\\w+\\b', analyzer='word')\n",
    "    analyze = vectorizer.build_analyzer()\n",
    "    return analyze(text)\n",
    "\n",
    "#Beispiel, wie ein text getokenized wird\n",
    "print(ngram_tokenizer(\"Testing of Information Retrieval Systems\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Iterator for our dataset. This is an Iterator where every element is a dict which contains a text and a docno.\n",
    "See the example print below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/ir-acl-anthology-20240504-inputs.zip?download=1\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 39.4M/39.4M [00:00<00:00, 64.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_datasets/ir-lab-sose-2024/ir-acl-anthology-20240504-training/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:   4%|▍         | 5496/126958 [00:00<00:02, 54941.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Bootstrapping Large Sense Tagged Corpora', 'docno': 'L02-1310'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 56152.48it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Erstelle einen Iterator von Dicts von unserem Dataset\n",
    "# Return type : Iterator[Dict[str, Any]]\n",
    "corpus_iter = test_dataset.get_corpus_iter()\n",
    "\n",
    "#Print example element of corpus_iter.\n",
    "#Corpus_iter is an iterator where all elements are dicts that look like this\n",
    "for doc in corpus_iter:\n",
    "     if doc.get(\"docno\") == \"L02-1310\":\n",
    "          print(doc)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erstelle custom Indexer, der IterDictIndexer extended\n",
    "class CustomTokenizerIterDictIndexer(IterDictIndexer):\n",
    "    # Constructor, der den Konstruktor der Superklasse aufruft und die Argumente weitergibt\n",
    "    def __init__(self, index_path, tokenizer, meta, meta_lengths):\n",
    "        super().__init__(index_path, meta, meta_lengths) # übergebe Parameter an Konstruktor der Superklasse\n",
    "        self.tokenizer = tokenizer  # Da man den tokenizer nicht als Parameter übergeben kann, muss man ihn so von hand ändern\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the custom indexer\n",
    "index_path = \"./ngram_test_index\"\n",
    "ngram_indexer = CustomTokenizerIterDictIndexer(index_path, ngram_tokenizer, meta=['docno', 'text'], meta_lengths=[20, 4096]) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "JavaException",
     "evalue": "JVM exception occurred: java.lang.NullPointerException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJavaException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Indexing\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# hier iwie ändern dass wenn schon ein index mit dem namen existiert der gelöscht werden muss. sonst muss man jedes mal ein neuer name eingeben\u001b[39;00m\n\u001b[1;32m     17\u001b[0m indexer \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mIterDictIndexer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./index_ngram3\u001b[39m\u001b[38;5;124m'\u001b[39m, meta\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], meta_lengths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m4096\u001b[39m], tokeniser\u001b[38;5;241m=\u001b[39mngram_tokenizer)\n\u001b[0;32m---> 18\u001b[0m index_ref \u001b[38;5;241m=\u001b[39m \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_corpus_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/index.py:1015\u001b[0m, in \u001b[0;36m_IterDictIndexer_fifo.index\u001b[0;34m(self, it, fields, meta, meta_lengths)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup(fields, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1013\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# ParallelIndexer expects the directory to exist\u001b[39;00m\n\u001b[0;32m-> 1015\u001b[0m Indexer, Merger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexerAndMergerClasses\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreads \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads must be positive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Indexer \u001b[38;5;129;01mis\u001b[39;00m BasicMemoryIndexer:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/index.py:573\u001b[0m, in \u001b[0;36mTerrierIndexer.indexerAndMergerClasses\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m     warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting of tokeniser property directly is deprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokeniser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m TerrierTokeniser\u001b[38;5;241m.\u001b[39m_to_class(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokeniser)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# inform terrier of all properties\u001b[39;00m\n\u001b[1;32m    576\u001b[0m ApplicationSetup\u001b[38;5;241m.\u001b[39mgetProperties()\u001b[38;5;241m.\u001b[39mputAll(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproperties)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jnius/reflect.py:406\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(self, k, v)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# protocol_map is a user-accessible API for patching class instances with additional methods\u001b[39;00m\n\u001b[1;32m    396\u001b[0m protocol_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava.util.Collection\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(),\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__contains__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, item: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontains(item),\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__delitem__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, item: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove(item)\n\u001b[1;32m    401\u001b[0m     },\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava.util.List\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__getitem__\u001b[39m\u001b[38;5;124m'\u001b[39m : _getitem\n\u001b[1;32m    404\u001b[0m     },\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava.util.Map\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setitem__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, k, v : \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__getitem__\u001b[39m\u001b[38;5;124m'\u001b[39m : _map_getitem,\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__delitem__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, item: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove(item),\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(),\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__contains__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, item: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainsKey(item),\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__iter__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeySet()\u001b[38;5;241m.\u001b[39miterator()\n\u001b[1;32m    412\u001b[0m     },\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava.util.Map$Entry\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__getitem__\u001b[39m\u001b[38;5;124m'\u001b[39m : _map_entry_getitem,\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__iter__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m: \u001b[38;5;28miter\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetKey(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetValue()]),\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m: \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    417\u001b[0m     },\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava.util.Iterator\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__iter__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m: \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__next__\u001b[39m\u001b[38;5;124m'\u001b[39m : _iterator_next,\n\u001b[1;32m    421\u001b[0m     },\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava.lang.Iterable\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__iter__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator(),\n\u001b[1;32m    424\u001b[0m     },\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# this also addresses java.io.Closeable\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava.lang.AutoCloseable\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__enter__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m: \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__exit__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    429\u001b[0m     },\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava.lang.Comparable\u001b[39m\u001b[38;5;124m'\u001b[39m : {\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;66;03m#__cmp__ is for Python 2 support\u001b[39;00m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__cmp__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompareTo(other),\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mequals(other),\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__ne__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mequals(other),\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__lt__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompareTo(other) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__gt__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompareTo(other) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__le__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompareTo(other) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__ge__\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompareTo(other) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    439\u001b[0m     }\n\u001b[1;32m    440\u001b[0m }\n",
      "File \u001b[0;32mjnius/jnius_export_class.pxi:1163\u001b[0m, in \u001b[0;36mjnius.JavaMultipleMethod.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_export_class.pxi:877\u001b[0m, in \u001b[0;36mjnius.JavaMethod.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_export_class.pxi:954\u001b[0m, in \u001b[0;36mjnius.JavaMethod.call_method\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_utils.pxi:79\u001b[0m, in \u001b[0;36mjnius.check_exception\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mJavaException\u001b[0m: JVM exception occurred: java.lang.NullPointerException"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Client' object has no attribute 'submissions_of_team'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mtira\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mir-lab-sose-2024-needthegrade/baseline-retrieval-system/index_ngram2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tira/pyterrier_integration.py:105\u001b[0m, in \u001b[0;36mPyTerrierIntegration.index\u001b[0;34m(self, approach, dataset)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyterrier\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpt\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtira\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mir_datasets_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m translate_irds_id_to_tirex\n\u001b[0;32m--> 105\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtira_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_run_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapproach\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslate_irds_id_to_tirex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/index\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pt\u001b[38;5;241m.\u001b[39mIndexFactory\u001b[38;5;241m.\u001b[39mof(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(ret))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tira/rest_api_client.py:266\u001b[0m, in \u001b[0;36mClient.get_run_output\u001b[0;34m(self, approach, dataset, allow_without_evaluation)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m    264\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 266\u001b[0m run_execution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_run_execution_or_none\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapproach\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_execution:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_zip_to_cache_directory(task, dataset, team, run_execution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tira/rest_api_client.py:297\u001b[0m, in \u001b[0;36mClient.get_run_execution_or_none\u001b[0;34m(self, approach, dataset, previous_stage_run_id)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m public_runs:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: task, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam\u001b[39m\u001b[38;5;124m'\u001b[39m: team, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m'\u001b[39m: public_runs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]}\n\u001b[0;32m--> 297\u001b[0m df_eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmissions_of_team\u001b[49m(task\u001b[38;5;241m=\u001b[39mtask, dataset\u001b[38;5;241m=\u001b[39mdataset, team\u001b[38;5;241m=\u001b[39mteam)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df_eval) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Client' object has no attribute 'submissions_of_team'"
     ]
    }
   ],
   "source": [
    "\n",
    "#index = tira.pt.index('ir-lab-sose-2024-needthegrade/baseline-retrieval-system/index_ngram2', test_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Retrieval Pipeline\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, we have a short look at the first three topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>machine learning language identification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>social media detect self harm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                                     query\n",
       "0   1  retrieval system improving effectiveness\n",
       "1   2  machine learning language identification\n",
       "2   3             social media detect self harm"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('First, we have a short look at the first three topics:')\n",
    "\n",
    "test_dataset.get_topics('text').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n",
      "Done. Here are the first 10 entries of the run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>94858</td>\n",
       "      <td>2004.cikm_conference-2004.47</td>\n",
       "      <td>0</td>\n",
       "      <td>19.564883</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>125137</td>\n",
       "      <td>1989.ipm_journal-ir0volumeA25A4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>19.211812</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>125817</td>\n",
       "      <td>2005.ipm_journal-ir0volumeA41A5.11</td>\n",
       "      <td>2</td>\n",
       "      <td>19.090084</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>84876</td>\n",
       "      <td>2016.ntcir_conference-2016.90</td>\n",
       "      <td>3</td>\n",
       "      <td>19.011221</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>124801</td>\n",
       "      <td>2006.ipm_journal-ir0volumeA42A3.2</td>\n",
       "      <td>4</td>\n",
       "      <td>18.919271</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>82472</td>\n",
       "      <td>1998.sigirconf_conference-98.15</td>\n",
       "      <td>5</td>\n",
       "      <td>18.856596</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>74513</td>\n",
       "      <td>2001.clef_workshop-2001w.24</td>\n",
       "      <td>6</td>\n",
       "      <td>18.825665</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>81840</td>\n",
       "      <td>2006.sigirconf_conference-2006.103</td>\n",
       "      <td>7</td>\n",
       "      <td>18.813841</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>82490</td>\n",
       "      <td>1998.sigirconf_conference-98.33</td>\n",
       "      <td>8</td>\n",
       "      <td>18.794527</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>17496</td>\n",
       "      <td>O01-2005</td>\n",
       "      <td>9</td>\n",
       "      <td>18.793485</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                               docno  rank      score  \\\n",
       "0   1   94858        2004.cikm_conference-2004.47     0  19.564883   \n",
       "1   1  125137   1989.ipm_journal-ir0volumeA25A4.2     1  19.211812   \n",
       "2   1  125817  2005.ipm_journal-ir0volumeA41A5.11     2  19.090084   \n",
       "3   1   84876       2016.ntcir_conference-2016.90     3  19.011221   \n",
       "4   1  124801   2006.ipm_journal-ir0volumeA42A3.2     4  18.919271   \n",
       "5   1   82472     1998.sigirconf_conference-98.15     5  18.856596   \n",
       "6   1   74513         2001.clef_workshop-2001w.24     6  18.825665   \n",
       "7   1   81840  2006.sigirconf_conference-2006.103     7  18.813841   \n",
       "8   1   82490     1998.sigirconf_conference-98.33     8  18.794527   \n",
       "9   1   17496                            O01-2005     9  18.793485   \n",
       "\n",
       "                                      query  \n",
       "0  retrieval system improving effectiveness  \n",
       "1  retrieval system improving effectiveness  \n",
       "2  retrieval system improving effectiveness  \n",
       "3  retrieval system improving effectiveness  \n",
       "4  retrieval system improving effectiveness  \n",
       "5  retrieval system improving effectiveness  \n",
       "6  retrieval system improving effectiveness  \n",
       "7  retrieval system improving effectiveness  \n",
       "8  retrieval system improving effectiveness  \n",
       "9  retrieval system improving effectiveness  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "run = bm25(test_dataset.get_topics('text'))\n",
    "\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "run.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes for docid 0: {'docno': 'O02-2002', 'text': 'a study on word similarity using context vector models there is a need to measure word similarity when processing natural languages especially when using generalization classification or example based approaches usually measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy the taxonomy approaches are more or less semantic based that do not consider syntactic similarit ies however in real applications both semantic and syntactic'}\n",
      "Attributes for docid 1: {'docno': 'L02-1310', 'text': 'bootstrapping large sense tagged corpora bootstrapping large large sense sense tagged tagged corpora bootstrapping large sense large sense tagged sense tagged corpora'}\n",
      "Attributes for docid 2: {'docno': 'R13-1042', 'text': 'headerless quoteless but not hopeless using pairwise email classification to disentangle email threads thread disentanglement is the task of separating out conversations whose thread structure is implicit distorted or lost in this paper we perform email thread disentanglement through pairwise classification using text similarity measures on non quoted texts in emails we show that i content text similarity metrics outperform style and structure text similarity metrics in both a class balanced and class imba'}\n",
      "Attributes for docid 3: {'docno': 'W05-0819', 'text': 'aligning words in e nglish h indi parallel corpora in this paper we describe a word alignment algorithm for english hindi parallel data the system was developed to participate in the shared task on word alignment for languages with scarce resources at the acl 2005 workshop on building and using parallel texts data driven machine translation and beyond our word alignment algorithm is based on a hybrid method which performs local word grouping on hindi sentences and uses other methods such as dictionary look'}\n",
      "Attributes for docid 4: {'docno': 'L02-1309', 'text': 'proposal of a very large corpus acquisition method by cell formed registration proposal of of a a very very large large corpus corpus acquisition acquisition method method by by cell cell formed formed registration proposal of a of a very a very large very large corpus large corpus acquisition corpus acquisition method acquisition method by method by cell by cell formed cell formed registration'}\n",
      "Attributes for docid 5868: {'docno': 'W05-0704', 'text': 'examining the effect of improved context sensitive morphology on a rabic information retrieval this paper explores the effect of improved morphological analysis particularly context sensitive morphology on monolingual arabic information retrieval ir it also compares the effect of context sensitive morphology to noncontext sensitive morphology the results show that better coverage and improved correctness have a dramatic effect on ir effectiveness and that context sensitive morphology further improves retri'}\n"
     ]
    }
   ],
   "source": [
    "index = pt.IndexFactory.of(index_ref)\n",
    "meta = index.getMetaIndex()\n",
    "\n",
    "# List of metadata keys\n",
    "meta_keys = ['docno', 'text']  # Adjust this list based on your meta fields\n",
    "\n",
    "# Function to print document attributes\n",
    "def print_doc_attributes(docid):\n",
    "    attributes = {key: meta.getItem(key, docid) for key in meta_keys}\n",
    "    print(f\"Attributes for docid {docid}: {attributes}\")\n",
    "\n",
    "# Example: Print attributes for the first 5 documents\n",
    "for docid in range(5):\n",
    "    print_doc_attributes(docid)\n",
    "\n",
    "# Function to print document attributes by docno\n",
    "def print_doc_attributes_by_docno(docno):\n",
    "    try:\n",
    "        docid = meta.getDocument(\"docno\", docno)\n",
    "        print_doc_attributes(docid)\n",
    "    except KeyError:\n",
    "        print(f\"Document with docno {docno} not found.\")\n",
    "\n",
    "# List of specific docnos to retrieve\n",
    "docnos = ['W05-0704']\n",
    "\n",
    "# Retrieve and print attributes for each specified docno\n",
    "for docno in docnos:\n",
    "    print_doc_attributes_by_docno(docno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
