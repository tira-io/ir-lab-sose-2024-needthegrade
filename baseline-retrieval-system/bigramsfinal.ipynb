{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tira in /usr/local/lib/python3.10/dist-packages (0.0.134)\n",
      "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
      "Requirement already satisfied: python-terrier in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
      "Requirement already satisfied: requests==2.*,>=2.26 in /usr/local/lib/python3.10/dist-packages (from tira) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tira) (4.66.1)\n",
<<<<<<< HEAD
      "Requirement already satisfied: docker==7.*,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from tira) (7.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tira) (23.2)\n",
      "Requirement already satisfied: numpy==1.* in /usr/local/lib/python3.10/dist-packages (from tira) (1.26.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker==7.*,>=7.1.0->tira) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.3.2)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (6.0.1)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.12)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.3.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.6)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.9.3)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.3.2)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (3.2.3)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.6)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.3)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.2)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.7)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.4)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: matchpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.4)\n",
=======
      "Requirement already satisfied: docker==6.*,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tira) (6.1.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (2.1.0)\n",
      "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (23.2)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (1.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (2023.11.17)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.3)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.3.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (6.0.1)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.3.2)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (3.2.3)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.9.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.12.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.6)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.2)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.12)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.6)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (1.26.2)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: matchpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.4)\n",
      "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.7)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
>>>>>>> df622b7c6b8019f6547576a2ab5a32e29ef8dd0b
      "Requirement already satisfied: typish>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
<<<<<<< HEAD
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
=======
      "Requirement already satisfied: regex>=2021.8.3 in /root/.local/lib/python3.10/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
>>>>>>> df622b7c6b8019f6547576a2ab5a32e29ef8dd0b
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.5)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /usr/local/lib/python3.10/dist-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.10/dist-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (2.1.3)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->python-terrier) (3.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (0.5.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install tira ir-datasets python-terrier nltk pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from tira.third_party_integrations import persist_and_normalize_run,  ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start pyterrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "# Initialize PyTerrier\n",
    "\n",
    "\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "#jvm_options = [\"-Xmx4g\"]\n",
    "#pt.init(jvm_opts=jvm_options)\n",
    "\n",
    "#start Tira\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dataset and Indexing\n",
    "First, we get the Dataset from pyterrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 52426.98it/s]"
=======
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 55144.06it/s]"
>>>>>>> df622b7c6b8019f6547576a2ab5a32e29ef8dd0b
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 126958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Get dataset\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "#Create Iterator of our dataset\n",
    "docs =  pt_dataset.get_corpus_iter()\n",
    "docs = list(docs)\n",
    "count = sum(1 for _ in docs)\n",
    "\n",
    "print(\"Number of documents:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we delete all but the first n documents. \n",
    "We need to do so as otherwise the notebook will crash.\n",
    "\n",
    "TODO: Use full dataset and try it in Tira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'A Study on Word Similarity using Context Vector Models\\n\\n\\n There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.', 'docno': 'O02-2002'}\n",
      "{'text': 'Bootstrapping Large Sense Tagged Corpora', 'docno': 'L02-1310'}\n",
      "{'text': 'Headerless, Quoteless, but not Hopeless? Using Pairwise Email Classification to Disentangle Email Threads\\n\\n\\n Thread disentanglement is the task of separating out conversations whose thread structure is implicit, distorted, or lost. In this paper, we perform email thread disentanglement through pairwise classification, using text similarity measures on non-quoted texts in emails. We show that i) content text similarity metrics outperform style and structure text similarity metrics in both a class-balanced and class-imbalanced setting, and ii) although feature performance is dependent on the semantic similarity of the corpus, content features are still effective even when controlling for semantic similarity. We make available the Enron Threads Corpus, a newly-extracted corpus of 70,178 multiemail threads with emails from the Enron Email Corpus.', 'docno': 'R13-1042'}\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "#Create List from our docs iterator\n",
    "#docs = list(docs)\n",
    "#cutoff the list after the first 500 documents\n",
    "#docs = docs[:126959]\n",
    "#docs = docs[89909:89912]\n",
    "\n",
    "#Print some example documents\n",
    "for i,doc in enumerate(docs):\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we process the documents in multiple steps.\n",
    "First, we remove all special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that removes all special characters from a String, and returns either a String or a list of all words\n",
    "def clean_text(text, return_as_list = False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Use regular expression to remove non-alphanumeric characters, except spaces\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    if return_as_list:\n",
    "        word_list = cleaned_text.split()\n",
    "        return word_list\n",
    "    else:\n",
    "        return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a method to remove all stopwords from our document text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Ensure NLTK data directory is set correctly\n",
    "nltk.data.path.append(\"/usr/local/nltk_data\")\n",
    "\n",
    "# Download 'stopwords' corpus to the specified directory\n",
    "nltk.download('stopwords', download_dir=\"/usr/nltk_data\")\n",
    "\n",
    "# Get the set of stopwords for the English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text, return_as_list = False):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Join the filtered words back into a single string\n",
    "\n",
    "    if return_as_list:\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a method to apply a Snowball Stemmer to the document text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef remove_stopwords_from_text(text):\\n    # Tokenize the text\\n    words = word_tokenize(text)\\n\\n    #filtered_and_stemmed_text = [stemmer.stem(word) for word in words not in stop_words]\\n\\n    # Filter out the stopwords\\n    filtered_and_stemmed_text = [word for word in words if word.lower() not in stop_words]\\n    # Reconstruct the string from the filtered words\\n    filtered_text = ' '.join(filtered_and_stemmed_text)\\n    return filtered_text\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_text(text, return_as_list = False):\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    if return_as_list:\n",
    "        return stemmed_words\n",
    "    # Join the stemmed words back into a single string\n",
    "    else:\n",
    "        return ' '.join(stemmed_words)\n",
    "'''\n",
    "def remove_stopwords_from_text(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    #filtered_and_stemmed_text = [stemmer.stem(word) for word in words not in stop_words]\n",
    "\n",
    "    # Filter out the stopwords\n",
    "    filtered_and_stemmed_text = [word for word in words if word.lower() not in stop_words]\n",
    "    # Reconstruct the string from the filtered words\n",
    "    filtered_text = ' '.join(filtered_and_stemmed_text)\n",
    "    return filtered_text\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define our ngram tokeniser method. We will use monograms, bigrams, and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our ngram tokenizer. It takes a string and returns a dict of all ngrams, where each ngram is seperated by $$ so it will be parsed as one token\n",
    "\n",
    "def tokenize_ngrams_to_dict(text, n1=1, n2=1):\n",
    "    # Replace spaces with dollar signs\n",
    "    #text_with_dollar_signs = re.sub(r'\\s+', '$', text)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    #words = text_with_dollar_signs.split('$')\n",
    "    words = text.split(' ')\n",
    "    words = [word for word in words if len(''.join(format(ord(c), '08b') for c in word)) <= 60]\n",
    "\n",
    "    # Initialize an empty Counter to hold all n-grams\n",
    "    all_ngram_counts = Counter()\n",
    "    \n",
    "    # Loop through each n from n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Update the Counter with the current n-grams\n",
    "        all_ngram_counts.update(ngrams)\n",
    "    \n",
    "    return dict(all_ngram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we process our dataset by applying the methods we just specified to our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'docno': 'O02-2002', 'toks': {'studi': 1, 'word': 7, 'similar': 8, 'use': 3, 'context': 5, 'vector': 3, 'model': 2, 'need': 1, 'measur': 2, 'process': 1, 'natur': 1, 'languag': 1, 'especi': 1, 'general': 1, 'classif': 1, 'exampl': 1, 'base': 3, 'usual': 1, 'two': 1, 'defin': 1, 'accord': 3, 'distanc': 1, 'semant': 6, 'class': 2, 'less': 1, 'consid': 1, 'syntact': 5, 'ie': 2, 'howev': 1, 'real': 1, 'applic': 1, 'requir': 1, 'weight': 1, 'differ': 1, 'mixtur': 1, 'paper': 1, 'propos': 1, 'relat': 1, 'co': 2, 'occurr': 2, 'adopt': 1, 'inform': 1, 'theoret': 1, 'solv': 1, 'problem': 1, 'data': 1, 'spars': 1, 'precis': 1, 'featur': 2, 'deriv': 1, 'pars': 1, 'environ': 1, 'adjust': 1, 'idf': 1, 'invers': 1, 'valu': 2, 'agglom': 1, 'cluster': 1, 'appli': 1, 'group': 2, 'turn': 1, 'togeth': 1}}\n",
      "{'docno': 'L02-1310', 'toks': {'larg': 1, 'sens': 1, 'tag': 1, 'corpora': 1}}\n",
      "{'docno': 'R13-1042', 'toks': {'use': 2, 'pairwis': 2, 'email': 6, 'classif': 2, 'thread': 6, 'task': 1, 'separ': 1, 'convers': 1, 'whose': 1, 'distort': 1, 'lost': 1, 'paper': 1, 'perform': 2, 'text': 4, 'similar': 5, 'measur': 1, 'non': 1, 'quot': 1, 'show': 1, 'content': 2, 'metric': 2, 'style': 1, 'class': 2, 'balanc': 1, 'set': 1, 'ii': 1, 'featur': 2, 'depend': 1, 'semant': 2, 'corpus': 4, 'still': 1, 'effect': 1, 'even': 1, 'control': 1, 'make': 1, 'avail': 1, 'enron': 2, 'newli': 1, 'extract': 1, '70': 1, '178': 1}}\n"
     ]
    }
   ],
   "source": [
    "# Apply n-gram tokenization to the dataset\n",
    "#This will delete the 'text' field from the documents,\n",
    "# and create a new 'toks' field which contains the tokens with their frequencies\n",
    "for doc in docs:\n",
    "        if 'text' in doc:\n",
    "            doc['text'] = clean_text(doc['text'])\n",
    "            doc['text'] = remove_stopwords(doc['text'])\n",
    "            doc['text'] = stem_text(doc['text'])\n",
    "            \n",
    "            doc_1gram = tokenize_ngrams_to_dict(doc['text'], n1=1, n2=1)\n",
    "\n",
    "            doc['toks'] = doc_1gram\n",
    "            del doc['text']  \n",
    "    \n",
    "for i, doc in enumerate(docs):\n",
    "     if i == 3:\n",
    "           break\n",
    "     print(doc)\n",
    "\n",
    "#remove all empty documents\n",
    "docs = [d for d in docs if any(k != '' for k in d['toks'].keys())]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'docno': 'O02-2002', 'toks': {'studi': 1, 'word': 7, 'similar': 8, 'use': 3, 'context': 5, 'vector': 3, 'model': 2, 'need': 1, 'measur': 2, 'process': 1, 'natur': 1, 'languag': 1, 'especi': 1, 'general': 1, 'classif': 1, 'exampl': 1, 'base': 3, 'usual': 1, 'two': 1, 'defin': 1, 'accord': 3, 'distanc': 1, 'semant': 6, 'class': 2, 'less': 1, 'consid': 1, 'syntact': 5, 'ie': 2, 'howev': 1, 'real': 1, 'applic': 1, 'requir': 1, 'weight': 1, 'differ': 1, 'mixtur': 1, 'paper': 1, 'propos': 1, 'relat': 1, 'co': 2, 'occurr': 2, 'adopt': 1, 'inform': 1, 'theoret': 1, 'solv': 1, 'problem': 1, 'data': 1, 'spars': 1, 'precis': 1, 'featur': 2, 'deriv': 1, 'pars': 1, 'environ': 1, 'adjust': 1, 'idf': 1, 'invers': 1, 'valu': 2, 'agglom': 1, 'cluster': 1, 'appli': 1, 'group': 2, 'turn': 1, 'togeth': 1}}\n",
      "{'docno': 'L02-1310', 'toks': {'larg': 1, 'sens': 1, 'tag': 1, 'corpora': 1}}\n",
      "{'docno': 'R13-1042', 'toks': {'use': 2, 'pairwis': 2, 'email': 6, 'classif': 2, 'thread': 6, 'task': 1, 'separ': 1, 'convers': 1, 'whose': 1, 'distort': 1, 'lost': 1, 'paper': 1, 'perform': 2, 'text': 4, 'similar': 5, 'measur': 1, 'non': 1, 'quot': 1, 'show': 1, 'content': 2, 'metric': 2, 'style': 1, 'class': 2, 'balanc': 1, 'set': 1, 'ii': 1, 'featur': 2, 'depend': 1, 'semant': 2, 'corpus': 4, 'still': 1, 'effect': 1, 'even': 1, 'control': 1, 'make': 1, 'avail': 1, 'enron': 2, 'newli': 1, 'extract': 1, '70': 1, '178': 1}}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our Index. \n",
    "We will use an IterDictIndexer with pretokenised=True, as we already created the tokens manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 126824\n",
      "Number of terms: 64344\n",
      "Number of postings: 5012369\n",
      "Number of fields: 0\n",
      "Number of tokens: 7407494\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n",
      "0\n",
      "9406\n",
      "00\n",
      "278\n",
      "000\n",
      "1760\n",
      "0000\n",
      "5\n"
     ]
    }
   ],
>>>>>>> df622b7c6b8019f6547576a2ab5a32e29ef8dd0b
   "source": [
    "\n",
    "# Initialize the IterDictIndexer with pretokenised set to True\n",
    "iter_indexer = pt.IterDictIndexer(\"./ngramindex\", overwrite=True, meta={'docno': 35}, pretokenised=True, verbose = True)#, type = pt.index.IndexingType.SINGLEPASS)\n",
    "\n",
    "# Index our pretokenized documents\n",
    "index_ref = iter_indexer.index(docs)\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "#Print some stats about our index\n",
    "print(index.getCollectionStatistics())\n",
    "\n",
    "# Access the MetaIndex and Lexicon\n",
    "meta = index.getMetaIndex()\n",
    "lexicon = index.getLexicon()\n",
    "\n",
    "#Print some example terms from the index. We see that numbers arent removed\n",
    "i = 0\n",
    "for term, le in index.getLexicon():\n",
    "    i = i+1\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(term) \n",
    "    print(le.getFrequency())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the retrieval pipeline\n",
    "First, we define a new method that works exactly like the index ngram tokeniser method, just that it will return a list of tokens instead of a dictionary, and it will not count each token.\n",
    "\n",
    "This method will be used as one of the transformers for our retrieval pipeline and it is responsible for tokenising the query in the same format as we tokenized our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a string and returns a list with all ngrams\n",
    "def tokenize_ngrams_to_list(text, n1=1, n2=1):\n",
    "    words = text.split()\n",
    "\n",
    "    # Initialize an empty list to hold all n-grams\n",
    "    all_ngrams = []\n",
    "    \n",
    "    # Loop through each n between n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Add all current ngrams to the all_ngrams list\n",
    "        all_ngrams.extend(ngrams)\n",
    "    \n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieval system improving effectiveness retrieval$$system\n",
      "['retrieval', 'system', 'improving', 'effectiveness', 'retrieval$$system']\n",
      "['machine$$learning', 'machine']\n",
      "run$$shoe are better than rune $$fast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#1. Chatgpt anwenden\n",
    "\n",
    "expanded_queries = [['1', 'retrieval system improving effectiveness', 'retrieval system improving effectiveness retrieval$$system'], ['2', 'machine learning language identification', 'machine learning language identification machine$$learning language$$identification'], ['3', 'social media detect self harm', 'social media detect self harm social$$media self$$harm'], ['4', 'stemming for arabic languages', 'stemming for arabic languages arabic$$languages'], ['5', 'audio based animal recognition', 'audio based animal recognition audio$$based animal$$recognition'], ['6', 'comparison different retrieval models', 'comparison different retrieval models retrieval$$models'], ['7', 'cache architecture', 'cache architecture'], ['8', 'document scoping formula', 'document scoping formula document$$scoping'], ['9', 'pseudo relevance feedback', 'pseudo relevance feedback pseudo$$relevance$$feedback pseudo$$relevance relevance$$feedback'], ['10', 'how to represent natural conversations in word nets', 'how to represent natural conversations in word nets natural$$conversations word$$nets'], ['11', 'algorithm acceleration with nvidia cuda', 'algorithm acceleration with nvidia cuda nvidia$$cuda'], ['12', 'mention of algorithm', 'mention of algorithm'], ['13', 'at least three authors', 'at least three authors three$$authors'], ['14', 'german domain', 'german domain german$$domain'], ['15', 'mention of open source', 'mention of open source open$$source'], ['16', 'inclusion of text mining', 'inclusion of text mining text$$mining'], ['17', 'the ethics of artificial intelligence', 'the ethics of artificial intelligence artificial$$intelligence'], ['19', 'machine learning for more relevant results', 'machine learning for more relevant results machine$$learning relevant$$results'], ['20', 'crawling websites using machine learning', 'crawling websites using machine learning machine$$learning'], ['21', 'recommenders influence on users', 'recommenders influence on users'], ['22', 'search engine caching effects', 'search engine caching effects search$$engine caching$$effects'], ['23', 'consumer product reviews', 'consumer product reviews consumer$$product product$$reviews'], ['24', 'limitations machine learning', 'limitations machine learning machine$$learning'], ['25', 'medicine related research', 'medicine related research medicine$$related related$$research'], ['26', 'natural language processing', 'natural language processing natural$$language$$processing natural$$language'], ['27', 'graph based ranking', 'graph based ranking graph$$based based$$ranking'], ['28', 'medical studies that use information retrieval', 'medical studies that use information retrieval information$$retrieval'], ['29', 'information retrieval on different language sources', 'information retrieval on different language sources information$$retrieval language$$sources'], ['30', 'papers that compare multiple information retrieval methods', 'papers that compare multiple information retrieval methods information$$retrieval'], ['31', 'risks of information retrieval in social media', 'risks of information retrieval in social media information$$retrieval social$$media'], ['32', 'actual experiments that strengthen theoretical knowledge', 'actual experiments that strengthen theoretical knowledge theoretical$$knowledge'], ['33', 'fake news detection', 'fake news detection fake$$news news$$detection'], ['34', 'multimedia retrieval', 'multimedia retrieval multimedia$$retrieval'], ['35', 'processing natural language for information retrieval', 'processing natural language for information retrieval natural$$language information$$retrieval'], ['36', 'recommendation systems', 'recommendation systems recommendation$$systems'], ['37', 'personalised search in e commerce', 'personalised search in e commerce personalised$$search e$$commerce'], ['38', 'sentiment analysis', 'sentiment analysis sentiment$$analysis'], ['39', 'informational retrieval using neural networks', 'informational retrieval using neural networks informational$$retrieval neural$$networks'], ['40', 'query log analysis', 'query log analysis query$$log log$$analysis'], ['41', 'entity recognition', 'entity recognition entity$$recognition'], ['42', 'relevance assessments', 'relevance assessments relevance$$assessments'], ['43', 'deep neural networks', 'deep neural networks deep$$neural neural$$networks'], ['44', 'information retrieval', 'information retrieval information$$retrieval'], ['45', 'analysis for android apps', 'analysis for android apps android$$apps'], ['46', 'the university of amsterdam', 'the university of amsterdam university$$of of$$amsterdam'], ['47', 'neural ranking for ecommerce product search', 'neural ranking for ecommerce product search neural$$ranking product$$search'], ['48', 'web pages evolution', 'web pages evolution web$$pages'], ['49', 'exhaustivity of index', 'exhaustivity of index'], ['50', 'query optimization', 'query optimization query$$optimization'], ['51', 'cosine similarity vector', 'cosine similarity vector cosine$$similarity'], ['52', 'reverse indexing', 'reverse indexing reverse$$indexing'], ['53', 'index compression techniques', 'index compression techniques index$$compression compression$$techniques'], ['54', 'search engine optimization with query logs', 'search engine optimization with query logs search$$engine engine$$optimization query$$logs'], ['55', 'bm25', 'bm25'], ['56', 'what makes natural language processing natural', 'what makes natural language processing natural natural$$language$$processing natural$$language'], ['57', 'principle of a information retrieval indexing', 'principle of a information retrieval indexing information$$retrieval'], ['58', 'architecture of web search engine', 'architecture of web search engine web$$search search$$engine'], ['59', 'what is ahp', 'what is ahp'], ['60', 'what is information retrieval', 'what is information retrieval information$$retrieval'], ['61', 'efficient retrieval algorithms', 'efficient retrieval algorithms retrieval$$algorithms'], ['62', 'how to avoid spam results', 'how to avoid spam results spam$$results'], ['63', 'information retrieval with algorithms', 'information retrieval with algorithms information$$retrieval'], ['64', 'misspellings in queries', 'misspellings in queries'], ['65', 'information in different language', 'information in different language different$$language'], ['66', 'abbreviations in queries', 'abbreviations in queries'], ['67', 'lemmatization algorithms', 'lemmatization algorithms'], ['68', 'filter ad rich documents', 'filter ad rich documents ad$$rich'], ['18', 'advancements in information retrieval', 'advancements in information retrieval information$$retrieval']]\n",
    "\n",
    "# Extract qid and query from each sublist\n",
    "data_expd = [(sublist[0], sublist[2]) for sublist in expanded_queries]\n",
    "\n",
    "# Create pandas DataFrame\n",
    "df_expd = pd.DataFrame(data_expd, columns=['qid', 'query'])\n",
    "#TODO hier chatgpt-methode einfügen, die jedes mal die expanded queries liste neu generiert, da wir auch andere queries haben könnten\n",
    "\n",
    "#Nimmt eine qid als String und returnt die expanded query von chatgpt\n",
    "def get_expanded_query(qid):\n",
    "    for query_list in expanded_queries:\n",
    "        if query_list[0] == qid:\n",
    "            return query_list[2]\n",
    "    return None\n",
    "\n",
    "print(get_expanded_query('1'))\n",
    "\n",
    "#TODO über die liste ist es nicht so gut, da wir in der pipeline eh jede query einzeln als string behandeln\n",
    "#daher hier die chatgpt methode in the transformer einfügen, und dann am ende noch als liste tokenisen\n",
    "#expand_query_transf = pt.rewrite.tokenise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2. special characters löschen ausser $$\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text_with_dollar_signs(text, return_as_list=False, keep_dollar_signs=False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    if keep_dollar_signs:\n",
    "        # Replace double dollar signs with a unique placeholder\n",
    "        text = text.replace('$$', 'DOUBLEDOLLARNGRAMS')\n",
    "        # Remove all non-alphanumeric characters except spaces\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "        # Replace placeholder back to double dollar signs\n",
    "        cleaned_text = cleaned_text.replace('DOUBLEDOLLARNGRAMS', '$$')\n",
    "    else:\n",
    "        # Remove all non-alphanumeric characters except spaces\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    if return_as_list:\n",
    "        word_list = cleaned_text.split()\n",
    "        return word_list\n",
    "    else:\n",
    "        return cleaned_text\n",
    "\n",
    "# Examples\n",
    "print(clean_text_with_dollar_signs(get_expanded_query('1'), keep_dollar_signs=True, return_as_list=True))\n",
    "\n",
    "transf_clean_text = pt.rewrite.tokenise(lambda query: clean_text_with_dollar_signs(query, return_as_list=True, keep_dollar_signs=True))\n",
    "\n",
    "#3. stopwords löschen\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Ensure NLTK data directory is set correctly\n",
    "nltk.data.path.append(\"/usr/local/nltk_data\")\n",
    "\n",
    "# Download 'stopwords' corpus to the specified directory\n",
    "nltk.download('stopwords', download_dir=\"/usr/nltk_data\")\n",
    "\n",
    "# Get the set of stopwords for the English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords_with_dollar_signs(text, return_as_list=False):\n",
    "    words = text.split()\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if '$$' in word:\n",
    "            # Split the word at '$$' and check both parts\n",
    "            part1, part2 = word.split('$$')\n",
    "            if part1.lower() in stop_words or part2.lower() in stop_words:\n",
    "                continue  # Skip this word if either part is a stopword\n",
    "            else:\n",
    "                filtered_words.append(word)\n",
    "        else:\n",
    "            if word.lower() not in stop_words:\n",
    "                filtered_words.append(word)\n",
    "    \n",
    "    if return_as_list:\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "transf_remove_stopwords = pt.rewrite.tokenise(lambda query: remove_stopwords_with_dollar_signs(query, return_as_list=True))\n",
    "# Examples\n",
    "print(remove_stopwords_with_dollar_signs(\"machine$$learning of learning$$of machine\", return_as_list=True))\n",
    "\n",
    "#4. stemming\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_text_with_dollar_signs(text, return_as_list=False):\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        if '$$' in word:\n",
    "            # Split the word at '$$' and stem each part\n",
    "            part1, part2 = word.split('$$')\n",
    "            stemmed_part1 = stemmer.stem(part1)\n",
    "            stemmed_part2 = stemmer.stem(part2)\n",
    "            stemmed_word = f\"{stemmed_part1}$${stemmed_part2}\"\n",
    "        else:\n",
    "            # Stem the word normally\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "        \n",
    "        stemmed_words.append(stemmed_word)\n",
    "\n",
    "    if return_as_list:\n",
    "        return stemmed_words\n",
    "    else:\n",
    "        return ' '.join(stemmed_words)\n",
    "    \n",
    "\n",
    "transf_stem_text = pt.rewrite.tokenise(lambda query: stem_text_with_dollar_signs(query, return_as_list=True))\n",
    "# Example\n",
    "print(stem_text(\"running$$shoes are better than runing $$fast\", return_as_list=False))\n",
    "\n",
    "#5. bm25 ohne termpipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can finally define the actual retrieval pipeline. It consists of two steps; first, we tokenize the queries into ngrams, and after that, we apply bm25 to the query with our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_special_characters = pt.rewrite.tokenise(lambda query: clean_text(query, return_as_list=True), matchop=True)\n",
    "remove_stopwords_from_query = pt.rewrite.tokenise(lambda query: remove_stopwords(query, return_as_list=True),matchop=True)\n",
    "stem_query = pt.rewrite.tokenise(lambda query: stem_text(query, return_as_list=True), matchop=True)\n",
    "# This transformer will tokenise the queries into the ngrams\n",
    "tokenise_query_ngram = pt.rewrite.tokenise(lambda query: tokenize_ngrams_to_list(query), matchop=True)\n",
    "\n",
    "# This transformer will do the retrieval using bm25, and explicitly not apply any stemming and stopword removal\n",
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", verbose = True, properties={\"termpipelines\" : \"\"})\n",
    "\n",
    "# This is our retrieval pipeline\n",
    "#retr_pipeline = remove_special_characters >> remove_stopwords_from_query >> stem_query >> tokenise_query_ngram >> bm25\n",
    "\n",
    "retr_pipeline = transf_clean_text >> transf_remove_stopwords >> transf_remove_stopwords >> bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n",
      "Index(['qid', 'text', 'title', 'query', 'description', 'narrative'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>machine learning language identification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>social media detect self harm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                                     query\n",
       "0   1  retrieval system improving effectiveness\n",
       "1   2  machine learning language identification\n",
       "2   3             social media detect self harm"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at some example queries\n",
    "print(pt_dataset.get_topics().columns)\n",
    "pt_dataset.get_topics('query').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ndf = pd.DataFrame(pt_dataset.get_topics())\\nif 'query' not in df.columns:\\n    df['query'] = df['text']\\n\\n\\n# Convert the DataFrame to a list of dictionaries\\nqueries = df[['qid', 'query']].to_dict(orient='records')\\n\\n# Print the result\\nprint(queries)\\n\\n#TODO sonderzeichen aus query löschen\\ndef clean_queries(queries):\\n    for query in queries:\\n        if 'query' in query:\\n            query['query'] = clean_text(query['query'])\\nclean_queries(queries)\\nfor query in queries:\\n        if 'query' in query:\\n            query['query'] = remove_stopwords(query['query'])\\n            query['query'] = stem_text(query['query'])\\n\\nfor i,query in enumerate(queries):\\n   \\n\\n    query_ngram = tokenize_ngrams_to_dict(query['query'], n1=1, n2=3)\\n\\n    query['query'] = query_ngram\\n  \\nfor query in queries:\\n     print(query)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not needed anymore since we process the queries directly in the pipeline\n",
    "'''\n",
    "\n",
    "df = pd.DataFrame(pt_dataset.get_topics())\n",
    "if 'query' not in df.columns:\n",
    "    df['query'] = df['text']\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "queries = df[['qid', 'query']].to_dict(orient='records')\n",
    "\n",
    "# Print the result\n",
    "print(queries)\n",
    "\n",
    "#TODO sonderzeichen aus query löschen\n",
    "def clean_queries(queries):\n",
    "    for query in queries:\n",
    "        if 'query' in query:\n",
    "            query['query'] = clean_text(query['query'])\n",
    "clean_queries(queries)\n",
    "for query in queries:\n",
    "        if 'query' in query:\n",
    "            query['query'] = remove_stopwords(query['query'])\n",
    "            query['query'] = stem_text(query['query'])\n",
    "\n",
    "for i,query in enumerate(queries):\n",
    "   \n",
    "\n",
    "    query_ngram = tokenize_ngrams_to_dict(query['query'], n1=1, n2=3)\n",
    "\n",
    "    query['query'] = query_ngram\n",
    "  \n",
    "for query in queries:\n",
    "     print(query)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n",
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25):   0%|          | 0/68 [00:00<?, ?q/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25): 100%|██████████| 68/68 [00:02<00:00, 27.04q/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Here are the first 10 entries of the run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>query_3</th>\n",
       "      <th>description</th>\n",
       "      <th>narrative</th>\n",
       "      <th>query_2</th>\n",
       "      <th>query_1</th>\n",
       "      <th>query_0</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>94795</td>\n",
       "      <td>2004.cikm_conference-2004.47</td>\n",
       "      <td>0</td>\n",
       "      <td>15.528247</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>125005</td>\n",
       "      <td>1989.ipm_journal-ir0volumeA25A4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>14.961664</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5867</td>\n",
       "      <td>W05-0704</td>\n",
       "      <td>2</td>\n",
       "      <td>14.057796</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>124670</td>\n",
       "      <td>2006.ipm_journal-ir0volumeA42A3.2</td>\n",
       "      <td>3</td>\n",
       "      <td>13.966575</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>125684</td>\n",
       "      <td>2005.ipm_journal-ir0volumeA41A5.11</td>\n",
       "      <td>4</td>\n",
       "      <td>13.846308</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>84816</td>\n",
       "      <td>2016.ntcir_conference-2016.90</td>\n",
       "      <td>5</td>\n",
       "      <td>13.810891</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>94352</td>\n",
       "      <td>2008.cikm_conference-2008.183</td>\n",
       "      <td>6</td>\n",
       "      <td>13.739452</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>82414</td>\n",
       "      <td>1998.sigirconf_conference-98.15</td>\n",
       "      <td>7</td>\n",
       "      <td>13.694661</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>17491</td>\n",
       "      <td>O01-2005</td>\n",
       "      <td>8</td>\n",
       "      <td>13.615381</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>73857</td>\n",
       "      <td>2007.ntcir_workshop-2007.65</td>\n",
       "      <td>9</td>\n",
       "      <td>13.554828</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                               docno  rank      score  \\\n",
       "0   1   94795        2004.cikm_conference-2004.47     0  15.528247   \n",
       "1   1  125005   1989.ipm_journal-ir0volumeA25A4.2     1  14.961664   \n",
       "2   1    5867                            W05-0704     2  14.057796   \n",
       "3   1  124670   2006.ipm_journal-ir0volumeA42A3.2     3  13.966575   \n",
       "4   1  125684  2005.ipm_journal-ir0volumeA41A5.11     4  13.846308   \n",
       "5   1   84816       2016.ntcir_conference-2016.90     5  13.810891   \n",
       "6   1   94352       2008.cikm_conference-2008.183     6  13.739452   \n",
       "7   1   82414     1998.sigirconf_conference-98.15     7  13.694661   \n",
       "8   1   17491                            O01-2005     8  13.615381   \n",
       "9   1   73857         2007.ntcir_workshop-2007.65     9  13.554828   \n",
       "\n",
       "                                       text  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                      title  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                    query_3  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                         description  \\\n",
       "0  What papers focus on improving the effectivene...   \n",
       "1  What papers focus on improving the effectivene...   \n",
       "2  What papers focus on improving the effectivene...   \n",
       "3  What papers focus on improving the effectivene...   \n",
       "4  What papers focus on improving the effectivene...   \n",
       "5  What papers focus on improving the effectivene...   \n",
       "6  What papers focus on improving the effectivene...   \n",
       "7  What papers focus on improving the effectivene...   \n",
       "8  What papers focus on improving the effectivene...   \n",
       "9  What papers focus on improving the effectivene...   \n",
       "\n",
       "                                           narrative  \\\n",
       "0  Relevant papers include research on what makes...   \n",
       "1  Relevant papers include research on what makes...   \n",
       "2  Relevant papers include research on what makes...   \n",
       "3  Relevant papers include research on what makes...   \n",
       "4  Relevant papers include research on what makes...   \n",
       "5  Relevant papers include research on what makes...   \n",
       "6  Relevant papers include research on what makes...   \n",
       "7  Relevant papers include research on what makes...   \n",
       "8  Relevant papers include research on what makes...   \n",
       "9  Relevant papers include research on what makes...   \n",
       "\n",
       "                                    query_2  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                    query_1                       query_0  \\\n",
       "0  retrieval system improving effectiveness  retriev system improv effect   \n",
       "1  retrieval system improving effectiveness  retriev system improv effect   \n",
       "2  retrieval system improving effectiveness  retriev system improv effect   \n",
       "3  retrieval system improving effectiveness  retriev system improv effect   \n",
       "4  retrieval system improving effectiveness  retriev system improv effect   \n",
       "5  retrieval system improving effectiveness  retriev system improv effect   \n",
       "6  retrieval system improving effectiveness  retriev system improv effect   \n",
       "7  retrieval system improving effectiveness  retriev system improv effect   \n",
       "8  retrieval system improving effectiveness  retriev system improv effect   \n",
       "9  retrieval system improving effectiveness  retriev system improv effect   \n",
       "\n",
       "                          query  \n",
       "0  retriev system improv effect  \n",
       "1  retriev system improv effect  \n",
       "2  retriev system improv effect  \n",
       "3  retriev system improv effect  \n",
       "4  retriev system improv effect  \n",
       "5  retriev system improv effect  \n",
       "6  retriev system improv effect  \n",
       "7  retriev system improv effect  \n",
       "8  retriev system improv effect  \n",
       "9  retriev system improv effect  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "#run = retr_pipeline(pt_dataset.get_topics()) #queries\n",
    "run = retr_pipeline(df_expd)\n",
    "\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "run.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist the run file for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs/\".\n",
      "Done. run file is stored under \"../runs//run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='ngrams', default_output='../runs/')\n",
    "\n",
    "import os\n",
    "os.rename('../runs/run.txt', '../runs/runngram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queries = [\\n    {\\'qid\\':1 , \\'query\\':\\'machine learning\\'},\\n    {\\'qid\\':2 , \\'query\\':\\'natural language processing techniques\\'}\\n]\\n\\n# Print the new query representation with ngrams included. This is how our query will get passed to bm25\\ndf = pd.DataFrame(queries)\\nprint(df)\\ntransformed_df = tokenise_query_ngram.transform(df)\\nprint(\"Transformed:\")\\nprint(transformed_df)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''queries = [\n",
    "    {'qid':1 , 'query':'machine learning'},\n",
    "    {'qid':2 , 'query':'natural language processing techniques'}\n",
    "]\n",
    "\n",
    "# Print the new query representation with ngrams included. This is how our query will get passed to bm25\n",
    "df = pd.DataFrame(queries)\n",
    "print(df)\n",
    "transformed_df = tokenise_query_ngram.transform(df)\n",
    "print(\"Transformed:\")\n",
    "print(transformed_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for index, row in transformed_df.iterrows():\\n    query_id = row[\\'qid\\']\\n    query_text = row[\\'query\\']\\n    print(\"test\")\\n    print(f\"Processing query ID {query_id} with text: {query_text}\")\\n    \\n    # Execute the search\\n    results = retr_pipeline.search(query_text)\\n    \\n    # Print or process the results\\n    print(f\"Results for query ID {query_id}:\")\\n    print(results)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for index, row in transformed_df.iterrows():\n",
    "    query_id = row['qid']\n",
    "    query_text = row['query']\n",
    "    print(\"test\")\n",
    "    print(f\"Processing query ID {query_id} with text: {query_text}\")\n",
    "    \n",
    "    # Execute the search\n",
    "    results = retr_pipeline.search(query_text)\n",
    "    \n",
    "    # Print or process the results\n",
    "    print(f\"Results for query ID {query_id}:\")\n",
    "    print(results)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
