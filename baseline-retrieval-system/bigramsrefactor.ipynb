{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from tira.third_party_integrations import persist_and_normalize_run,  ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start pyterrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "# Initialize PyTerrier\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "\n",
    "#start Tira\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dataset and Indexing\n",
    "First, we get the Dataset from pyterrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 52751.91it/s]\n"
     ]
    }
   ],
   "source": [
    "#Get dataset\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "#Create Iterator of our dataset\n",
    "docs =  pt_dataset.get_corpus_iter()\n",
    "docs = list(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we delete all but the first n documents. \n",
    "We need to do so as otherwise the notebook will crash.\n",
    "\n",
    "TODO: Use full dataset and try it in Tira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'A Study on Word Similarity using Context Vector Models\\n\\n\\n There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.', 'docno': 'O02-2002'}\n",
      "{'text': 'Bootstrapping Large Sense Tagged Corpora', 'docno': 'L02-1310'}\n",
      "{'text': 'Headerless, Quoteless, but not Hopeless? Using Pairwise Email Classification to Disentangle Email Threads\\n\\n\\n Thread disentanglement is the task of separating out conversations whose thread structure is implicit, distorted, or lost. In this paper, we perform email thread disentanglement through pairwise classification, using text similarity measures on non-quoted texts in emails. We show that i) content text similarity metrics outperform style and structure text similarity metrics in both a class-balanced and class-imbalanced setting, and ii) although feature performance is dependent on the semantic similarity of the corpus, content features are still effective even when controlling for semantic similarity. We make available the Enron Threads Corpus, a newly-extracted corpus of 70,178 multiemail threads with emails from the Enron Email Corpus.', 'docno': 'R13-1042'}\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "#Create List from our docs iterator\n",
    "#docs = list(docs)\n",
    "#cutoff the list after the first 500 documents\n",
    "#docs = docs[:500]\n",
    "\n",
    "#Print some example documents\n",
    "for i,doc in enumerate(docs):\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we process the documents in multiple steps.\n",
    "First, we remove all special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that removes all special characters from a String, and returns either a String or a list of all words\n",
    "def clean_text(text, return_as_list = False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Use regular expression to remove non-alphanumeric characters, except spaces\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    if return_as_list:\n",
    "        word_list = cleaned_text.split()\n",
    "        return word_list\n",
    "    else:\n",
    "        return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a method to remove all stopwords from our document text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the set of stopwords for the English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text, return_as_list = False):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Join the filtered words back into a single string\n",
    "\n",
    "    if return_as_list:\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a method to apply a Snowball Stemmer to the document text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef remove_stopwords_from_text(text):\\n    # Tokenize the text\\n    words = word_tokenize(text)\\n\\n    #filtered_and_stemmed_text = [stemmer.stem(word) for word in words not in stop_words]\\n\\n    # Filter out the stopwords\\n    filtered_and_stemmed_text = [word for word in words if word.lower() not in stop_words]\\n    # Reconstruct the string from the filtered words\\n    filtered_text = ' '.join(filtered_and_stemmed_text)\\n    return filtered_text\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_text(text, return_as_list = False):\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    if return_as_list:\n",
    "        return stemmed_words\n",
    "    # Join the stemmed words back into a single string\n",
    "    else:\n",
    "        return ' '.join(stemmed_words)\n",
    "'''\n",
    "def remove_stopwords_from_text(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    #filtered_and_stemmed_text = [stemmer.stem(word) for word in words not in stop_words]\n",
    "\n",
    "    # Filter out the stopwords\n",
    "    filtered_and_stemmed_text = [word for word in words if word.lower() not in stop_words]\n",
    "    # Reconstruct the string from the filtered words\n",
    "    filtered_text = ' '.join(filtered_and_stemmed_text)\n",
    "    return filtered_text\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define our ngram tokeniser method. We will use monograms, bigrams, and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our ngram tokenizer. It takes a string and returns a dict of all ngrams, where each ngram is seperated by $$ so it will be parsed as one token\n",
    "\n",
    "def tokenize_ngrams_to_dict(text, n1=1, n2=3):\n",
    "    # Replace spaces with dollar signs\n",
    "    #text_with_dollar_signs = re.sub(r'\\s+', '$', text)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    #words = text_with_dollar_signs.split('$')\n",
    "    words = text.split(' ')\n",
    "    # Initialize an empty Counter to hold all n-grams\n",
    "    all_ngram_counts = Counter()\n",
    "    \n",
    "    # Loop through each n from n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Update the Counter with the current n-grams\n",
    "        all_ngram_counts.update(ngrams)\n",
    "    \n",
    "    return dict(all_ngram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we process our dataset by applying the methods we just specified to our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'docno': 'O02-2002', 'toks': {'studi': 1, 'word': 7, 'similar': 8, 'use': 3, 'context': 5, 'vector': 3, 'model': 2, 'need': 1, 'measur': 2, 'process': 1, 'natur': 1, 'languag': 1, 'especi': 1, 'general': 1, 'classif': 1, 'exampl': 1, 'base': 3, 'approach': 2, 'usual': 1, 'two': 1, 'defin': 1, 'accord': 3, 'distanc': 1, 'semant': 6, 'class': 2, 'taxonomi': 2, 'less': 1, 'consid': 1, 'syntact': 5, 'similarit': 2, 'ie': 2, 'howev': 1, 'real': 1, 'applic': 1, 'requir': 1, 'weight': 1, 'differ': 1, 'mixtur': 1, 'paper': 1, 'propos': 1, 'relat': 1, 'co': 2, 'occurr': 2, 'adopt': 1, 'inform': 1, 'theoret': 1, 'solv': 1, 'problem': 1, 'data': 1, 'spars': 1, 'characterist': 1, 'precis': 1, 'probabilist': 1, 'distribut': 1, 'featur': 2, 'deriv': 1, 'pars': 1, 'contextu': 1, 'environ': 1, 'adjust': 1, 'idf': 1, 'invers': 1, 'document': 1, 'frequenc': 1, 'valu': 2, 'agglom': 1, 'cluster': 1, 'algorithm': 1, 'appli': 1, 'group': 2, 'turn': 1, 'categori': 1, 'togeth': 1, 'studi$$word': 1, 'word$$similar': 4, 'similar$$use': 1, 'use$$context': 1, 'context$$vector': 3, 'vector$$model': 1, 'model$$need': 1, 'need$$measur': 1, 'measur$$word': 1, 'similar$$process': 1, 'process$$natur': 1, 'natur$$languag': 1, 'languag$$especi': 1, 'especi$$use': 1, 'use$$general': 1, 'general$$classif': 1, 'classif$$exampl': 1, 'exampl$$base': 1, 'base$$approach': 1, 'approach$$usual': 1, 'usual$$measur': 1, 'measur$$similar': 1, 'similar$$two': 1, 'two$$word': 1, 'word$$defin': 1, 'defin$$accord': 1, 'accord$$distanc': 1, 'distanc$$semant': 1, 'semant$$class': 2, 'class$$semant': 1, 'semant$$taxonomi': 1, 'taxonomi$$taxonomi': 1, 'taxonomi$$approach': 1, 'approach$$less': 1, 'less$$semant': 1, 'semant$$base': 1, 'base$$consid': 1, 'consid$$syntact': 1, 'syntact$$similarit': 1, 'similarit$$ie': 2, 'ie$$howev': 1, 'howev$$real': 1, 'real$$applic': 1, 'applic$$semant': 1, 'semant$$syntact': 1, 'syntact$$similar': 1, 'similar$$requir': 1, 'requir$$weight': 1, 'weight$$differ': 1, 'differ$$word': 1, 'similar$$base': 1, 'base$$context': 1, 'vector$$mixtur': 1, 'mixtur$$syntact': 1, 'syntact$$semant': 1, 'semant$$similarit': 1, 'ie$$paper': 1, 'paper$$propos': 1, 'propos$$use': 1, 'use$$syntact': 1, 'syntact$$relat': 1, 'relat$$co': 1, 'co$$occurr': 2, 'occurr$$context': 2, 'vector$$adopt': 1, 'adopt$$inform': 1, 'inform$$theoret': 1, 'theoret$$model': 1, 'model$$solv': 1, 'solv$$problem': 1, 'problem$$data': 1, 'data$$spars': 1, 'spars$$characterist': 1, 'characterist$$precis': 1, 'precis$$probabilist': 1, 'probabilist$$distribut': 1, 'distribut$$co': 1, 'context$$featur': 2, 'featur$$deriv': 1, 'deriv$$pars': 1, 'pars$$contextu': 1, 'contextu$$environ': 1, 'environ$$word': 1, 'word$$context': 1, 'featur$$adjust': 1, 'adjust$$accord': 1, 'accord$$idf': 1, 'idf$$invers': 1, 'invers$$document': 1, 'document$$frequenc': 1, 'frequenc$$valu': 1, 'valu$$agglom': 1, 'agglom$$cluster': 1, 'cluster$$algorithm': 1, 'algorithm$$appli': 1, 'appli$$group': 1, 'group$$similar': 1, 'similar$$word': 1, 'word$$accord': 1, 'accord$$similar': 1, 'similar$$valu': 1, 'valu$$turn': 1, 'turn$$word': 1, 'similar$$syntact': 1, 'syntact$$categori': 1, 'categori$$semant': 1, 'class$$group': 1, 'group$$togeth': 1, 'studi$$word$$similar': 1, 'word$$similar$$use': 1, 'similar$$use$$context': 1, 'use$$context$$vector': 1, 'context$$vector$$model': 1, 'vector$$model$$need': 1, 'model$$need$$measur': 1, 'need$$measur$$word': 1, 'measur$$word$$similar': 1, 'word$$similar$$process': 1, 'similar$$process$$natur': 1, 'process$$natur$$languag': 1, 'natur$$languag$$especi': 1, 'languag$$especi$$use': 1, 'especi$$use$$general': 1, 'use$$general$$classif': 1, 'general$$classif$$exampl': 1, 'classif$$exampl$$base': 1, 'exampl$$base$$approach': 1, 'base$$approach$$usual': 1, 'approach$$usual$$measur': 1, 'usual$$measur$$similar': 1, 'measur$$similar$$two': 1, 'similar$$two$$word': 1, 'two$$word$$defin': 1, 'word$$defin$$accord': 1, 'defin$$accord$$distanc': 1, 'accord$$distanc$$semant': 1, 'distanc$$semant$$class': 1, 'semant$$class$$semant': 1, 'class$$semant$$taxonomi': 1, 'semant$$taxonomi$$taxonomi': 1, 'taxonomi$$taxonomi$$approach': 1, 'taxonomi$$approach$$less': 1, 'approach$$less$$semant': 1, 'less$$semant$$base': 1, 'semant$$base$$consid': 1, 'base$$consid$$syntact': 1, 'consid$$syntact$$similarit': 1, 'syntact$$similarit$$ie': 1, 'similarit$$ie$$howev': 1, 'ie$$howev$$real': 1, 'howev$$real$$applic': 1, 'real$$applic$$semant': 1, 'applic$$semant$$syntact': 1, 'semant$$syntact$$similar': 1, 'syntact$$similar$$requir': 1, 'similar$$requir$$weight': 1, 'requir$$weight$$differ': 1, 'weight$$differ$$word': 1, 'differ$$word$$similar': 1, 'word$$similar$$base': 1, 'similar$$base$$context': 1, 'base$$context$$vector': 1, 'context$$vector$$mixtur': 1, 'vector$$mixtur$$syntact': 1, 'mixtur$$syntact$$semant': 1, 'syntact$$semant$$similarit': 1, 'semant$$similarit$$ie': 1, 'similarit$$ie$$paper': 1, 'ie$$paper$$propos': 1, 'paper$$propos$$use': 1, 'propos$$use$$syntact': 1, 'use$$syntact$$relat': 1, 'syntact$$relat$$co': 1, 'relat$$co$$occurr': 1, 'co$$occurr$$context': 2, 'occurr$$context$$vector': 1, 'context$$vector$$adopt': 1, 'vector$$adopt$$inform': 1, 'adopt$$inform$$theoret': 1, 'inform$$theoret$$model': 1, 'theoret$$model$$solv': 1, 'model$$solv$$problem': 1, 'solv$$problem$$data': 1, 'problem$$data$$spars': 1, 'data$$spars$$characterist': 1, 'spars$$characterist$$precis': 1, 'characterist$$precis$$probabilist': 1, 'precis$$probabilist$$distribut': 1, 'probabilist$$distribut$$co': 1, 'distribut$$co$$occurr': 1, 'occurr$$context$$featur': 1, 'context$$featur$$deriv': 1, 'featur$$deriv$$pars': 1, 'deriv$$pars$$contextu': 1, 'pars$$contextu$$environ': 1, 'contextu$$environ$$word': 1, 'environ$$word$$context': 1, 'word$$context$$featur': 1, 'context$$featur$$adjust': 1, 'featur$$adjust$$accord': 1, 'adjust$$accord$$idf': 1, 'accord$$idf$$invers': 1, 'idf$$invers$$document': 1, 'invers$$document$$frequenc': 1, 'document$$frequenc$$valu': 1, 'frequenc$$valu$$agglom': 1, 'valu$$agglom$$cluster': 1, 'agglom$$cluster$$algorithm': 1, 'cluster$$algorithm$$appli': 1, 'algorithm$$appli$$group': 1, 'appli$$group$$similar': 1, 'group$$similar$$word': 1, 'similar$$word$$accord': 1, 'word$$accord$$similar': 1, 'accord$$similar$$valu': 1, 'similar$$valu$$turn': 1, 'valu$$turn$$word': 1, 'turn$$word$$similar': 1, 'word$$similar$$syntact': 1, 'similar$$syntact$$categori': 1, 'syntact$$categori$$semant': 1, 'categori$$semant$$class': 1, 'semant$$class$$group': 1, 'class$$group$$togeth': 1}}\n",
      "{'docno': 'L02-1310', 'toks': {'bootstrap': 1, 'larg': 1, 'sens': 1, 'tag': 1, 'corpora': 1, 'bootstrap$$larg': 1, 'larg$$sens': 1, 'sens$$tag': 1, 'tag$$corpora': 1, 'bootstrap$$larg$$sens': 1, 'larg$$sens$$tag': 1, 'sens$$tag$$corpora': 1}}\n",
      "{'docno': 'R13-1042', 'toks': {'headerless': 1, 'quoteless': 1, 'hopeless': 1, 'use': 2, 'pairwis': 2, 'email': 6, 'classif': 2, 'disentangl': 3, 'thread': 6, 'task': 1, 'separ': 1, 'convers': 1, 'whose': 1, 'structur': 2, 'implicit': 1, 'distort': 1, 'lost': 1, 'paper': 1, 'perform': 2, 'text': 4, 'similar': 5, 'measur': 1, 'non': 1, 'quot': 1, 'show': 1, 'content': 2, 'metric': 2, 'outperform': 1, 'style': 1, 'class': 2, 'balanc': 1, 'imbalanc': 1, 'set': 1, 'ii': 1, 'although': 1, 'featur': 2, 'depend': 1, 'semant': 2, 'corpus': 4, 'still': 1, 'effect': 1, 'even': 1, 'control': 1, 'make': 1, 'avail': 1, 'enron': 2, 'newli': 1, 'extract': 1, '70': 1, '178': 1, 'multiemail': 1, 'headerless$$quoteless': 1, 'quoteless$$hopeless': 1, 'hopeless$$use': 1, 'use$$pairwis': 1, 'pairwis$$email': 1, 'email$$classif': 1, 'classif$$disentangl': 1, 'disentangl$$email': 1, 'email$$thread': 2, 'thread$$thread': 1, 'thread$$disentangl': 2, 'disentangl$$task': 1, 'task$$separ': 1, 'separ$$convers': 1, 'convers$$whose': 1, 'whose$$thread': 1, 'thread$$structur': 1, 'structur$$implicit': 1, 'implicit$$distort': 1, 'distort$$lost': 1, 'lost$$paper': 1, 'paper$$perform': 1, 'perform$$email': 1, 'disentangl$$pairwis': 1, 'pairwis$$classif': 1, 'classif$$use': 1, 'use$$text': 1, 'text$$similar': 3, 'similar$$measur': 1, 'measur$$non': 1, 'non$$quot': 1, 'quot$$text': 1, 'text$$email': 1, 'email$$show': 1, 'show$$content': 1, 'content$$text': 1, 'similar$$metric': 2, 'metric$$outperform': 1, 'outperform$$style': 1, 'style$$structur': 1, 'structur$$text': 1, 'metric$$class': 1, 'class$$balanc': 1, 'balanc$$class': 1, 'class$$imbalanc': 1, 'imbalanc$$set': 1, 'set$$ii': 1, 'ii$$although': 1, 'although$$featur': 1, 'featur$$perform': 1, 'perform$$depend': 1, 'depend$$semant': 1, 'semant$$similar': 2, 'similar$$corpus': 1, 'corpus$$content': 1, 'content$$featur': 1, 'featur$$still': 1, 'still$$effect': 1, 'effect$$even': 1, 'even$$control': 1, 'control$$semant': 1, 'similar$$make': 1, 'make$$avail': 1, 'avail$$enron': 1, 'enron$$thread': 1, 'thread$$corpus': 1, 'corpus$$newli': 1, 'newli$$extract': 1, 'extract$$corpus': 1, 'corpus$$70': 1, '70$$178': 1, '178$$multiemail': 1, 'multiemail$$thread': 1, 'thread$$email': 1, 'email$$enron': 1, 'enron$$email': 1, 'email$$corpus': 1, 'headerless$$quoteless$$hopeless': 1, 'quoteless$$hopeless$$use': 1, 'hopeless$$use$$pairwis': 1, 'use$$pairwis$$email': 1, 'pairwis$$email$$classif': 1, 'email$$classif$$disentangl': 1, 'classif$$disentangl$$email': 1, 'disentangl$$email$$thread': 1, 'email$$thread$$thread': 1, 'thread$$thread$$disentangl': 1, 'thread$$disentangl$$task': 1, 'disentangl$$task$$separ': 1, 'task$$separ$$convers': 1, 'separ$$convers$$whose': 1, 'convers$$whose$$thread': 1, 'whose$$thread$$structur': 1, 'thread$$structur$$implicit': 1, 'structur$$implicit$$distort': 1, 'implicit$$distort$$lost': 1, 'distort$$lost$$paper': 1, 'lost$$paper$$perform': 1, 'paper$$perform$$email': 1, 'perform$$email$$thread': 1, 'email$$thread$$disentangl': 1, 'thread$$disentangl$$pairwis': 1, 'disentangl$$pairwis$$classif': 1, 'pairwis$$classif$$use': 1, 'classif$$use$$text': 1, 'use$$text$$similar': 1, 'text$$similar$$measur': 1, 'similar$$measur$$non': 1, 'measur$$non$$quot': 1, 'non$$quot$$text': 1, 'quot$$text$$email': 1, 'text$$email$$show': 1, 'email$$show$$content': 1, 'show$$content$$text': 1, 'content$$text$$similar': 1, 'text$$similar$$metric': 2, 'similar$$metric$$outperform': 1, 'metric$$outperform$$style': 1, 'outperform$$style$$structur': 1, 'style$$structur$$text': 1, 'structur$$text$$similar': 1, 'similar$$metric$$class': 1, 'metric$$class$$balanc': 1, 'class$$balanc$$class': 1, 'balanc$$class$$imbalanc': 1, 'class$$imbalanc$$set': 1, 'imbalanc$$set$$ii': 1, 'set$$ii$$although': 1, 'ii$$although$$featur': 1, 'although$$featur$$perform': 1, 'featur$$perform$$depend': 1, 'perform$$depend$$semant': 1, 'depend$$semant$$similar': 1, 'semant$$similar$$corpus': 1, 'similar$$corpus$$content': 1, 'corpus$$content$$featur': 1, 'content$$featur$$still': 1, 'featur$$still$$effect': 1, 'still$$effect$$even': 1, 'effect$$even$$control': 1, 'even$$control$$semant': 1, 'control$$semant$$similar': 1, 'semant$$similar$$make': 1, 'similar$$make$$avail': 1, 'make$$avail$$enron': 1, 'avail$$enron$$thread': 1, 'enron$$thread$$corpus': 1, 'thread$$corpus$$newli': 1, 'corpus$$newli$$extract': 1, 'newli$$extract$$corpus': 1, 'extract$$corpus$$70': 1, 'corpus$$70$$178': 1, '70$$178$$multiemail': 1, '178$$multiemail$$thread': 1, 'multiemail$$thread$$email': 1, 'thread$$email$$enron': 1, 'email$$enron$$email': 1, 'enron$$email$$corpus': 1}}\n"
     ]
    }
   ],
   "source": [
    "# Apply n-gram tokenization to the dataset\n",
    "#This will delete the 'text' field from the documents,\n",
    "# and create a new 'toks' field which contains the tokens with their frequencies\n",
    "for doc in docs:\n",
    "        if 'text' in doc:\n",
    "            doc['text'] = clean_text(doc['text'])\n",
    "            doc['text'] = remove_stopwords(doc['text'])\n",
    "            doc['text'] = stem_text(doc['text'])\n",
    "            \n",
    "            doc_1gram = tokenize_ngrams_to_dict(doc['text'], n1=1, n2=3)\n",
    "\n",
    "            doc['toks'] = doc_1gram\n",
    "            del doc['text']  \n",
    "    \n",
    "for i, doc in enumerate(docs):\n",
    "     if i == 3:\n",
    "           break\n",
    "     print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our Index. \n",
    "We will use an IterDictIndexer with pretokenised=True, as we already created the tokens manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 500\n",
      "Number of terms: 60550\n",
      "Number of postings: 88887\n",
      "Number of fields: 0\n",
      "Number of tokens: 102993\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n",
      "0\n",
      "47\n",
      "0$$2\n",
      "2\n",
      "0$$2$$0\n",
      "1\n",
      "0$$2$$rather\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the IterDictIndexer with pretokenised set to True\n",
    "iter_indexer = pt.IterDictIndexer(\"./ngramindex\", overwrite=True, meta={'docno': 20}, pretokenised=True)\n",
    "\n",
    "# Index our pretokenized documents\n",
    "index_ref = iter_indexer.index(docs)\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "#Print some stats about our index\n",
    "print(index.getCollectionStatistics())\n",
    "\n",
    "# Access the MetaIndex and Lexicon\n",
    "meta = index.getMetaIndex()\n",
    "lexicon = index.getLexicon()\n",
    "\n",
    "#Print some example terms from the index. We see that numbers arent removed\n",
    "i = 0\n",
    "for term, le in index.getLexicon():\n",
    "    i = i+1\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(term) \n",
    "    print(le.getFrequency())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the retrieval pipeline\n",
    "First, we define a new method that works exactly like the index ngram tokeniser method, just that it will return a list of tokens instead of a dictionary, and it will not count each token.\n",
    "\n",
    "This method will be used as one of the transformers for our retrieval pipeline and it is responsible for tokenising the query in the same format as we tokenized our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a string and returns a list with all ngrams\n",
    "def tokenize_ngrams_to_list(text, n1=1, n2=3):\n",
    "    words = text.split()\n",
    "\n",
    "    # Initialize an empty list to hold all n-grams\n",
    "    all_ngrams = []\n",
    "    \n",
    "    # Loop through each n between n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Add all current ngrams to the all_ngrams list\n",
    "        all_ngrams.extend(ngrams)\n",
    "    \n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can finally define the actual retrieval pipeline. It consists of two steps; first, we tokenize the queries into ngrams, and after that, we apply bm25 to the query with our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_special_characters = pt.rewrite.tokenise(lambda query: clean_text(query, return_as_list=True))\n",
    "remove_stopwords_from_query = pt.rewrite.tokenise(lambda query: remove_stopwords(query, return_as_list=True))\n",
    "stem_query = pt.rewrite.tokenise(lambda query: stem_text(query, return_as_list=True))\n",
    "# This transformer will tokenise the queries into the ngrams\n",
    "tokenise_query_ngram = pt.rewrite.tokenise(lambda query: tokenize_ngrams_to_list(query))\n",
    "\n",
    "# This transformer will do the retrieval using bm25, and explicitly not apply any stemming and stopword removal\n",
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", verbose = True, properties={\"termpipelines\" : \"\"})\n",
    "\n",
    "# This is our retrieval pipeline\n",
    "retr_pipeline = remove_special_characters >> remove_stopwords_from_query >> stem_query >> tokenise_query_ngram >> bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n",
      "Index(['qid', 'text', 'title', 'query', 'description', 'narrative'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>machine learning language identification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>social media detect self harm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                                     query\n",
       "0   1  retrieval system improving effectiveness\n",
       "1   2  machine learning language identification\n",
       "2   3             social media detect self harm"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at some example queries\n",
    "print(pt_dataset.get_topics().columns)\n",
    "pt_dataset.get_topics('query').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ndf = pd.DataFrame(pt_dataset.get_topics())\\nif 'query' not in df.columns:\\n    df['query'] = df['text']\\n\\n\\n# Convert the DataFrame to a list of dictionaries\\nqueries = df[['qid', 'query']].to_dict(orient='records')\\n\\n# Print the result\\nprint(queries)\\n\\n#TODO sonderzeichen aus query löschen\\ndef clean_queries(queries):\\n    for query in queries:\\n        if 'query' in query:\\n            query['query'] = clean_text(query['query'])\\nclean_queries(queries)\\nfor query in queries:\\n        if 'query' in query:\\n            query['query'] = remove_stopwords(query['query'])\\n            query['query'] = stem_text(query['query'])\\n\\nfor i,query in enumerate(queries):\\n   \\n\\n    query_ngram = tokenize_ngrams_to_dict(query['query'], n1=1, n2=3)\\n\\n    query['query'] = query_ngram\\n  \\nfor query in queries:\\n     print(query)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not needed anymore since we process the queries directly in the pipeline\n",
    "'''\n",
    "\n",
    "df = pd.DataFrame(pt_dataset.get_topics())\n",
    "if 'query' not in df.columns:\n",
    "    df['query'] = df['text']\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "queries = df[['qid', 'query']].to_dict(orient='records')\n",
    "\n",
    "# Print the result\n",
    "print(queries)\n",
    "\n",
    "#TODO sonderzeichen aus query löschen\n",
    "def clean_queries(queries):\n",
    "    for query in queries:\n",
    "        if 'query' in query:\n",
    "            query['query'] = clean_text(query['query'])\n",
    "clean_queries(queries)\n",
    "for query in queries:\n",
    "        if 'query' in query:\n",
    "            query['query'] = remove_stopwords(query['query'])\n",
    "            query['query'] = stem_text(query['query'])\n",
    "\n",
    "for i,query in enumerate(queries):\n",
    "   \n",
    "\n",
    "    query_ngram = tokenize_ngrams_to_dict(query['query'], n1=1, n2=3)\n",
    "\n",
    "    query['query'] = query_ngram\n",
    "  \n",
    "for query in queries:\n",
    "     print(query)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n",
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25): 100%|██████████| 68/68 [00:00<00:00, 123.99q/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Here are the first 10 entries of the run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>query_3</th>\n",
       "      <th>description</th>\n",
       "      <th>narrative</th>\n",
       "      <th>query_2</th>\n",
       "      <th>query_1</th>\n",
       "      <th>query_0</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>414</td>\n",
       "      <td>S07-1088</td>\n",
       "      <td>0</td>\n",
       "      <td>17.367886</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>308</td>\n",
       "      <td>D19-3006</td>\n",
       "      <td>1</td>\n",
       "      <td>12.982733</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>264</td>\n",
       "      <td>2012.iwslt-evaluati</td>\n",
       "      <td>2</td>\n",
       "      <td>12.363764</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>341</td>\n",
       "      <td>P05-1007</td>\n",
       "      <td>3</td>\n",
       "      <td>12.021729</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>C10-2174</td>\n",
       "      <td>4</td>\n",
       "      <td>11.991876</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>2021.emnlp-main.148</td>\n",
       "      <td>5</td>\n",
       "      <td>9.380649</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "      <td>L16-1093</td>\n",
       "      <td>6</td>\n",
       "      <td>9.101785</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>R13-1056</td>\n",
       "      <td>7</td>\n",
       "      <td>9.022026</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>366</td>\n",
       "      <td>2009.mtsummit-poste</td>\n",
       "      <td>8</td>\n",
       "      <td>8.699967</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>475</td>\n",
       "      <td>W18-5026</td>\n",
       "      <td>9</td>\n",
       "      <td>7.870108</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "      <td>retriev system improv effect retriev$$system s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid                docno  rank      score  \\\n",
       "0   1    414             S07-1088     0  17.367886   \n",
       "1   1    308             D19-3006     1  12.982733   \n",
       "2   1    264  2012.iwslt-evaluati     2  12.363764   \n",
       "3   1    341             P05-1007     3  12.021729   \n",
       "4   1    121             C10-2174     4  11.991876   \n",
       "5   1    306  2021.emnlp-main.148     5   9.380649   \n",
       "6   1    360             L16-1093     6   9.101785   \n",
       "7   1     44             R13-1056     7   9.022026   \n",
       "8   1    366  2009.mtsummit-poste     8   8.699967   \n",
       "9   1    475             W18-5026     9   7.870108   \n",
       "\n",
       "                                       text  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                      title  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                    query_3  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                         description  \\\n",
       "0  What papers focus on improving the effectivene...   \n",
       "1  What papers focus on improving the effectivene...   \n",
       "2  What papers focus on improving the effectivene...   \n",
       "3  What papers focus on improving the effectivene...   \n",
       "4  What papers focus on improving the effectivene...   \n",
       "5  What papers focus on improving the effectivene...   \n",
       "6  What papers focus on improving the effectivene...   \n",
       "7  What papers focus on improving the effectivene...   \n",
       "8  What papers focus on improving the effectivene...   \n",
       "9  What papers focus on improving the effectivene...   \n",
       "\n",
       "                                           narrative  \\\n",
       "0  Relevant papers include research on what makes...   \n",
       "1  Relevant papers include research on what makes...   \n",
       "2  Relevant papers include research on what makes...   \n",
       "3  Relevant papers include research on what makes...   \n",
       "4  Relevant papers include research on what makes...   \n",
       "5  Relevant papers include research on what makes...   \n",
       "6  Relevant papers include research on what makes...   \n",
       "7  Relevant papers include research on what makes...   \n",
       "8  Relevant papers include research on what makes...   \n",
       "9  Relevant papers include research on what makes...   \n",
       "\n",
       "                                    query_2  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                    query_1                       query_0  \\\n",
       "0  retrieval system improving effectiveness  retriev system improv effect   \n",
       "1  retrieval system improving effectiveness  retriev system improv effect   \n",
       "2  retrieval system improving effectiveness  retriev system improv effect   \n",
       "3  retrieval system improving effectiveness  retriev system improv effect   \n",
       "4  retrieval system improving effectiveness  retriev system improv effect   \n",
       "5  retrieval system improving effectiveness  retriev system improv effect   \n",
       "6  retrieval system improving effectiveness  retriev system improv effect   \n",
       "7  retrieval system improving effectiveness  retriev system improv effect   \n",
       "8  retrieval system improving effectiveness  retriev system improv effect   \n",
       "9  retrieval system improving effectiveness  retriev system improv effect   \n",
       "\n",
       "                                               query  \n",
       "0  retriev system improv effect retriev$$system s...  \n",
       "1  retriev system improv effect retriev$$system s...  \n",
       "2  retriev system improv effect retriev$$system s...  \n",
       "3  retriev system improv effect retriev$$system s...  \n",
       "4  retriev system improv effect retriev$$system s...  \n",
       "5  retriev system improv effect retriev$$system s...  \n",
       "6  retriev system improv effect retriev$$system s...  \n",
       "7  retriev system improv effect retriev$$system s...  \n",
       "8  retriev system improv effect retriev$$system s...  \n",
       "9  retriev system improv effect retriev$$system s...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "run = retr_pipeline(pt_dataset.get_topics()) #queries\n",
    "\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "run.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist the run file for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs/\".\n",
      "Done. run file is stored under \"../runs//run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='ngrams', default_output='../runs/')\n",
    "\n",
    "import os\n",
    "os.rename('../runs/run.txt', '../runs/runngram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queries = [\\n    {\\'qid\\':1 , \\'query\\':\\'machine learning\\'},\\n    {\\'qid\\':2 , \\'query\\':\\'natural language processing techniques\\'}\\n]\\n\\n# Print the new query representation with ngrams included. This is how our query will get passed to bm25\\ndf = pd.DataFrame(queries)\\nprint(df)\\ntransformed_df = tokenise_query_ngram.transform(df)\\nprint(\"Transformed:\")\\nprint(transformed_df)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''queries = [\n",
    "    {'qid':1 , 'query':'machine learning'},\n",
    "    {'qid':2 , 'query':'natural language processing techniques'}\n",
    "]\n",
    "\n",
    "# Print the new query representation with ngrams included. This is how our query will get passed to bm25\n",
    "df = pd.DataFrame(queries)\n",
    "print(df)\n",
    "transformed_df = tokenise_query_ngram.transform(df)\n",
    "print(\"Transformed:\")\n",
    "print(transformed_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for index, row in transformed_df.iterrows():\\n    query_id = row[\\'qid\\']\\n    query_text = row[\\'query\\']\\n    print(\"test\")\\n    print(f\"Processing query ID {query_id} with text: {query_text}\")\\n    \\n    # Execute the search\\n    results = retr_pipeline.search(query_text)\\n    \\n    # Print or process the results\\n    print(f\"Results for query ID {query_id}:\")\\n    print(results)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for index, row in transformed_df.iterrows():\n",
    "    query_id = row['qid']\n",
    "    query_text = row['query']\n",
    "    print(\"test\")\n",
    "    print(f\"Processing query ID {query_id} with text: {query_text}\")\n",
    "    \n",
    "    # Execute the search\n",
    "    results = retr_pipeline.search(query_text)\n",
    "    \n",
    "    # Print or process the results\n",
    "    print(f\"Results for query ID {query_id}:\")\n",
    "    print(results)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
