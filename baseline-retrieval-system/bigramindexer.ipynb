{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from tira.third_party_integrations import persist_and_normalize_run,  ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Start pyterrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize PyTerrier\n",
    "if not pt.started():\n",
    "    pt.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Load real dataset from tira and process it\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    {'docno': 'd1', 'text': 'do goldfish grow?'},\n",
    "    {'docno': 'd2', 'text': 'a quick brown fox'},\n",
    "    {'docno' : 'd3', 'text' : 'a brown quick fox'}\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    {\"docno\": \"d1\", \"text\": \"machine learning is fun\"},\n",
    "    {\"docno\": \"d2\", \"text\": \"machines are helpful\"},\n",
    "    {\"docno\": \"d3\", \"text\": \"machine learning algorithms\"},\n",
    "    {\"docno\": \"d4\", \"text\": \"machine learning algorithms are interesting\"},\n",
    "    {\"docno\": \"d5\", \"text\": \"nothing related here\"},\n",
    "    {\"docno\": \"d6\", \"text\": \"machine machine machine learning learning learning algorithms algorithms\"}  # Increased termÂ frequency\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Convert documents to include ngrams\n",
    "Right now, we only include bigrams\n",
    "We will change to to include variable ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our ngram tokenizer. It takes a string and returns a dict of all ngrams, where each ngram is seperated by $$ so it will be parsed as one token\n",
    "\n",
    "def tokenize_ngrams_to_dict(text, n1=1, n2=3):\n",
    "    # Replace spaces with dollar signs\n",
    "    text_with_dollar_signs = re.sub(r'\\s+', '$', text)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text_with_dollar_signs.split('$')\n",
    "    \n",
    "    # Initialize an empty Counter to hold all n-grams\n",
    "    all_ngram_counts = Counter()\n",
    "    \n",
    "    # Loop through each n from n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Update the Counter with the current n-grams\n",
    "        all_ngram_counts.update(ngrams)\n",
    "    \n",
    "    return dict(all_ngram_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply n-gram tokenization to the dataset\n",
    "for doc in documents:\n",
    "    doc_1gram = tokenize_ngrams_to_dict(doc['text'], n1=1, n2= 3)\n",
    "\n",
    "    doc['toks'] = doc_1gram\n",
    "    del doc['text']  # Remove the 'text' field as it's not needed anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing complete: <org.terrier.querying.IndexRef at 0x7a74d7b0f060 jclass=org/terrier/querying/IndexRef jself=<LocalRef obj=0x634f5e66dfe8 at 0x7a74d7c85cf0>>\n",
      "Number of documents: 6\n",
      "Number of terms: 38\n",
      "Number of postings: 53\n",
      "Number of fields: 0\n",
      "Number of tokens: 60\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n",
      "algorithms\n",
      "4\n",
      "algorithms$$algorithms\n",
      "1\n",
      "algorithms$$are\n",
      "1\n",
      "algorithms$$are$$interesting\n",
      "1\n",
      "are\n",
      "2\n",
      "are$$helpful\n",
      "1\n",
      "are$$interesting\n",
      "1\n",
      "fun\n",
      "1\n",
      "helpful\n",
      "1\n",
      "here\n",
      "1\n",
      "interesting\n",
      "1\n",
      "is\n",
      "1\n",
      "is$$fun\n",
      "1\n",
      "learning\n",
      "6\n",
      "learning$$algorithms\n",
      "3\n",
      "learning$$algorithms$$algorithms\n",
      "1\n",
      "learning$$algorithms$$are\n",
      "1\n",
      "learning$$is\n",
      "1\n",
      "learning$$is$$fun\n",
      "1\n",
      "learning$$learning\n",
      "2\n",
      "learning$$learning$$algorithms\n",
      "1\n",
      "learning$$learning$$learning\n",
      "1\n",
      "machine\n",
      "6\n",
      "machine$$learning\n",
      "4\n",
      "machine$$learning$$algorithms\n",
      "2\n",
      "machine$$learning$$is\n",
      "1\n",
      "machine$$learning$$learning\n",
      "1\n",
      "machine$$machine\n",
      "2\n",
      "machine$$machine$$learning\n",
      "1\n",
      "machine$$machine$$machine\n",
      "1\n",
      "machines\n",
      "1\n",
      "machines$$are\n",
      "1\n",
      "machines$$are$$helpful\n",
      "1\n",
      "nothing\n",
      "1\n",
      "nothing$$related\n",
      "1\n",
      "nothing$$related$$here\n",
      "1\n",
      "related\n",
      "1\n",
      "related$$here\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the IterDictIndexer with pretokenised set to True\n",
    "iter_indexer = pt.IterDictIndexer(\"./pretokindex\", overwrite=True, meta={'docno': 20}, pretokenised=True)\n",
    "\n",
    "# Index the pretokenized dataset\n",
    "index_ref = iter_indexer.index(documents)\n",
    "\n",
    "print(f\"Indexing complete: {index_ref}\")\n",
    "# Now you can use the index_ref as usual\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "print(index.getCollectionStatistics())\n",
    "for term, le in index.getLexicon():\n",
    "    print(term) \n",
    "    print(le.getFrequency())\n",
    "\n",
    "# Access the MetaIndex and Lexicon\n",
    "meta = index.getMetaIndex()\n",
    "lexicon = index.getLexicon()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Define the retrieval pipeline\n",
    "First, we will define a method that takes a string (our query) and returns a list of all ngrams in the same $$-Format as we used during indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a string and returns a list with all ngrams\n",
    "def tokenize_ngrams_to_list(text, n1=1, n2=3):\n",
    "    #Split the text into all individual words\n",
    "    #TODO use another tokenizer here first to get rid of all special characters and to do stemming etc\n",
    "    words = text.split()\n",
    "\n",
    "    # Initialize an empty list to hold all n-grams\n",
    "    all_ngrams = []\n",
    "    \n",
    "    # Loop through each n between n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Add all current ngrams to the all_ngrams list\n",
    "        all_ngrams.extend(ngrams)\n",
    "    \n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define our actual retrieval pipeline. It consists of two steps. First, we tokenise the query into the ngrams with the method above. Then, we do the retrieval using the standard bm25 algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This transformer will tokenise the queries into the ngrams\n",
    "tokenise_query_ngram = pt.rewrite.tokenise(lambda query: tokenize_ngrams_to_list(query))\n",
    "\n",
    "# This transformer will do the retrieval using bm25\n",
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", verbose = True)\n",
    "\n",
    "# This is our retrieval pipeline\n",
    "retr_pipeline = tokenise_query_ngram >> pt.BatchRetrieve(index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Load the queries\n",
    "Here, we load the queries and rewrite them to include all the ngrams, also seperated by $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine learning algorithms machine$$learning learning$$algorithms machine$$learning$$algorithms']\n"
     ]
    }
   ],
   "source": [
    "#TODO put multiple queries into list and run on all queries\n",
    "#TODO load queries from Tira\n",
    "\n",
    "query = 'machine learning algorithms'\n",
    "\n",
    "# Print the new query representation with ngrams included. This is how our query will get passed to bm25\n",
    "df = pd.DataFrame([{\"query\": query}])\n",
    "transformed_df = tokenise_query_ngram.transform(df)\n",
    "print(transformed_df[\"query\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create the run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  qid  docid docno  rank     score                      query_0  \\\n",
      "0   1      2    d3     0  5.327106  machine learning algorithms   \n",
      "1   1      3    d4     1  4.098277  machine learning algorithms   \n",
      "2   1      5    d6     2  2.982158  machine learning algorithms   \n",
      "3   1      0    d1     3  1.881809  machine learning algorithms   \n",
      "\n",
      "                                               query  \n",
      "0  machine learning algorithms machine$$learning ...  \n",
      "1  machine learning algorithms machine$$learning ...  \n",
      "2  machine learning algorithms machine$$learning ...  \n",
      "3  machine learning algorithms machine$$learning ...  \n"
     ]
    }
   ],
   "source": [
    "run = retr_pipeline.search(query)\n",
    "print(run.head(10))\n",
    "#results = retr_pipeline.transform(query)\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legacy methods that might still be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\n# Sample queries\\nqueries = pd.DataFrame([\\n    {\\'qid\\': \\'q1\\', \\'query\\': \\'do goldfish grow?\\'},\\n    {\\'qid\\': \\'q2\\', \\'query\\': \\'quick brown fox\\'}\\n])\\n\\n# Retrieve results\\nrun = retr_pipe(queries)\\nrun.head(10)\\nresults = retr_pipe.transform(queries)\\nprint(results)\\n\\n\\n\\n#index = pt.IndexFactory.of(index_ref)\\nmeta = index.getMetaIndex()\\n\\n# List of metadata keys\\nmeta_keys = [\\'docno\\', \\'text\\']  # Adjust this list based on your meta fields\\n\\n# Function to print document attributes\\ndef print_doc_attributes(docid):\\n    attributes = {key: meta.getItem(key, docid) for key in meta_keys}\\n    print(f\"Attributes for docid {docid}: {attributes}\")\\n\\n# Example: Print attributes for the first 5 documents\\nfor docid in range(3):\\n    print_doc_attributes(docid)\\n\\n# Function to print document attributes by docno\\ndef print_doc_attributes_by_docno(docno):\\n    try:\\n        docid = meta.getDocument(\"docno\", docno)\\n        print_doc_attributes(docid)\\n    except KeyError:\\n        print(f\"Document with docno {docno} not found.\")\\n\\n# List of specific docnos to retrieve\\n#docnos = [\\'W05-0704\\']\\n\\n# Retrieve and print attributes for each specified docno\\n#for docno in docnos:\\n #   print_doc_attributes_by_docno(docno)\\n\\n\\n\\n\\ndef tokenize_ngrams_dollar_sign_query_OLD(text, n=2):\\n    tokens = text.split()\\n    ngrams = [\\'$$\\'.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\\n    return ngrams\\n\\n\\n \\ndef tokenize_ngrams_dollar_signOLD(text, n=2):\\n    # Replace spaces with dollar signs\\n    text_with_dollar_signs = re.sub(r\\'\\\\s+\\', \\'$\\', text)\\n    \\n    # Tokenize the text into words\\n    words = text_with_dollar_signs.split(\\'$\\')\\n    \\n    # Generate n-grams manually\\n    ngrams = [\\'$$\\'.join(words[i:i+n]) for i in range(len(words)-n+1)]\\n    \\n    # Count occurrences of each n-gram\\n    ngram_counts = Counter(ngrams)\\n    \\n    return dict(ngram_counts)\\n\\n\\n\\n\\n# Function to tokenize text into n-grams\\ndef tokenize_ngrams(text, n=2):\\n    vectorizer = CountVectorizer(ngram_range=(n, n), token_pattern=r\\'\\x08\\\\w+\\x08\\')\\n    X = vectorizer.fit_transform([text])\\n    ngrams = vectorizer.get_feature_names_out()\\n    counts = X.toarray().flatten()\\n    return dict(zip(ngrams, counts))\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "# Function to tokenize query into n-grams\n",
    "def query_tokenize_ngrams(query, n=2):\n",
    "    return list(tokenize_ngrams(query, n=n).keys())\n",
    "\n",
    "# Define the retrieval pipeline\n",
    "class NgramTokenizeTransform:\n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "\n",
    "    def transform(self, queries):\n",
    "        def ngrams(text, n):\n",
    "            tokens = text.split()\n",
    "            return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        \n",
    "        queries['query'] = queries['query'].apply(lambda x: ' '.join(ngrams(x, self.n)))\n",
    "        return queries\n",
    "\n",
    "ngram_tokenize = NgramTokenizeTransform(n=2)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#print('Now we do the retrieval...')\n",
    "#run = bm25(test_dataset.get_topics('text'))\n",
    "#run = bm25([{\"qid\": \"0\", \"query\" : \"retrieval systems\"}])\n",
    "#print('Done. Here are the first 10 entries of the run')\n",
    "#run.head(10)\n",
    "\n",
    "\n",
    "# tokenize ngram the queries\n",
    "# Create a query pipeline that includes the n-gram tokenization\n",
    "#ngram_tokenize = NgramTokenizeTransform(n=2)\n",
    "#retr_pipe = ngram_tokenize >> bm25\n",
    "'''\n",
    "import pandas as pd\n",
    "# Sample queries\n",
    "queries = pd.DataFrame([\n",
    "    {'qid': 'q1', 'query': 'do goldfish grow?'},\n",
    "    {'qid': 'q2', 'query': 'quick brown fox'}\n",
    "])\n",
    "\n",
    "# Retrieve results\n",
    "run = retr_pipe(queries)\n",
    "run.head(10)\n",
    "results = retr_pipe.transform(queries)\n",
    "print(results)\n",
    "\n",
    "\n",
    "\n",
    "#index = pt.IndexFactory.of(index_ref)\n",
    "meta = index.getMetaIndex()\n",
    "\n",
    "# List of metadata keys\n",
    "meta_keys = ['docno', 'text']  # Adjust this list based on your meta fields\n",
    "\n",
    "# Function to print document attributes\n",
    "def print_doc_attributes(docid):\n",
    "    attributes = {key: meta.getItem(key, docid) for key in meta_keys}\n",
    "    print(f\"Attributes for docid {docid}: {attributes}\")\n",
    "\n",
    "# Example: Print attributes for the first 5 documents\n",
    "for docid in range(3):\n",
    "    print_doc_attributes(docid)\n",
    "\n",
    "# Function to print document attributes by docno\n",
    "def print_doc_attributes_by_docno(docno):\n",
    "    try:\n",
    "        docid = meta.getDocument(\"docno\", docno)\n",
    "        print_doc_attributes(docid)\n",
    "    except KeyError:\n",
    "        print(f\"Document with docno {docno} not found.\")\n",
    "\n",
    "# List of specific docnos to retrieve\n",
    "#docnos = ['W05-0704']\n",
    "\n",
    "# Retrieve and print attributes for each specified docno\n",
    "#for docno in docnos:\n",
    " #   print_doc_attributes_by_docno(docno)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_ngrams_dollar_sign_query_OLD(text, n=2):\n",
    "    tokens = text.split()\n",
    "    ngrams = ['$$'.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return ngrams\n",
    "\n",
    "\n",
    " \n",
    "def tokenize_ngrams_dollar_signOLD(text, n=2):\n",
    "    # Replace spaces with dollar signs\n",
    "    text_with_dollar_signs = re.sub(r'\\s+', '$', text)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text_with_dollar_signs.split('$')\n",
    "    \n",
    "    # Generate n-grams manually\n",
    "    ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    \n",
    "    # Count occurrences of each n-gram\n",
    "    ngram_counts = Counter(ngrams)\n",
    "    \n",
    "    return dict(ngram_counts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to tokenize text into n-grams\n",
    "def tokenize_ngrams(text, n=2):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), token_pattern=r'\\b\\w+\\b')\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    ngrams = vectorizer.get_feature_names_out()\n",
    "    counts = X.toarray().flatten()\n",
    "    return dict(zip(ngrams, counts))\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
