{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from tira.third_party_integrations import persist_and_normalize_run,  ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Start pyterrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize PyTerrier\n",
    "if not pt.started():\n",
    "    pt.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    {'docno': 'd1', 'text': 'do goldfish grow?'},\n",
    "    {'docno': 'd2', 'text': 'a quick brown fox'},\n",
    "    {'docno' : 'd3', 'text' : 'a brown quick fox'}\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    {\"docno\": \"d1\", \"text\": \"machine learning is fun\"},\n",
    "    {\"docno\": \"d2\", \"text\": \"machines are helpful\"},\n",
    "    {\"docno\": \"d3\", \"text\": \"machine learning algorithms\"},\n",
    "    {\"docno\": \"d4\", \"text\": \"machine learning algorithms are interesting\"},\n",
    "    {\"docno\": \"d5\", \"text\": \"nothing related here\"},\n",
    "    {\"docno\": \"d6\", \"text\": \"machine machine machine learning learning learning algorithms algorithms\"}  # Increased termÂ frequency\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Convert documents to include ngrams\n",
    "Right now, we only include bigrams\n",
    "We will change to to include variable ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize text into n-grams\n",
    "def tokenize_ngrams(text, n=2):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), token_pattern=r'\\b\\w+\\b')\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    ngrams = vectorizer.get_feature_names_out()\n",
    "    counts = X.toarray().flatten()\n",
    "    return dict(zip(ngrams, counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our ngram tokenizer. It takes a string and returns a dict of all ngrams, where each ngram is seperated by $$ so it will be parsed as one token\n",
    "\n",
    "def tokenize_ngrams_dollar_sign(text, n1=1, n2=3):\n",
    "    # Replace spaces with dollar signs\n",
    "    text_with_dollar_signs = re.sub(r'\\s+', '$', text)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text_with_dollar_signs.split('$')\n",
    "    \n",
    "    # Initialize an empty Counter to hold all n-grams\n",
    "    all_ngram_counts = Counter()\n",
    "    \n",
    "    # Loop through each n from n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Update the Counter with the current n-grams\n",
    "        all_ngram_counts.update(ngrams)\n",
    "    \n",
    "    return dict(all_ngram_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply n-gram tokenization to the dataset\n",
    "for doc in documents:\n",
    "    doc_1gram = tokenize_ngrams_dollar_sign(doc['text'], n1=1, n2= 3)\n",
    "\n",
    "    doc['toks'] = doc_1gram\n",
    "    del doc['text']  # Remove the 'text' field as it's not needed anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing complete: <org.terrier.querying.IndexRef at 0x7e3da80fc9f0 jclass=org/terrier/querying/IndexRef jself=<LocalRef obj=0x5dc04d548b20 at 0x7e3da82d9510>>\n",
      "Number of documents: 6\n",
      "Number of terms: 38\n",
      "Number of postings: 53\n",
      "Number of fields: 0\n",
      "Number of tokens: 60\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n",
      "algorithms\n",
      "4\n",
      "algorithms$$algorithms\n",
      "1\n",
      "algorithms$$are\n",
      "1\n",
      "algorithms$$are$$interesting\n",
      "1\n",
      "are\n",
      "2\n",
      "are$$helpful\n",
      "1\n",
      "are$$interesting\n",
      "1\n",
      "fun\n",
      "1\n",
      "helpful\n",
      "1\n",
      "here\n",
      "1\n",
      "interesting\n",
      "1\n",
      "is\n",
      "1\n",
      "is$$fun\n",
      "1\n",
      "learning\n",
      "6\n",
      "learning$$algorithms\n",
      "3\n",
      "learning$$algorithms$$algorithms\n",
      "1\n",
      "learning$$algorithms$$are\n",
      "1\n",
      "learning$$is\n",
      "1\n",
      "learning$$is$$fun\n",
      "1\n",
      "learning$$learning\n",
      "2\n",
      "learning$$learning$$algorithms\n",
      "1\n",
      "learning$$learning$$learning\n",
      "1\n",
      "machine\n",
      "6\n",
      "machine$$learning\n",
      "4\n",
      "machine$$learning$$algorithms\n",
      "2\n",
      "machine$$learning$$is\n",
      "1\n",
      "machine$$learning$$learning\n",
      "1\n",
      "machine$$machine\n",
      "2\n",
      "machine$$machine$$learning\n",
      "1\n",
      "machine$$machine$$machine\n",
      "1\n",
      "machines\n",
      "1\n",
      "machines$$are\n",
      "1\n",
      "machines$$are$$helpful\n",
      "1\n",
      "nothing\n",
      "1\n",
      "nothing$$related\n",
      "1\n",
      "nothing$$related$$here\n",
      "1\n",
      "related\n",
      "1\n",
      "related$$here\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the IterDictIndexer with pretokenised set to True\n",
    "iter_indexer = pt.IterDictIndexer(\"./pretokindex\", overwrite=True, meta={'docno': 20}, pretokenised=True)\n",
    "\n",
    "# Index the pretokenized dataset\n",
    "index_ref = iter_indexer.index(documents)\n",
    "\n",
    "print(f\"Indexing complete: {index_ref}\")\n",
    "# Now you can use the index_ref as usual\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "print(index.getCollectionStatistics())\n",
    "for term, le in index.getLexicon():\n",
    "    print(term) \n",
    "    print(le.getFrequency())\n",
    "\n",
    "# Access the MetaIndex and Lexicon\n",
    "meta = index.getMetaIndex()\n",
    "lexicon = index.getLexicon()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Define the retrieval pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a string and returns a list with all ngrams\n",
    "def tokenize_ngrams_dollar_sign_query(text, n1=1, n2=3):\n",
    "    # Replace spaces with dollar signs\n",
    "    text_with_dollar_signs = re.sub(r'\\s+', '$', text)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text_with_dollar_signs.split('$')\n",
    "    \n",
    "    # Initialize an empty list to hold all n-grams\n",
    "    all_ngrams = []\n",
    "    \n",
    "    # Loop through each n from n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Extend the list with the current n-grams\n",
    "        all_ngrams.extend(ngrams)\n",
    "    \n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", verbose = True)\n",
    "\n",
    "tokenise_query_ngram = pt.rewrite.tokenise(lambda query: tokenize_ngrams_dollar_sign_query(query))\n",
    "\n",
    "retr_pipeline = tokenise_query_ngram >> pt.BatchRetrieve(index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Load the queries\n",
    "Here, we load the queries and rewrite them to include all the ngrams, also seperated by $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine learning algorithms machine$$learning learning$$algorithms machine$$learning$$algorithms']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = 'machine learning algorithms'\n",
    "\n",
    "df = pd.DataFrame([{\"query\": query}])\n",
    "transformed_df = tokenise_query_ngram.transform(df)\n",
    "print(transformed_df[\"query\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create the run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  qid  docid docno  rank     score                      query_0  \\\n",
      "0   1      2    d3     0  5.327106  machine learning algorithms   \n",
      "1   1      3    d4     1  4.098277  machine learning algorithms   \n",
      "2   1      5    d6     2  2.982158  machine learning algorithms   \n",
      "3   1      0    d1     3  1.881809  machine learning algorithms   \n",
      "\n",
      "                                               query  \n",
      "0  machine learning algorithms machine$$learning ...  \n",
      "1  machine learning algorithms machine$$learning ...  \n",
      "2  machine learning algorithms machine$$learning ...  \n",
      "3  machine learning algorithms machine$$learning ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#TODO put multiple queries into list and run on all queries\n",
    "#TODO load queries from Tira\n",
    "#TODO add ngrams to the queries like shown, but only for real ngrams. perhaps use LLMs to find out which ngrams are real phrases from the english language and only put those\n",
    "# or perhaps just weight all the ngrams but dont make them necessary\n",
    "\n",
    "run = retr_pipeline.search(query)\n",
    "print(run.head(10))\n",
    "#results = retr_pipeline.transform(query)\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legacy methods that might still be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ValueError() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNow we do the retrieval...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#run = bm25(test_dataset.get_topics('text'))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#run = bm25([{\"qid\": \"0\", \"query\" : \"retrieval systems\"}])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print('Done. Here are the first 10 entries of the run')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a query pipeline that includes the n-gram tokenization\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#ngram_tokenize = NgramTokenizeTransform(n=2)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m retr_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mngram_tokenize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbm25\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Sample queries\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/transformer.py:232\u001b[0m, in \u001b[0;36mTransformer.__rrshift__\u001b[0;34m(self, left)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rrshift__\u001b[39m(\u001b[38;5;28mself\u001b[39m, left) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComposedPipeline\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mComposedPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matchpy/expressions/expressions.py:284\u001b[0m, in \u001b[0;36m_OperationMeta.__call__\u001b[0;34m(cls, variable_name, *operands)\u001b[0m\n\u001b[1;32m    282\u001b[0m operation \u001b[38;5;241m=\u001b[39m Expression\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39munpacked_args_to_init:\n\u001b[0;32m--> 284\u001b[0m     \u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     operation\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39moperands, variable_name\u001b[38;5;241m=\u001b[39mvariable_name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:29\u001b[0m, in \u001b[0;36mNAryTransformerBase.__init__\u001b[0;34m(self, operands, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(operands\u001b[38;5;241m=\u001b[39moperands, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     28\u001b[0m models \u001b[38;5;241m=\u001b[39m operands\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:29\u001b[0m, in \u001b[0;36mNAryTransformerBase.__init__.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(operands\u001b[38;5;241m=\u001b[39moperands, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     28\u001b[0m models \u001b[38;5;241m=\u001b[39m operands\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m( \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[43mget_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m, models) )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/transformer.py:42\u001b[0m, in \u001b[0;36mget_transformer\u001b[0;34m(v, stacklevel)\u001b[0m\n\u001b[1;32m     40\u001b[0m     warn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoercion of a dataframe into a transformer is deprecated; use a pt.Transformer.from_df() instead\u001b[39m\u001b[38;5;124m'\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SourceTransformer(v)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;43;01mValueError\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPassed parameter \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m of type \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m cannot be coerced into a transformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;167;43;01mDeprecationWarning\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: ValueError() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "# Function to tokenize query into n-grams\n",
    "def query_tokenize_ngrams(query, n=2):\n",
    "    return list(tokenize_ngrams(query, n=n).keys())\n",
    "\n",
    "# Define the retrieval pipeline\n",
    "class NgramTokenizeTransform:\n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "\n",
    "    def transform(self, queries):\n",
    "        def ngrams(text, n):\n",
    "            tokens = text.split()\n",
    "            return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        \n",
    "        queries['query'] = queries['query'].apply(lambda x: ' '.join(ngrams(x, self.n)))\n",
    "        return queries\n",
    "\n",
    "ngram_tokenize = NgramTokenizeTransform(n=2)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#print('Now we do the retrieval...')\n",
    "#run = bm25(test_dataset.get_topics('text'))\n",
    "#run = bm25([{\"qid\": \"0\", \"query\" : \"retrieval systems\"}])\n",
    "#print('Done. Here are the first 10 entries of the run')\n",
    "#run.head(10)\n",
    "\n",
    "\n",
    "# tokenize ngram the queries\n",
    "# Create a query pipeline that includes the n-gram tokenization\n",
    "#ngram_tokenize = NgramTokenizeTransform(n=2)\n",
    "#retr_pipe = ngram_tokenize >> bm25\n",
    "'''\n",
    "import pandas as pd\n",
    "# Sample queries\n",
    "queries = pd.DataFrame([\n",
    "    {'qid': 'q1', 'query': 'do goldfish grow?'},\n",
    "    {'qid': 'q2', 'query': 'quick brown fox'}\n",
    "])\n",
    "\n",
    "# Retrieve results\n",
    "run = retr_pipe(queries)\n",
    "run.head(10)\n",
    "results = retr_pipe.transform(queries)\n",
    "print(results)\n",
    "\n",
    "\n",
    "\n",
    "#index = pt.IndexFactory.of(index_ref)\n",
    "meta = index.getMetaIndex()\n",
    "\n",
    "# List of metadata keys\n",
    "meta_keys = ['docno', 'text']  # Adjust this list based on your meta fields\n",
    "\n",
    "# Function to print document attributes\n",
    "def print_doc_attributes(docid):\n",
    "    attributes = {key: meta.getItem(key, docid) for key in meta_keys}\n",
    "    print(f\"Attributes for docid {docid}: {attributes}\")\n",
    "\n",
    "# Example: Print attributes for the first 5 documents\n",
    "for docid in range(3):\n",
    "    print_doc_attributes(docid)\n",
    "\n",
    "# Function to print document attributes by docno\n",
    "def print_doc_attributes_by_docno(docno):\n",
    "    try:\n",
    "        docid = meta.getDocument(\"docno\", docno)\n",
    "        print_doc_attributes(docid)\n",
    "    except KeyError:\n",
    "        print(f\"Document with docno {docno} not found.\")\n",
    "\n",
    "# List of specific docnos to retrieve\n",
    "#docnos = ['W05-0704']\n",
    "\n",
    "# Retrieve and print attributes for each specified docno\n",
    "#for docno in docnos:\n",
    " #   print_doc_attributes_by_docno(docno)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_ngrams_dollar_sign_query_OLD(text, n=2):\n",
    "    tokens = text.split()\n",
    "    ngrams = ['$$'.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return ngrams\n",
    "\n",
    "\n",
    " \n",
    "def tokenize_ngrams_dollar_signOLD(text, n=2):\n",
    "    # Replace spaces with dollar signs\n",
    "    text_with_dollar_signs = re.sub(r'\\s+', '$', text)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text_with_dollar_signs.split('$')\n",
    "    \n",
    "    # Generate n-grams manually\n",
    "    ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    \n",
    "    # Count occurrences of each n-gram\n",
    "    ngram_counts = Counter(ngrams)\n",
    "    \n",
    "    return dict(ngram_counts)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
