{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tira in /usr/local/lib/python3.10/dist-packages (0.0.132)\n",
      "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
      "Requirement already satisfied: python-terrier in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tira) (2.1.3)\n",
      "Requirement already satisfied: docker==6.*,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tira) (6.1.3)\n",
      "Requirement already satisfied: requests==2.*,>=2.26 in /usr/local/lib/python3.10/dist-packages (from tira) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tira) (4.66.1)\n",
      "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (23.2)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (1.7.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.3.2)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.9.3)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.3.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.6)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.6)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (3.2.3)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.2)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.3.2)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (1.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (6.0.1)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.12)\n",
      "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.7)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.0)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: matchpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: typish>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.5)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /usr/local/lib/python3.10/dist-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.10/dist-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (2.1.3)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3.post1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->python-terrier) (3.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (0.5.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->python-terrier) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# You only need to execute this cell if you are using Google Golab.\n",
    "# If you use GitHub Codespaces, everything is already installed.\n",
    "!pip3 install tira ir-datasets python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "from pyterrier.index import IterDictIndexer\n",
    "import pandas as pd #extra import\n",
    "from sklearn.feature_extraction.text import CountVectorizer #extra import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset and the Index\n",
    "\n",
    "The type of the index object that we load is `<class 'jnius.reflect.org.terrier.structures.Index'>`, in fact a [Java class](http://terrier.org/docs/v3.6/javadoc/org/terrier/structures/Index.html) wrapped into Python. However, you do not need to worry about this: at this point, we will simply use the provided Index object to run procedures defined in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRDSDataset('ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n"
     ]
    }
   ],
   "source": [
    "# Datenset laden\n",
    "# Lade den Datensatz in test_dataset\n",
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "\n",
    "test_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom tokenizer method.\n",
    "This method takes a String as an argument and returns a List of all tokens, including ngrams between 1 and 3 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['testing', 'of', 'information', 'retrieval', 'systems', 'testing of', 'of information', 'information retrieval', 'retrieval systems', 'testing of information', 'of information retrieval', 'information retrieval systems']\n"
     ]
    }
   ],
   "source": [
    "# Methode, die einen String einliest und eine Liste aller ngrams in der Range ausgibt. 1grams: alle Wörter einzeln. 2grams: alle paare aus benachbarten etc.\n",
    "def ngram_tokenizer(text, ngram_range=(1, 3)):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, token_pattern=r'\\b\\w+\\b', analyzer='word')\n",
    "    analyze = vectorizer.build_analyzer()\n",
    "    return analyze(text)\n",
    "\n",
    "#Beispiel, wie ein text getokenized wird\n",
    "print(ngram_tokenizer(\"Testing of Information Retrieval Systems\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Iterator for our dataset. This is an Iterator where every element is a dict which contains a text and a docno.\n",
    "See the example print below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:   8%|▊         | 9737/126958 [00:00<00:02, 49278.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Bootstrapping Large Sense Tagged Corpora', 'docno': 'L02-1310'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 54531.28it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Erstelle einen Iterator von Dicts von unserem Dataset\n",
    "# Return type : Iterator[Dict[str, Any]]\n",
    "corpus_iter = test_dataset.get_corpus_iter()\n",
    "\n",
    "#Print example element of corpus_iter.\n",
    "#Corpus_iter is an iterator where all elements are dicts that look like this\n",
    "for doc in corpus_iter:\n",
    "     if doc.get(\"docno\") == \"L02-1310\":\n",
    "          print(doc)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erstelle custom Indexer, der IterDictIndexer extended\n",
    "class CustomTokenizerIterDictIndexer(IterDictIndexer):\n",
    "    # Constructor, der den Konstruktor der Superklasse aufruft und die Argumente weitergibt\n",
    "    def __init__(self, index_path, tokenizer):\n",
    "        super().__init__(index_path) # übergebe Parameter an Konstruktor der Superklasse\n",
    "        self.tokenizer = tokenizer  # Da man den tokenizer nicht als Parameter übergeben kann, muss man ihn so von hand ändern\n",
    "\n",
    "    def _document_to_dict(self, doc):\n",
    "        \"\"\"\n",
    "        Convert a document into a dictionary representation.\n",
    "        Override this method to include custom tokenization.\n",
    "        \"\"\"\n",
    "        if isinstance(doc, dict):\n",
    "            doc_dict = doc\n",
    "        else:\n",
    "            doc_dict = {\n",
    "                \"docno\": str(doc[0]),\n",
    "                \"text\": doc[1]\n",
    "            }\n",
    "        \n",
    "        # Tokenize the text using the custom tokenizer\n",
    "        tokens = self.tokenizer(doc_dict[\"text\"])\n",
    "        \n",
    "        # Create a dictionary with tokenized terms\n",
    "        doc_dict[\"text\"] = \" \".join(tokens)\n",
    "        return doc_dict\n",
    "\n",
    "    def index(self, iter, fields=('text',), **kwargs):\n",
    "        \"\"\"\n",
    "        Override the index method if necessary.\n",
    "        \"\"\"\n",
    "        return super().index(iter, fields, **kwargs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:   0%|          | 0/126958 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the custom indexer\n",
    "index_path = \"./ngram_test_index1\"\n",
    "ngram_indexer = CustomTokenizerIterDictIndexer(index_path, ngram_tokenizer) \n",
    "\n",
    "\n",
    "#indexer = pt.IterDictIndexer('./index_ngram3', meta=['docno', 'text'], meta_lengths=[20, 4096])\n",
    "#index_ref = indexer.index(transformed_corpus_iter, fields=['text'])\n",
    "\n",
    "docs = test_dataset.get_corpus_iter()\n",
    "\n",
    "docs =  [\n",
    "    {\"docno\": \"1\", \"text\": \"Information Retrieval Systems\"},\n",
    "    {\"docno\": \"2\", \"text\": \"Systems Retrieval Information\"}\n",
    "]\n",
    "index_ref = ngram_indexer.index(docs)   #,  fields=['text'])\n",
    "\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   qid                                     query\n",
      "0    1  retrieval system improving effectiveness\n",
      "1    2  machine learning language identification\n",
      "2    3             social media detect self harm\n",
      "3    4             stemming for arabic languages\n",
      "4    5            audio based animal recognition\n",
      "..  ..                                       ...\n",
      "63  65         information in different language\n",
      "64  66                  abbreviations in queries\n",
      "65  67                  lemmatization algorithms\n",
      "66  68                  filter ad rich documents\n",
      "67  18     advancements in information retrieval\n",
      "\n",
      "[68 rows x 2 columns]\n",
      "Number of documents: 5\n",
      "Number of terms: 4\n",
      "Number of postings: 10\n",
      "Number of fields: 1\n",
      "Number of tokens: 16\n",
      "Field names: [text]\n",
      "Positions:   false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.get_topics('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:29:23.940 [ForkJoinPool-41-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (1) - further warnings are suppressed\n",
      "15:29:24.179 [ForkJoinPool-41-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 1 empty documents\n",
      "Number of documents: 2\n",
      "Number of terms: 8\n",
      "Number of postings: 8\n",
      "Number of fields: 1\n",
      "Number of tokens: 10\n",
      "Field names: [text]\n",
      "Positions:   false\n",
      "\n",
      "doesnt\n",
      "1\n",
      "million\n",
      "1\n",
      "pipelin\n",
      "1\n",
      "retriev\n",
      "2\n",
      "system\n",
      "2\n",
      "time\n",
      "1\n",
      "try\n",
      "1\n",
      "work\n",
      "1\n",
      "<org.terrier.structures.PostingIndex at 0x7b6ea6033a10 jclass=org/terrier/structures/PostingIndex jself=<LocalRef obj=0x5e3bfdd9e1d8 at 0x7b6ea786ef90>>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'org.terrier.structures.PostingIndex' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m inverted \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mgetInvertedIndex()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(inverted)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m term, le \u001b[38;5;129;01min\u001b[39;00m inverted:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mprint\u001b[39m(term) \n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(le\u001b[38;5;241m.\u001b[39mgetFrequency())\n",
      "\u001b[0;31mTypeError\u001b[0m: 'org.terrier.structures.PostingIndex' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "#index = tira.pt.index('ir-lab-sose-2024-needthegrade/baseline-retrieval-system/index_ngram2', test_dataset)\n",
    "\n",
    "import pyterrier as pt\n",
    "from pyterrier.index import IterDictIndexer\n",
    "\n",
    "\n",
    "def custom_tokenizer(self, text):\n",
    "        \"\"\"\n",
    "        Custom tokenizer method that lowercases and splits text by spaces.\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(ngram_range=(2,2), token_pattern=r'\\b\\w+\\b', analyzer='word')\n",
    "        analyze = vectorizer.build_analyzer()\n",
    "        return analyze(text)\n",
    "def ngram_tokenizer(text, ngram_range=(2,2)):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, token_pattern=r'\\b\\w+\\b', analyzer='word')\n",
    "    analyze = vectorizer.build_analyzer()\n",
    "    return analyze(text)\n",
    "\n",
    "class CustomTokenizerIterDictIndexer(IterDictIndexer):\n",
    "    def __init__(self, index_path, overwrite, tokenizer):\n",
    "        super().__init__(index_path, overwrite)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "\n",
    "    def get_text_dict(self, doc):\n",
    "        text = doc['text']\n",
    "        tokens = custom_tokenizer(text)\n",
    "        return {'text': ' '.join(tokens)}\n",
    "    \n",
    "\n",
    "    def _document_to_dict(self, doc):\n",
    "        \"\"\"\n",
    "        Convert a document into a dictionary representation.\n",
    "        Override this method to include custom tokenization.\n",
    "        \"\"\"\n",
    "        if isinstance(doc, dict):\n",
    "            doc_dict = doc\n",
    "        else:\n",
    "            doc_dict = {\n",
    "                \"docno\": str(doc[0]),\n",
    "                \"text\": doc[1]\n",
    "            }\n",
    "        \n",
    "        # Tokenize the text using the custom tokenizer\n",
    "        tokens = self.custom_tokenizer(doc_dict[\"text\"])\n",
    "        \n",
    "        # Create a dictionary with tokenized terms\n",
    "        doc_dict[\"text\"] = \" \".join(tokens)\n",
    "        return doc_dict\n",
    "\n",
    "    def index(self, iter, fields=('text',), **kwargs):\n",
    "        \"\"\"\n",
    "        Override the index method if necessary.\n",
    "        \"\"\"\n",
    "        return super().index(iter, fields, **kwargs)\n",
    "\n",
    "# Initialize PyTerrier\n",
    "\n",
    "\n",
    "# Create an instance of the custom indexer\n",
    "index_path = \"./my_custom_index\"\n",
    "custom_indexer = CustomTokenizerIterDictIndexer(index_path, overwrite = True, tokenizer=ngram_tokenizer)\n",
    "\n",
    "# Example documents to index\n",
    "docs = [\n",
    "    {\"docno\": \"0\", \"text\": \"information retrieval systems for about a million times i try to do the system for the retrieval pipeline, why doesnt it work\"},\n",
    "    {\"docno\": \"1\", \"text\": \"\"}\n",
    "]\n",
    "\n",
    "# Index the documents\n",
    "index_ref = custom_indexer.index(docs)\n",
    "\n",
    "# Now you can use the index_ref as usual\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "print(index.getCollectionStatistics())\n",
    "for term, le in index.getLexicon():\n",
    "    print(term) \n",
    "    print(le.getFrequency())\n",
    "\n",
    "# Access the MetaIndex and Lexicon\n",
    "meta = index.getMetaIndex()\n",
    "lexicon = index.getLexicon()\n",
    "inverted = index.getInvertedIndex()\n",
    "print(inverted)\n",
    "print(inverted)\n",
    "\n",
    "\n",
    "# Print indexed terms for each document\n",
    "for doc in docs:\n",
    "    doc_id = meta.getDocument(\"docno\", doc[\"docno\"])\n",
    "    print(f\"Document {doc['docno']} index entries:\")\n",
    "    for termid in index.getInvertedIndex().getLexicon().getLexiconEntry(termid):\n",
    "        term = lexicon.getLexiconEntry(termid).getKey()\n",
    "        freq = index.getDocumentIndex()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Retrieval Pipeline\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, we have a short look at the first three topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>machine learning language identification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>social media detect self harm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                                     query\n",
       "0   1  retrieval system improving effectiveness\n",
       "1   2  machine learning language identification\n",
       "2   3             social media detect self harm"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('First, we have a short look at the first three topics:')\n",
    "\n",
    "test_dataset.get_topics('text').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n",
      "Done. Here are the first 10 entries of the run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.394619</td>\n",
       "      <td>retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.394619</td>\n",
       "      <td>retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.573390</td>\n",
       "      <td>retrieval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid docno  rank     score      query\n",
       "0   0      0     0     0 -0.394619  retrieval\n",
       "1   0      4     4     1 -0.394619  retrieval\n",
       "2   0      1     1     2 -0.573390  retrieval"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "#run = bm25(test_dataset.get_topics('text'))\n",
    "run = bm25([{\"qid\": \"0\", \"query\" : \"retrieval\"}])\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "run.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes for docid 0: {'docno': '0', 'text': ''}\n",
      "Attributes for docid 1: {'docno': '1', 'text': ''}\n",
      "Attributes for docid 2: {'docno': '2', 'text': ''}\n",
      "Attributes for docid 3: {'docno': '3', 'text': ''}\n"
     ]
    }
   ],
   "source": [
    "#index = pt.IndexFactory.of(index_ref)\n",
    "meta = index.getMetaIndex()\n",
    "\n",
    "# List of metadata keys\n",
    "meta_keys = ['docno', 'text']  # Adjust this list based on your meta fields\n",
    "\n",
    "# Function to print document attributes\n",
    "def print_doc_attributes(docid):\n",
    "    attributes = {key: meta.getItem(key, docid) for key in meta_keys}\n",
    "    print(f\"Attributes for docid {docid}: {attributes}\")\n",
    "\n",
    "# Example: Print attributes for the first 5 documents\n",
    "for docid in range(4):\n",
    "    print_doc_attributes(docid)\n",
    "\n",
    "# Function to print document attributes by docno\n",
    "def print_doc_attributes_by_docno(docno):\n",
    "    try:\n",
    "        docid = meta.getDocument(\"docno\", docno)\n",
    "        print_doc_attributes(docid)\n",
    "    except KeyError:\n",
    "        print(f\"Document with docno {docno} not found.\")\n",
    "\n",
    "# List of specific docnos to retrieve\n",
    "#docnos = ['W05-0704']\n",
    "\n",
    "# Retrieve and print attributes for each specified docno\n",
    "#for docno in docnos:\n",
    " #   print_doc_attributes_by_docno(docno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
