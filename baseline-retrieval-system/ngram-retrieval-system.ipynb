{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tira in /usr/local/lib/python3.10/dist-packages (0.0.132)\n",
      "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
      "Requirement already satisfied: python-terrier in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
      "Requirement already satisfied: docker==6.*,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tira) (6.1.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tira) (2.1.3)\n",
      "Requirement already satisfied: requests==2.*,>=2.26 in /usr/local/lib/python3.10/dist-packages (from tira) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tira) (4.66.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (1.7.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (2.1.0)\n",
      "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (23.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.3.2)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.3.2)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (6.0.1)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.3.2)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.6)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.12.2)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.12)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.3)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.9.3)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.6)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (1.26.2)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (3.2.3)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.7)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: matchpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.0)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.4)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: typish>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.5)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /usr/local/lib/python3.10/dist-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.10/dist-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (2.1.3)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3.post1)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (0.5.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->python-terrier) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install tira ir-datasets python-terrier\n",
    "import pyterrier as pt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tira.third_party_integrations import persist_and_normalize_run,  ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Start Pyterrier\n",
    "Only start pyterrier if it's not already started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "# Initialize PyTerrier\n",
    "if not pt.started():\n",
    "  pt.init()\n",
    "ensure_pyterrier_is_loaded\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the dataset and convert it into panda dataframe\n",
    "Right now, we are only manually loading a small dataset for testing purposes. \n",
    "This is where later, we need to load the proper dataset from Tira and put it into the correct panda dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = [\n",
    "    [\"d1\", \"systems for implementing machine learning algorithms\"], \n",
    "    [\"d2\", \"algorithms that help with optimizing machine learning\"], \n",
    "    [\"d3\", \"machine learning systems\"], \n",
    "    [\"d4\", \"learning machine systems\"],\n",
    "    [\"d5\", \"white quick fox\"]\n",
    "    ]\n",
    "#for some reason, i cannot get the dataset in this notebook, while it works in another notebook\n",
    "#real_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "#print(test_dataset)\n",
    "\n",
    "docs_df = pd.DataFrame(test_dataset, columns=[\"docno\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful method for getting all ngrams in a String, returns a list of the ngrams\n",
    "# Not currently used anywhere\n",
    "def tokenize_ngrams(text, n=2):\n",
    "    tokens = text.split()\n",
    "    ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the index\n",
    "Here, we create our index. We use DFIndexer which is used for indexing Panda dataframes. We chose this indexer as IterDictIndexer, our first choice, didn't support block queries, which are integral to our ngram approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.50documents/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing complete: ./test_index/data.properties\n",
      "\n",
      "Some useful stats about our index:\n",
      "Number of documents: 5\n",
      "Number of terms: 10\n",
      "Number of postings: 19\n",
      "Number of fields: 0\n",
      "Number of tokens: 19\n",
      "Field names: []\n",
      "Positions:   true\n",
      "\n",
      "Total frequencies for each term in our index:\n",
      "2 algorithm\n",
      "1 fox\n",
      "1 help\n",
      "1 implement\n",
      "4 learn\n",
      "4 machin\n",
      "1 optim\n",
      "1 quick\n",
      "3 system\n",
      "1 white\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Create Indexer \n",
    "#overwrite = True : if index already exists, delete the old one\n",
    "#blocks = True : allow block queries (which we use for ngrams)\n",
    "indexer = pt.DFIndexer(\"./test_index\", overwrite=True, blocks=True, verbose=True)\n",
    "\n",
    "# Create the index from our data using our indexer. Here, We need to specify the attributes from the dataframe to take into the index\n",
    "index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n",
    "\n",
    "print(f\"Indexing complete: {index_ref.toString()}\")\n",
    "\n",
    "# Create the actual index which will be used for retrieval\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "# print some stats from our index for debugging purposes\n",
    "print(\"\\nSome useful stats about our index:\")\n",
    "print(index.getCollectionStatistics())\n",
    "\n",
    "# Print the frequency of every indexed term\n",
    "print(\"Total frequencies for each term in our index:\")\n",
    "for term, le in index.getLexicon():\n",
    "    #print(le.getFrequency() + \" \" term) \n",
    "    print(str(le.getFrequency()) + \" \"+ term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Define the retrieval pipeline\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bm25 = pt.BatchRetrieve(index)\n",
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: load and modify the queries\n",
    "This is where we modify the queries to include ngrams. We achieve this by using specific operators from the pyterrier query language https://github.com/terrier-org/terrier-core/blob/5.x/doc/querylanguage.md . \n",
    "\n",
    "Right now, we only load one single example query for testing purposes. But this would be where we first load all the queries from Tira and then modify them to include ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "['machine$learning']\n"
     ]
    }
   ],
   "source": [
    "#query for ngrams by typing '\"word1 word2\"' will only return documents where \"word1 word2\" appear in this constellation (document where they dont appear like this will not get ranked)\n",
    "#query = 'brown fox'\n",
    "query = 'machine learning'\n",
    "tokenise_query_ngram = pt.rewrite.tokenise(lambda query: tokenize_ngrams(query))\n",
    "print()\n",
    "poortokenisation = tokenise_query_ngram >> pt.BatchRetrieve(index_ref)\n",
    "print()\n",
    "df = pd.DataFrame([{\"query\": query}])\n",
    "transformed_df = tokenise_query_ngram.transform(df)\n",
    "print(transformed_df[\"query\"].values)\n",
    "#TODO put multiple queries into list and run on all queries\n",
    "#TODO load queries from Tira\n",
    "#TODO add ngrams to the queries like shown, but only for real ngrams. perhaps use LLMs to find out which ngrams are real phrases from the english language and only put those\n",
    "# or perhaps just weight all the ngrams but dont make them necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Create the run\n",
    "Print the top 10 best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  qid  docid docno  rank     score           query_0             query\n",
      "0   1      2    d3     0  0.753881  machine learning  machine learning\n",
      "1   1      3    d4     1  0.753881  machine learning  machine learning\n",
      "2   1      0    d1     2  0.698101  machine learning  machine learning\n",
      "3   1      1    d2     3  0.698101  machine learning  machine learning\n"
     ]
    }
   ],
   "source": [
    "run = poortokenisation.search(query)\n",
    "print(run.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the attributes of the results. Not sure why the text turns empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes for docid 0: {'docno': 'd1', 'text': ''}\n",
      "Attributes for docid 1: {'docno': 'd2', 'text': ''}\n",
      "Attributes for docid 2: {'docno': 'd3', 'text': ''}\n"
     ]
    }
   ],
   "source": [
    "#index = pt.IndexFactory.of(index_ref)\n",
    "meta = index.getMetaIndex()\n",
    "\n",
    "# List of metadata keys\n",
    "meta_keys = ['docno', 'text']  # Adjust this list based on your meta fields\n",
    "\n",
    "# Function to print document attributes\n",
    "def print_doc_attributes(docid):\n",
    "    attributes = {key: meta.getItem(key, docid) for key in meta_keys}\n",
    "    print(f\"Attributes for docid {docid}: {attributes}\")\n",
    "\n",
    "# Example: Print attributes for the first 5 documents\n",
    "for docid in range(3):\n",
    "    print_doc_attributes(docid)\n",
    "\n",
    "# Function to print document attributes by docno\n",
    "def print_doc_attributes_by_docno(docno):\n",
    "    try:\n",
    "        docid = meta.getDocument(\"docno\", docno)\n",
    "        print_doc_attributes(docid)\n",
    "    except KeyError:\n",
    "        print(f\"Document with docno {docno} not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legacy Code and old methods that might still be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Apply n-gram tokenization to the dataset\\nfor doc in dataset:\\n    ngram_tokens1 = tokenize_ngrams(doc['text'], n=1)\\n    ngram_tokens2 = tokenize_ngrams(doc['text'], n=2)\\n    ngram_tokens = ngram_tokens1 + ngram_tokens2\\n    token_counts = Counter(ngram_tokens)\\n    doc['toks'] = token_counts\\n    del doc['text']  # Remove the 'text' field as it's not needed anymore\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Apply n-gram tokenization to the dataset\n",
    "for doc in dataset:\n",
    "    ngram_tokens1 = tokenize_ngrams(doc['text'], n=1)\n",
    "    ngram_tokens2 = tokenize_ngrams(doc['text'], n=2)\n",
    "    ngram_tokens = ngram_tokens1 + ngram_tokens2\n",
    "    token_counts = Counter(ngram_tokens)\n",
    "    doc['toks'] = token_counts\n",
    "    del doc['text']  # Remove the 'text' field as it's not needed anymore\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
