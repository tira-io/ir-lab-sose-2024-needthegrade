{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expanding abbrevations in a query -> combine with question query if there is a question query which contains an abbrevation\n",
    "#test phrase based expansion, expand to \"This paper introduces Information Retrieval\" -> set retrieval, boolean retrieval?\n",
    "#adjust bm25 parameters\n",
    "#adjust question prompt so that it catches the intent behind why questions better\n",
    "#What and Why are probably most common question words -> specialize on them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/2024-05-04-16-05-53.zip\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 19.5M/19.5M [00:00<00:00, 60.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_runs/ir-lab-sose-2024/ir-acl-anthology-20240504-training/tira-ir-starter\n"
     ]
    }
   ],
   "source": [
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "\n",
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Ngram Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/ir-acl-anthology-20240504-inputs.zip?download=1\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 39.4M/39.4M [00:00<00:00, 59.6MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_datasets/ir-lab-sose-2024/ir-acl-anthology-20240504-training/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 53248.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 126958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs =  pt_dataset.get_corpus_iter()\n",
    "docs = list(docs)\n",
    "count = sum(1 for _ in docs)\n",
    "docs = docs[:126959]\n",
    "print(\"Number of documents:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that removes all special characters from a String, and returns either a String or a list of all words\n",
    "def clean_text(text, return_as_list = False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text) #remove non-alphanumeric characters, except spaces\n",
    "    if return_as_list:\n",
    "        word_list = cleaned_text.split()\n",
    "        return word_list\n",
    "    else:\n",
    "        return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Ensure NLTK data directory is set correctly\n",
    "nltk.data.path.append(\"/usr/local/nltk_data\")\n",
    "\n",
    "# Download 'stopwords' corpus to the specified directory\n",
    "nltk.download('stopwords', download_dir=\"/usr/nltk_data\")\n",
    "\n",
    "# Get the set of stopwords for the English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text, return_as_list = False):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Join the filtered words back into a single string\n",
    "\n",
    "    if return_as_list:\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_text(text, return_as_list = False):\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    if return_as_list:\n",
    "        return stemmed_words\n",
    "    # Join the stemmed words back into a single string\n",
    "    else:\n",
    "        return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our ngram tokenizer. It takes a string and returns a dict of all ngrams, where each ngram is seperated by $$ so it will be parsed as one token\n",
    "\n",
    "def tokenize_ngrams_to_dict(text, n1=1, n2=2):\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text.split(' ')\n",
    "    words = [word for word in words if len(''.join(format(ord(c), '08b') for c in word)) <= 60]\n",
    "\n",
    "    # Initialize an empty Counter to hold all n-grams\n",
    "    all_ngram_counts = Counter()\n",
    "    \n",
    "    # Loop through each n from n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Update the Counter with the current n-grams\n",
    "        all_ngram_counts.update(ngrams)\n",
    "    \n",
    "    return dict(all_ngram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'docno': 'O02-2002', 'toks': {'studi': 1, 'word': 7, 'similar': 8, 'use': 3, 'context': 5, 'vector': 3, 'model': 2, 'need': 1, 'measur': 2, 'process': 1, 'natur': 1, 'languag': 1, 'especi': 1, 'general': 1, 'classif': 1, 'exampl': 1, 'base': 3, 'usual': 1, 'two': 1, 'defin': 1, 'accord': 3, 'distanc': 1, 'semant': 6, 'class': 2, 'less': 1, 'consid': 1, 'syntact': 5, 'ie': 2, 'howev': 1, 'real': 1, 'applic': 1, 'requir': 1, 'weight': 1, 'differ': 1, 'mixtur': 1, 'paper': 1, 'propos': 1, 'relat': 1, 'co': 2, 'occurr': 2, 'adopt': 1, 'inform': 1, 'theoret': 1, 'solv': 1, 'problem': 1, 'data': 1, 'spars': 1, 'precis': 1, 'featur': 2, 'deriv': 1, 'pars': 1, 'environ': 1, 'adjust': 1, 'idf': 1, 'invers': 1, 'valu': 2, 'agglom': 1, 'cluster': 1, 'appli': 1, 'group': 2, 'turn': 1, 'togeth': 1, 'studi$$word': 1, 'word$$similar': 4, 'similar$$use': 1, 'use$$context': 1, 'context$$vector': 3, 'vector$$model': 1, 'model$$need': 1, 'need$$measur': 1, 'measur$$word': 1, 'similar$$process': 1, 'process$$natur': 1, 'natur$$languag': 1, 'languag$$especi': 1, 'especi$$use': 1, 'use$$general': 1, 'general$$classif': 1, 'classif$$exampl': 1, 'exampl$$base': 1, 'base$$usual': 1, 'usual$$measur': 1, 'measur$$similar': 1, 'similar$$two': 1, 'two$$word': 1, 'word$$defin': 1, 'defin$$accord': 1, 'accord$$distanc': 1, 'distanc$$semant': 1, 'semant$$class': 2, 'class$$semant': 1, 'semant$$less': 1, 'less$$semant': 1, 'semant$$base': 1, 'base$$consid': 1, 'consid$$syntact': 1, 'syntact$$ie': 1, 'ie$$howev': 1, 'howev$$real': 1, 'real$$applic': 1, 'applic$$semant': 1, 'semant$$syntact': 1, 'syntact$$similar': 1, 'similar$$requir': 1, 'requir$$weight': 1, 'weight$$differ': 1, 'differ$$word': 1, 'similar$$base': 1, 'base$$context': 1, 'vector$$mixtur': 1, 'mixtur$$syntact': 1, 'syntact$$semant': 2, 'semant$$ie': 1, 'ie$$paper': 1, 'paper$$propos': 1, 'propos$$use': 1, 'use$$syntact': 1, 'syntact$$relat': 1, 'relat$$co': 1, 'co$$occurr': 2, 'occurr$$context': 2, 'vector$$adopt': 1, 'adopt$$inform': 1, 'inform$$theoret': 1, 'theoret$$model': 1, 'model$$solv': 1, 'solv$$problem': 1, 'problem$$data': 1, 'data$$spars': 1, 'spars$$precis': 1, 'precis$$co': 1, 'context$$featur': 2, 'featur$$deriv': 1, 'deriv$$pars': 1, 'pars$$environ': 1, 'environ$$word': 1, 'word$$context': 1, 'featur$$adjust': 1, 'adjust$$accord': 1, 'accord$$idf': 1, 'idf$$invers': 1, 'invers$$valu': 1, 'valu$$agglom': 1, 'agglom$$cluster': 1, 'cluster$$appli': 1, 'appli$$group': 1, 'group$$similar': 1, 'similar$$word': 1, 'word$$accord': 1, 'accord$$similar': 1, 'similar$$valu': 1, 'valu$$turn': 1, 'turn$$word': 1, 'similar$$syntact': 1, 'class$$group': 1, 'group$$togeth': 1}}\n",
      "{'docno': 'L02-1310', 'toks': {'larg': 1, 'sens': 1, 'tag': 1, 'corpora': 1, 'larg$$sens': 1, 'sens$$tag': 1, 'tag$$corpora': 1}}\n",
      "{'docno': 'R13-1042', 'toks': {'use': 2, 'pairwis': 2, 'email': 6, 'classif': 2, 'thread': 6, 'task': 1, 'separ': 1, 'convers': 1, 'whose': 1, 'distort': 1, 'lost': 1, 'paper': 1, 'perform': 2, 'text': 4, 'similar': 5, 'measur': 1, 'non': 1, 'quot': 1, 'show': 1, 'content': 2, 'metric': 2, 'style': 1, 'class': 2, 'balanc': 1, 'set': 1, 'ii': 1, 'featur': 2, 'depend': 1, 'semant': 2, 'corpus': 4, 'still': 1, 'effect': 1, 'even': 1, 'control': 1, 'make': 1, 'avail': 1, 'enron': 2, 'newli': 1, 'extract': 1, '70': 1, '178': 1, 'use$$pairwis': 1, 'pairwis$$email': 1, 'email$$classif': 1, 'classif$$email': 1, 'email$$thread': 2, 'thread$$thread': 1, 'thread$$task': 1, 'task$$separ': 1, 'separ$$convers': 1, 'convers$$whose': 1, 'whose$$thread': 1, 'thread$$distort': 1, 'distort$$lost': 1, 'lost$$paper': 1, 'paper$$perform': 1, 'perform$$email': 1, 'thread$$pairwis': 1, 'pairwis$$classif': 1, 'classif$$use': 1, 'use$$text': 1, 'text$$similar': 3, 'similar$$measur': 1, 'measur$$non': 1, 'non$$quot': 1, 'quot$$text': 1, 'text$$email': 1, 'email$$show': 1, 'show$$content': 1, 'content$$text': 1, 'similar$$metric': 2, 'metric$$style': 1, 'style$$text': 1, 'metric$$class': 1, 'class$$balanc': 1, 'balanc$$class': 1, 'class$$set': 1, 'set$$ii': 1, 'ii$$featur': 1, 'featur$$perform': 1, 'perform$$depend': 1, 'depend$$semant': 1, 'semant$$similar': 2, 'similar$$corpus': 1, 'corpus$$content': 1, 'content$$featur': 1, 'featur$$still': 1, 'still$$effect': 1, 'effect$$even': 1, 'even$$control': 1, 'control$$semant': 1, 'similar$$make': 1, 'make$$avail': 1, 'avail$$enron': 1, 'enron$$thread': 1, 'thread$$corpus': 1, 'corpus$$newli': 1, 'newli$$extract': 1, 'extract$$corpus': 1, 'corpus$$70': 1, '70$$178': 1, '178$$thread': 1, 'thread$$email': 1, 'email$$enron': 1, 'enron$$email': 1, 'email$$corpus': 1}}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "        if 'text' in doc:\n",
    "            doc['text'] = clean_text(doc['text'])\n",
    "            doc['text'] = remove_stopwords(doc['text'])\n",
    "            doc['text'] = stem_text(doc['text'])\n",
    "            \n",
    "            doc_1gram = tokenize_ngrams_to_dict(doc['text'], n1=1, n2=2) # Apply n-gram tokenization to the dataset\n",
    "\n",
    "            doc['toks'] = doc_1gram # create new toks field for tokenfrequency\n",
    "            del doc['text']  #This will delete the 'text' field from the documents\n",
    "    \n",
    "for i, doc in enumerate(docs):\n",
    "     if i == 3:\n",
    "           break\n",
    "     print(doc)\n",
    "\n",
    "#remove all empty documents\n",
    "docs = [d for d in docs if any(k != '' for k in d['toks'].keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 126824\n",
      "Number of terms: 1735213\n",
      "Number of postings: 11740755\n",
      "Number of fields: 0\n",
      "Number of tokens: 14688164\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n",
      "0\n",
      "9406\n",
      "00\n",
      "278\n",
      "000\n",
      "1761\n",
      "0001\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# Initialize the IterDictIndexer with pretokenised set to True\n",
    "iter_indexer = pt.IterDictIndexer(\"./ngramindex\", overwrite=True, meta={'docno': 35}, pretokenised=True, verbose = True, type = pt.index.IndexingType.SINGLEPASS)\n",
    "\n",
    "# Index our pretokenized documents\n",
    "index_ref = iter_indexer.index(docs)\n",
    "\n",
    "index_ngram = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "#Print some stats about our index\n",
    "print(index_ngram.getCollectionStatistics())\n",
    "\n",
    "# Access the MetaIndex and Lexicon\n",
    "meta = index_ngram.getMetaIndex()\n",
    "lexicon = index_ngram.getLexicon()\n",
    "\n",
    "\n",
    "i = 0\n",
    "for term, le in index.getLexicon():\n",
    "    i = i+1\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(term) \n",
    "    print(le.getFrequency())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI() #connect to OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt(prompt, model=\"gpt-4\", temperature=0):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiate between question querys, querys that contain an abbrevation and keyword querys to treat each query properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/ir-acl-anthology-20240504-truth.zip?download=1\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 29.6k/29.6k [00:00<00:00, 1.49MiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_datasets/ir-lab-sose-2024/ir-acl-anthology-20240504-training/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "querys = pt_dataset.get_topics('query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out each detected abbrevation and concat it to the query in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': False, '2': False, '3': False, '4': False, '5': False, '6': False, '7': False, '8': False, '9': False, '10': False, '11': True, '12': False, '13': False, '14': False, '15': False, '16': False, '17': False, '19': False, '20': False, '21': False, '22': False, '23': False, '24': False, '25': False, '26': False, '27': False, '28': False, '29': False, '30': False, '31': False, '32': False, '33': False, '34': False, '35': False, '36': False, '37': False, '38': False, '39': False, '40': False, '41': False, '42': False, '43': False, '44': False, '45': False, '46': False, '47': False, '48': False, '49': False, '50': False, '51': False, '52': False, '53': False, '54': False, '55': True, '56': False, '57': False, '58': False, '59': True, '60': False, '61': False, '62': False, '63': False, '64': False, '65': False, '66': False, '67': False, '68': False, '18': False}\n"
     ]
    }
   ],
   "source": [
    "answers = dict()\n",
    "for i in range(len(querys)):\n",
    "    determine_abbrevation = f\"\"\" \n",
    "    You are an scientific expert especially in the domain of Information Retrieval. Your task is to detect whether\n",
    "    a given query, which is given as a text below delimited by triple quotes, contains an abbrevation then answer with yes or not then answer with no.\n",
    "    For example given a query 'What is crx' you should answer yes, since cxr is the abbrevation for the medical term\n",
    "    'chest X-Ray'. However if the given query is 'What is Information Retrieval' you should answer no, since there is no\n",
    "    abbrevation in the query.\n",
    "\n",
    "    query: '''{querys['query'][i]}'''\n",
    "    \"\"\"\n",
    "    answer = ask_gpt(prompt=determine_abbrevation) #check answer more carefully perhaps model will return not only {yes,no}\n",
    "    #print(answer)\n",
    "    qid = str(querys['qid'][i])\n",
    "    if \"yes\" in answer.lower().strip():\n",
    "        answers[qid] = True\n",
    "    else:\n",
    "        answers[qid] = False\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old query:  algorithm acceleration with nvidia cuda\n",
      "New query:  algorithm acceleration with nvidia cuda compute unified device architecture\n",
      "Old query:  bm25\n",
      "New query:  bm25 best match 25\n",
      "Old query:  what is ahp\n",
      "New query:  what is ahp analytic hierarchy process\n"
     ]
    }
   ],
   "source": [
    "#could be more efficient if in answers text is saved only for qid where abbrevation=yes\n",
    "for key in answers.keys():\n",
    "    if bool(answers[key]):\n",
    "        #find query\n",
    "        for i in range(len(querys)):\n",
    "            if querys['qid'][i] == key:\n",
    "                query= querys['query'][i]\n",
    "                print(\"Old query: \",query)\n",
    "        #ask gpt to expand query\n",
    "        expand = f\"\"\" \n",
    "        You are an scientific expert especially in the domain of Information Retrieval. Your are given a query, which is below\n",
    "        delimited by triple quotes, which contains an abbrevation. Your task is to identify the abbrevation and write it, then\n",
    "        concat the original query with the written out abbrevation and return this new query as string only.\n",
    "        For example given a query 'What is crx' you should detect that the abbrevation is crx, since cxr is the abbrevation for the medical term\n",
    "        'chest X-Ray', then you should concat the originial query with the abbrevation 'chest X-Ray' resulting in a new query 'What is crx chest x-ray' which\n",
    "        you should return. Another example, given the query 'Algorithms of nlp' you should detect that the abbrevation is nlp, since nlp is the abbrevation\n",
    "        for the term 'natural language processing', then you should concat the original query 'Algorithms of nlp' with the abbrevation 'natural language processing'  \n",
    "        resulting in a new query 'Algorithms of nlp natural language processing' which you should return.\n",
    "        Please only answer with the new query. So your answer should only include the original query and the detected abbreviated words and no additional information.\n",
    "        Don't wrap your answer in quotation marks.\n",
    "\n",
    "        query: '''{query}'''\n",
    "        \"\"\"\n",
    "        new_query = ask_gpt(prompt=expand).lower().strip().replace(\"'\", \" \").replace('\"', ' ')\n",
    "        print(\"New query: \",new_query)\n",
    "        #overwrite old query\n",
    "        querys['query'][i] = new_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now differentiate between a keyword query where ngram tokenization is usefull and question querys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'retrieval system improving effectiveness'], ['2', 'machine learning language identification'], ['3', 'social media detect self harm'], ['4', 'stemming for arabic languages'], ['5', 'audio based animal recognition'], ['6', 'comparison different retrieval models'], ['7', 'cache architecture'], ['8', 'document scoping formula'], ['9', 'pseudo relevance feedback'], ['10', 'how to represent natural conversations in word nets'], ['11', 'algorithm acceleration with nvidia cuda'], ['12', 'mention of algorithm'], ['13', 'at least three authors'], ['14', 'german domain'], ['15', 'mention of open source'], ['16', 'inclusion of text mining'], ['17', 'the ethics of artificial intelligence'], ['19', 'machine learning for more relevant results'], ['20', 'crawling websites using machine learning'], ['21', 'recommenders influence on users'], ['22', 'search engine caching effects'], ['23', 'consumer product reviews'], ['24', 'limitations machine learning'], ['25', 'medicine related research'], ['26', 'natural language processing'], ['27', 'graph based ranking'], ['28', 'medical studies that use information retrieval'], ['29', 'information retrieval on different language sources'], ['30', 'papers that compare multiple information retrieval methods'], ['31', 'risks of information retrieval in social media'], ['32', 'actual experiments that strengthen theoretical knowledge'], ['33', 'fake news detection'], ['34', 'multimedia retrieval'], ['35', 'processing natural language for information retrieval'], ['36', 'recommendation systems'], ['37', 'personalised search in e commerce'], ['38', 'sentiment analysis'], ['39', 'informational retrieval using neural networks'], ['40', 'query log analysis'], ['41', 'entity recognition'], ['42', 'relevance assessments'], ['43', 'deep neural networks'], ['44', 'information retrieval'], ['45', 'analysis for android apps'], ['46', 'the university of amsterdam'], ['47', 'neural ranking for ecommerce product search'], ['48', 'web pages evolution'], ['49', 'exhaustivity of index'], ['50', 'query optimization'], ['51', 'cosine similarity vector'], ['52', 'reverse indexing'], ['53', 'index compression techniques'], ['54', 'search engine optimization with query logs'], ['55', 'bm25'], ['56', 'what makes natural language processing natural'], ['57', 'principle of a information retrieval indexing'], ['58', 'architecture of web search engine'], ['59', 'what is ahp'], ['60', 'what is information retrieval'], ['61', 'efficient retrieval algorithms'], ['62', 'how to avoid spam results'], ['63', 'information retrieval with algorithms'], ['64', 'misspellings in queries'], ['65', 'information in different language'], ['66', 'abbreviations in queries'], ['67', 'lemmatization algorithms'], ['68', 'filter ad rich documents'], ['18', 'what is ahp analytic hierarchy process']]\n",
      "what is ahp analytic hierarchy process\n",
      "68\n",
      "retrieval system improving effectiveness retrieval$$system\n",
      "machine learning language identification machine$$learning language$$identification\n",
      "social media detect self harm social$$media self$$harm\n",
      "stemming for arabic languages arabic$$languages\n",
      "audio based animal recognition audio$$based animal$$recognition\n",
      "comparison different retrieval models retrieval$$models\n",
      "cache architecture\n",
      "document scoping formula document$$scoping scoping$$formula\n",
      "pseudo relevance feedback pseudo$$relevance relevance$$feedback\n",
      "how to represent natural conversations in word nets natural$$conversations word$$nets\n",
      "algorithm acceleration with nvidia cuda nvidia$$cuda\n",
      "mention of algorithm\n",
      "at least three authors at$$least three$$authors\n",
      "german domain german$$domain\n",
      "mention of open source open$$source\n",
      "inclusion of text mining text$$mining\n",
      "the ethics of artificial intelligence artificial$$intelligence\n",
      "machine learning for more relevant results machine$$learning\n",
      "crawling websites using machine learning machine$$learning\n",
      "recommenders influence on users\n",
      "search engine caching effects search$$engine caching$$effects\n",
      "consumer product reviews consumer$$product product$$reviews\n",
      "limitations machine learning machine$$learning\n",
      "medicine related research medicine$$related related$$research\n",
      "natural language processing natural$$language language$$processing\n",
      "graph based ranking graph$$based based$$ranking\n",
      "medical studies that use information retrieval information$$retrieval\n",
      "information retrieval on different language sources information$$retrieval language$$sources\n",
      "papers that compare multiple information retrieval methods information$$retrieval\n",
      "risks of information retrieval in social media information$$retrieval social$$media\n",
      "actual experiments that strengthen theoretical knowledge theoretical$$knowledge\n",
      "fake news detection fake$$news news$$detection\n",
      "multimedia retrieval multimedia$$retrieval\n",
      "processing natural language for information retrieval natural$$language information$$retrieval\n",
      "recommendation systems recommendation$$systems\n",
      "personalised search in e commerce personalised$$search e$$commerce\n",
      "sentiment analysis sentiment$$analysis\n",
      "informational retrieval using neural networks informational$$retrieval neural$$networks\n",
      "query log analysis query$$log log$$analysis\n",
      "entity recognition entity$$recognition\n",
      "relevance assessments\n",
      "deep neural networks deep$$neural neural$$networks\n",
      "information retrieval information$$retrieval\n",
      "analysis for android apps android$$apps\n",
      "the university of amsterdam university$$of of$$amsterdam\n",
      "neural ranking for ecommerce product search neural$$ranking product$$search\n",
      "web pages evolution web$$pages\n",
      "exhaustivity of index\n",
      "query optimization query$$optimization\n",
      "cosine similarity vector cosine$$similarity\n",
      "reverse indexing reverse$$indexing\n",
      "index compression techniques index$$compression compression$$techniques\n",
      "search engine optimization with query logs search$$engine engine$$optimization query$$logs\n",
      "bm25\n",
      "what makes natural language processing natural natural$$language language$$processing\n",
      "principle of a information retrieval indexing information$$retrieval\n",
      "architecture of web search engine web$$search search$$engine\n",
      "what is ahp\n",
      "what is information retrieval information$$retrieval\n",
      "efficient retrieval algorithms retrieval$$algorithms\n",
      "how to avoid spam results spam$$results\n",
      "information retrieval with algorithms information$$retrieval\n",
      "misspellings in queries\n",
      "information in different language different$$language\n",
      "abbreviations in queries\n",
      "lemmatization algorithms\n",
      "filter ad rich documents ad$$rich\n",
      "what is ahp analytic hierarchy process analytic$$hierarchy hierarchy$$process\n"
     ]
    }
   ],
   "source": [
    "#querys = pt_dataset.get_topics('query')\n",
    "expanded_queries_list = querys.values.tolist()\n",
    "print(expanded_queries_list)\n",
    "print(expanded_queries_list[67][1])\n",
    "print(len(expanded_queries_list))\n",
    "#gpt goes through each query, question querys qid will be stored in answers (list)\n",
    "answers = list()\n",
    "for i in range(len(expanded_queries_list)):\n",
    "#for i in range(0, 2):\n",
    "    \n",
    "\n",
    "    determine_ngrams = f\"\"\" \n",
    "    You are an scientific expert in the domain of Information Retrieval and linguistics. Your task is to detect whether\n",
    "    a given query, which is given as a text below delimited by triple quotes, contains bigrams. This means, you should check for all \n",
    "    bigrams in the query, if they are an existing term consisting of multiple words. Then, your answer should be the original query \n",
    "    with all the bigrams you found appended in the format word1$$word2. Your answer should only include the query and the bigrams, no additional information.\n",
    "    This means that when there are no existing bigrams in the query, your answer should just be the original query. You should not wrap your answer in quotation marks.\n",
    "    For example given a query 'usage of machine learning in image recognition' you should answer\n",
    "      'usage of machine learning in image recognition machine$$learning image$$recognition'.\n",
    "\n",
    "    query: '''{expanded_queries_list[i][1]}'''\n",
    "    \"\"\"\n",
    "    answer = ask_gpt(prompt=determine_ngrams) #check answer more carefully perhaps model will return not only {yes,no}\n",
    "    print(answer)\n",
    "    expanded_queries_list[i].append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the retrieval pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine$$learning', 'machine']\n",
      "running$$sho are better than rune $$fast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#1. Chatgpt anwenden\n",
    "\n",
    "# Extract qid and query from each sublist\n",
    "data_expd = expanded_queries_list\n",
    "data_expd = [(sublist[0], sublist[1]) for sublist in expanded_queries_list]\n",
    "# Create pandas DataFrame\n",
    "df_expd = pd.DataFrame(data_expd, columns=['qid', 'query'])\n",
    "\n",
    "'''#Nimmt eine qid als String und returnt die expanded query von chatgpt\n",
    "def get_expanded_query(qid):\n",
    "    for query_list in expanded_queries:\n",
    "        if query_list[0] == qid:\n",
    "            return query_list[2]\n",
    "    return None\n",
    "\n",
    "print(get_expanded_query('1'))'''\n",
    "\n",
    "#TODO über die liste ist es nicht so gut, da wir in der pipeline eh jede query einzeln als string behandeln\n",
    "#daher hier die chatgpt methode in the transformer einfügen, und dann am ende noch als liste tokenisen\n",
    "#expand_query_transf = pt.rewrite.tokenise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2. special characters löschen ausser $$\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text_with_dollar_signs(text, return_as_list=False, keep_dollar_signs=False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    if keep_dollar_signs:\n",
    "        # Replace double dollar signs with a unique placeholder\n",
    "        text = text.replace('$$', 'DOUBLEDOLLARNGRAMS')\n",
    "        # Remove all non-alphanumeric characters except spaces\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "        # Replace placeholder back to double dollar signs\n",
    "        cleaned_text = cleaned_text.replace('DOUBLEDOLLARNGRAMS', '$$')\n",
    "    else:\n",
    "        # Remove all non-alphanumeric characters except spaces\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    if return_as_list:\n",
    "        word_list = cleaned_text.split()\n",
    "        return word_list\n",
    "    else:\n",
    "        return cleaned_text\n",
    "\n",
    "# Examples\n",
    "#print(clean_text_with_dollar_signs(get_expanded_query('1'), keep_dollar_signs=True, return_as_list=True))\n",
    "\n",
    "transf_clean_text = pt.rewrite.tokenise(lambda query: clean_text_with_dollar_signs(query, return_as_list=True, keep_dollar_signs=True))\n",
    "\n",
    "#3. stopwords löschen\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Ensure NLTK data directory is set correctly\n",
    "nltk.data.path.append(\"/usr/local/nltk_data\")\n",
    "\n",
    "# Download 'stopwords' corpus to the specified directory\n",
    "nltk.download('stopwords', download_dir=\"/usr/nltk_data\")\n",
    "\n",
    "# Get the set of stopwords for the English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords_with_dollar_signs(text, return_as_list=False):\n",
    "    words = text.split()\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if '$$' in word:\n",
    "            # Split the word at '$$' and check both parts\n",
    "            #part1, part2 = word.split('$$')\n",
    "            #if part1.lower() in stop_words or part2.lower() in stop_words:\n",
    "            #    continue  # Skip this word if either part is a stopword\n",
    "            #else:\n",
    "            #    filtered_words.append(word)\n",
    "\n",
    "            parts = word.split('$$')\n",
    "            skip_word = False\n",
    "            for part in parts:\n",
    "                if part.lower() in stop_words:\n",
    "                    skip_word = True\n",
    "                    break  # If any part is a stopword, skip the entire word\n",
    "            if not skip_word:\n",
    "                filtered_words.append(word)\n",
    "        else:\n",
    "            if word.lower() not in stop_words:\n",
    "                filtered_words.append(word)\n",
    "    \n",
    "    if return_as_list:\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "transf_remove_stopwords = pt.rewrite.tokenise(lambda query: remove_stopwords_with_dollar_signs(query, return_as_list=True))\n",
    "# Examples\n",
    "print(remove_stopwords_with_dollar_signs(\"machine$$learning of learning$$of machine\", return_as_list=True))\n",
    "\n",
    "#4. stemming\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_text_with_dollar_signs(text, return_as_list=False):\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        if '$$' in word:\n",
    "            # Split the word at '$$' and stem each part\n",
    "            #part1, part2 = word.split('$$')\n",
    "            #stemmed_part1 = stemmer.stem(part1)\n",
    "            #stemmed_part2 = stemmer.stem(part2)\n",
    "            #stemmed_word = f\"{stemmed_part1}$${stemmed_part2}\"\n",
    "\n",
    "            parts = word.split('$$')\n",
    "            stemmed_parts = [stemmer.stem(part) for part in parts]\n",
    "            stemmed_word = '$$'.join(stemmed_parts)\n",
    "        else:\n",
    "            # Stem the word normally\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "        \n",
    "        stemmed_words.append(stemmed_word)\n",
    "\n",
    "    if return_as_list:\n",
    "        return stemmed_words\n",
    "    else:\n",
    "        return ' '.join(stemmed_words)\n",
    "    \n",
    "\n",
    "transf_stem_text = pt.rewrite.tokenise(lambda query: stem_text_with_dollar_signs(query, return_as_list=True))\n",
    "# Example\n",
    "print(stem_text(\"running$$shoes are better than runing $$fast\", return_as_list=False))\n",
    "\n",
    "#5. bm25 ohne termpipelines\n",
    "\n",
    "# This transformer will do the retrieval using bm25, and explicitly not apply any stemming and stopword removal\n",
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", verbose = True, properties={\"termpipelines\" : \"\"}, controls={\"bm25.b\": 0.2})#, \"bm25.k_1\": 0.1})\n",
    "\n",
    "# This is our retrieval pipeline\n",
    "#retr_pipeline = remove_special_characters >> remove_stopwords_from_query >> stem_query >> tokenise_query_ngram >> bm25\n",
    "retr_pipeline = transf_clean_text >> transf_remove_stopwords >> transf_stem_text >> bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n",
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25):   0%|          | 0/68 [00:00<?, ?q/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25): 100%|██████████| 68/68 [00:01<00:00, 37.04q/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Here are the first 10 entries of the run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>query_2</th>\n",
       "      <th>description</th>\n",
       "      <th>narrative</th>\n",
       "      <th>query_1</th>\n",
       "      <th>query_0</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>94795</td>\n",
       "      <td>2004.cikm_conference-2004.47</td>\n",
       "      <td>0</td>\n",
       "      <td>16.287685</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>124670</td>\n",
       "      <td>2006.ipm_journal-ir0volumeA42A3.2</td>\n",
       "      <td>1</td>\n",
       "      <td>15.362926</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>83395</td>\n",
       "      <td>1997.sigirconf_conference-97.36</td>\n",
       "      <td>2</td>\n",
       "      <td>15.220951</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>81597</td>\n",
       "      <td>2018.sigirconf_conference-2018.234</td>\n",
       "      <td>3</td>\n",
       "      <td>15.035347</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>82044</td>\n",
       "      <td>2007.sigirconf_conference-2007.212</td>\n",
       "      <td>4</td>\n",
       "      <td>15.028518</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>82438</td>\n",
       "      <td>1998.sigirconf_conference-98.39</td>\n",
       "      <td>5</td>\n",
       "      <td>15.008899</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>122357</td>\n",
       "      <td>2010.sigirjournals_journal-ir0volu</td>\n",
       "      <td>6</td>\n",
       "      <td>14.928209</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>125684</td>\n",
       "      <td>2005.ipm_journal-ir0volumeA41A5.11</td>\n",
       "      <td>7</td>\n",
       "      <td>14.908299</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>84816</td>\n",
       "      <td>2016.ntcir_conference-2016.90</td>\n",
       "      <td>8</td>\n",
       "      <td>14.725026</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>94352</td>\n",
       "      <td>2008.cikm_conference-2008.183</td>\n",
       "      <td>9</td>\n",
       "      <td>14.694944</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>What papers focus on improving the effectivene...</td>\n",
       "      <td>Relevant papers include research on what makes...</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "      <td>retriev system improv effect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                               docno  rank      score  \\\n",
       "0   1   94795        2004.cikm_conference-2004.47     0  16.287685   \n",
       "1   1  124670   2006.ipm_journal-ir0volumeA42A3.2     1  15.362926   \n",
       "2   1   83395     1997.sigirconf_conference-97.36     2  15.220951   \n",
       "3   1   81597  2018.sigirconf_conference-2018.234     3  15.035347   \n",
       "4   1   82044  2007.sigirconf_conference-2007.212     4  15.028518   \n",
       "5   1   82438     1998.sigirconf_conference-98.39     5  15.008899   \n",
       "6   1  122357  2010.sigirjournals_journal-ir0volu     6  14.928209   \n",
       "7   1  125684  2005.ipm_journal-ir0volumeA41A5.11     7  14.908299   \n",
       "8   1   84816       2016.ntcir_conference-2016.90     8  14.725026   \n",
       "9   1   94352       2008.cikm_conference-2008.183     9  14.694944   \n",
       "\n",
       "                                       text  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                      title  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                    query_2  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                         description  \\\n",
       "0  What papers focus on improving the effectivene...   \n",
       "1  What papers focus on improving the effectivene...   \n",
       "2  What papers focus on improving the effectivene...   \n",
       "3  What papers focus on improving the effectivene...   \n",
       "4  What papers focus on improving the effectivene...   \n",
       "5  What papers focus on improving the effectivene...   \n",
       "6  What papers focus on improving the effectivene...   \n",
       "7  What papers focus on improving the effectivene...   \n",
       "8  What papers focus on improving the effectivene...   \n",
       "9  What papers focus on improving the effectivene...   \n",
       "\n",
       "                                           narrative  \\\n",
       "0  Relevant papers include research on what makes...   \n",
       "1  Relevant papers include research on what makes...   \n",
       "2  Relevant papers include research on what makes...   \n",
       "3  Relevant papers include research on what makes...   \n",
       "4  Relevant papers include research on what makes...   \n",
       "5  Relevant papers include research on what makes...   \n",
       "6  Relevant papers include research on what makes...   \n",
       "7  Relevant papers include research on what makes...   \n",
       "8  Relevant papers include research on what makes...   \n",
       "9  Relevant papers include research on what makes...   \n",
       "\n",
       "                                    query_1  \\\n",
       "0  retrieval system improving effectiveness   \n",
       "1  retrieval system improving effectiveness   \n",
       "2  retrieval system improving effectiveness   \n",
       "3  retrieval system improving effectiveness   \n",
       "4  retrieval system improving effectiveness   \n",
       "5  retrieval system improving effectiveness   \n",
       "6  retrieval system improving effectiveness   \n",
       "7  retrieval system improving effectiveness   \n",
       "8  retrieval system improving effectiveness   \n",
       "9  retrieval system improving effectiveness   \n",
       "\n",
       "                                    query_0                         query  \n",
       "0  retrieval system improving effectiveness  retriev system improv effect  \n",
       "1  retrieval system improving effectiveness  retriev system improv effect  \n",
       "2  retrieval system improving effectiveness  retriev system improv effect  \n",
       "3  retrieval system improving effectiveness  retriev system improv effect  \n",
       "4  retrieval system improving effectiveness  retriev system improv effect  \n",
       "5  retrieval system improving effectiveness  retriev system improv effect  \n",
       "6  retrieval system improving effectiveness  retriev system improv effect  \n",
       "7  retrieval system improving effectiveness  retriev system improv effect  \n",
       "8  retrieval system improving effectiveness  retriev system improv effect  \n",
       "9  retrieval system improving effectiveness  retriev system improv effect  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "run = retr_pipeline(pt_dataset.get_topics()) #queries\n",
    "\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "run.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist run file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs/\".\n",
      "Done. run file is stored under \"../runs//run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='ngrams', default_output='../runs/')\n",
    "\n",
    "import os\n",
    "os.rename('../runs/run.txt', '../runs/runngram.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
