{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expanding abbrevations in a query -> combine with question query if there is a question query which contains an abbrevation\n",
    "#test phrase based expansion, expand to \"This paper introduces Information Retrieval\" -> set retrieval, boolean retrieval?\n",
    "#adjust bm25 parameters\n",
    "#adjust question prompt so that it catches the intent behind why questions better\n",
    "#What and Why are probably most common question words -> specialize on them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "\n",
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Ngram Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:02<00:00, 57238.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 126958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs =  pt_dataset.get_corpus_iter()\n",
    "docs = list(docs)\n",
    "count = sum(1 for _ in docs)\n",
    "docs = docs[:126959]\n",
    "print(\"Number of documents:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method that removes all special characters from a String, and returns either a String or a list of all words\n",
    "def clean_text(text, return_as_list = False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text) #remove non-alphanumeric characters, except spaces\n",
    "    if return_as_list:\n",
    "        word_list = cleaned_text.split()\n",
    "        return word_list\n",
    "    else:\n",
    "        return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Ensure NLTK data directory is set correctly\n",
    "nltk.data.path.append(\"/usr/local/nltk_data\")\n",
    "\n",
    "# Download 'stopwords' corpus to the specified directory\n",
    "nltk.download('stopwords', download_dir=\"/usr/nltk_data\")\n",
    "\n",
    "# Get the set of stopwords for the English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text, return_as_list = False):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Join the filtered words back into a single string\n",
    "\n",
    "    if return_as_list:\n",
    "        return filtered_words\n",
    "    else:\n",
    "        return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_text(text, return_as_list = False):\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    if return_as_list:\n",
    "        return stemmed_words\n",
    "    # Join the stemmed words back into a single string\n",
    "    else:\n",
    "        return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our ngram tokenizer. It takes a string and returns a dict of all ngrams, where each ngram is seperated by $$ so it will be parsed as one token\n",
    "\n",
    "def tokenize_ngrams_to_dict(text, n1=1, n2=2):\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text.split(' ')\n",
    "    words = [word for word in words if len(''.join(format(ord(c), '08b') for c in word)) <= 60]\n",
    "\n",
    "    # Initialize an empty Counter to hold all n-grams\n",
    "    all_ngram_counts = Counter()\n",
    "    \n",
    "    # Loop through each n from n1 to n2\n",
    "    for n in range(n1, n2 + 1):\n",
    "        # Generate n-grams for the current n\n",
    "        ngrams = ['$$'.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        # Update the Counter with the current n-grams\n",
    "        all_ngram_counts.update(ngrams)\n",
    "    \n",
    "    return dict(all_ngram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'docno': 'O02-2002', 'toks': {'studi': 1, 'word': 7, 'similar': 8, 'use': 3, 'context': 5, 'vector': 3, 'model': 2, 'need': 1, 'measur': 2, 'process': 1, 'natur': 1, 'languag': 1, 'especi': 1, 'general': 1, 'classif': 1, 'exampl': 1, 'base': 3, 'usual': 1, 'two': 1, 'defin': 1, 'accord': 3, 'distanc': 1, 'semant': 6, 'class': 2, 'less': 1, 'consid': 1, 'syntact': 5, 'ie': 2, 'howev': 1, 'real': 1, 'applic': 1, 'requir': 1, 'weight': 1, 'differ': 1, 'mixtur': 1, 'paper': 1, 'propo': 1, 'relat': 1, 'co': 2, 'occurr': 2, 'adopt': 1, 'inform': 1, 'theoret': 1, 'solv': 1, 'problem': 1, 'data': 1, 'spar': 1, 'preci': 1, 'featur': 2, 'deriv': 1, 'par': 1, 'environ': 1, 'adjust': 1, 'idf': 1, 'inver': 1, 'valu': 2, 'agglom': 1, 'cluster': 1, 'appli': 1, 'group': 2, 'turn': 1, 'togeth': 1, 'studi$$word': 1, 'word$$similar': 4, 'similar$$use': 1, 'use$$context': 1, 'context$$vector': 3, 'vector$$model': 1, 'model$$need': 1, 'need$$measur': 1, 'measur$$word': 1, 'similar$$process': 1, 'process$$natur': 1, 'natur$$languag': 1, 'languag$$especi': 1, 'especi$$use': 1, 'use$$general': 1, 'general$$classif': 1, 'classif$$exampl': 1, 'exampl$$base': 1, 'base$$usual': 1, 'usual$$measur': 1, 'measur$$similar': 1, 'similar$$two': 1, 'two$$word': 1, 'word$$defin': 1, 'defin$$accord': 1, 'accord$$distanc': 1, 'distanc$$semant': 1, 'semant$$class': 2, 'class$$semant': 1, 'semant$$less': 1, 'less$$semant': 1, 'semant$$base': 1, 'base$$consid': 1, 'consid$$syntact': 1, 'syntact$$ie': 1, 'ie$$howev': 1, 'howev$$real': 1, 'real$$applic': 1, 'applic$$semant': 1, 'semant$$syntact': 1, 'syntact$$similar': 1, 'similar$$requir': 1, 'requir$$weight': 1, 'weight$$differ': 1, 'differ$$word': 1, 'similar$$base': 1, 'base$$context': 1, 'vector$$mixtur': 1, 'mixtur$$syntact': 1, 'syntact$$semant': 2, 'semant$$ie': 1, 'ie$$paper': 1, 'paper$$propo': 1, 'propo$$use': 1, 'use$$syntact': 1, 'syntact$$relat': 1, 'relat$$co': 1, 'co$$occurr': 2, 'occurr$$context': 2, 'vector$$adopt': 1, 'adopt$$inform': 1, 'inform$$theoret': 1, 'theoret$$model': 1, 'model$$solv': 1, 'solv$$problem': 1, 'problem$$data': 1, 'data$$spar': 1, 'spar$$preci': 1, 'preci$$co': 1, 'context$$featur': 2, 'featur$$deriv': 1, 'deriv$$par': 1, 'par$$environ': 1, 'environ$$word': 1, 'word$$context': 1, 'featur$$adjust': 1, 'adjust$$accord': 1, 'accord$$idf': 1, 'idf$$inver': 1, 'inver$$valu': 1, 'valu$$agglom': 1, 'agglom$$cluster': 1, 'cluster$$appli': 1, 'appli$$group': 1, 'group$$similar': 1, 'similar$$word': 1, 'word$$accord': 1, 'accord$$similar': 1, 'similar$$valu': 1, 'valu$$turn': 1, 'turn$$word': 1, 'similar$$syntact': 1, 'class$$group': 1, 'group$$togeth': 1}}\n",
      "{'docno': 'L02-1310', 'toks': {'larg': 1, 'sens': 1, 'tag': 1, 'corpora': 1, 'larg$$sens': 1, 'sens$$tag': 1, 'tag$$corpora': 1}}\n",
      "{'docno': 'R13-1042', 'toks': {'use': 2, 'pairwis': 2, 'email': 6, 'classif': 2, 'thread': 6, 'task': 1, 'separ': 1, 'convers': 1, 'whose': 1, 'distort': 1, 'lost': 1, 'paper': 1, 'perform': 2, 'text': 4, 'similar': 5, 'measur': 1, 'non': 1, 'quot': 1, 'show': 1, 'content': 2, 'metric': 2, 'style': 1, 'class': 2, 'balanc': 1, 'set': 1, 'ii': 1, 'featur': 2, 'depend': 1, 'semant': 2, 'corpus': 4, 'still': 1, 'effect': 1, 'even': 1, 'control': 1, 'make': 1, 'avail': 1, 'enron': 2, 'newli': 1, 'extract': 1, '70': 1, '178': 1, 'use$$pairwis': 1, 'pairwis$$email': 1, 'email$$classif': 1, 'classif$$email': 1, 'email$$thread': 2, 'thread$$thread': 1, 'thread$$task': 1, 'task$$separ': 1, 'separ$$convers': 1, 'convers$$whose': 1, 'whose$$thread': 1, 'thread$$distort': 1, 'distort$$lost': 1, 'lost$$paper': 1, 'paper$$perform': 1, 'perform$$email': 1, 'thread$$pairwis': 1, 'pairwis$$classif': 1, 'classif$$use': 1, 'use$$text': 1, 'text$$similar': 3, 'similar$$measur': 1, 'measur$$non': 1, 'non$$quot': 1, 'quot$$text': 1, 'text$$email': 1, 'email$$show': 1, 'show$$content': 1, 'content$$text': 1, 'similar$$metric': 2, 'metric$$style': 1, 'style$$text': 1, 'metric$$class': 1, 'class$$balanc': 1, 'balanc$$class': 1, 'class$$set': 1, 'set$$ii': 1, 'ii$$featur': 1, 'featur$$perform': 1, 'perform$$depend': 1, 'depend$$semant': 1, 'semant$$similar': 2, 'similar$$corpus': 1, 'corpus$$content': 1, 'content$$featur': 1, 'featur$$still': 1, 'still$$effect': 1, 'effect$$even': 1, 'even$$control': 1, 'control$$semant': 1, 'similar$$make': 1, 'make$$avail': 1, 'avail$$enron': 1, 'enron$$thread': 1, 'thread$$corpus': 1, 'corpus$$newli': 1, 'newli$$extract': 1, 'extract$$corpus': 1, 'corpus$$70': 1, '70$$178': 1, '178$$thread': 1, 'thread$$email': 1, 'email$$enron': 1, 'enron$$email': 1, 'email$$corpus': 1}}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "        if 'text' in doc:\n",
    "            doc['text'] = clean_text(doc['text'])\n",
    "            doc['text'] = remove_stopwords(doc['text'])\n",
    "            doc['text'] = stem_text(doc['text'])\n",
    "            \n",
    "            doc_1gram = tokenize_ngrams_to_dict(doc['text'], n1=1, n2=2) # Apply n-gram tokenization to the dataset\n",
    "\n",
    "            doc['toks'] = doc_1gram # create new toks field for tokenfrequency\n",
    "            del doc['text']  #This will delete the 'text' field from the documents\n",
    "    \n",
    "for i, doc in enumerate(docs):\n",
    "     if i == 3:\n",
    "           break\n",
    "     print(doc)\n",
    "\n",
    "#remove all empty documents\n",
    "docs = [d for d in docs if any(k != '' for k in d['toks'].keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 126824\n",
      "Number of terms: 1735219\n",
      "Number of postings: 11740755\n",
      "Number of fields: 0\n",
      "Number of tokens: 14688164\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n",
      "0\n",
      "9406\n",
      "00\n",
      "278\n",
      "000\n",
      "1761\n",
      "0001\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# Initialize the IterDictIndexer with pretokenised set to True\n",
    "iter_indexer = pt.IterDictIndexer(\"./ngramindex\", overwrite=True, meta={'docno': 35}, pretokenised=True, verbose = True, type = pt.index.IndexingType.SINGLEPASS)\n",
    "\n",
    "# Index our pretokenized documents\n",
    "index_ref = iter_indexer.index(docs)\n",
    "\n",
    "index_ngram = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "#Print some stats about our index\n",
    "print(index_ngram.getCollectionStatistics())\n",
    "\n",
    "# Access the MetaIndex and Lexicon\n",
    "meta = index_ngram.getMetaIndex()\n",
    "lexicon = index_ngram.getLexicon()\n",
    "\n",
    "\n",
    "i = 0\n",
    "for term, le in index.getLexicon():\n",
    "    i = i+1\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(term) \n",
    "    print(le.getFrequency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(add question query, why, to further see impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify if query contains an abbrevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify if query is a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand selected query accoridingly -> play with tempereature\n",
    "#prompt should react differently for What against Why questions\n",
    "#question querys should be extracted from the dataframe\n",
    "#querys that contain abbrevations but are not a question can remain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(write prompt that uses phrase-based expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank querys without question querys with bm25\n",
    "#maybe adjust bm25 parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over each subquery generated for a question query\n",
    "#rank each subquery with bm25.search then aggregate the score for the question query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the resulting dataframe with the already done bm25 ranking dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save and normalize run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
